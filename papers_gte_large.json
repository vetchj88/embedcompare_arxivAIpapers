[
  {
    "title": "How to Design and Train Your Implicit Neural Representation for Video Compression",
    "date": "2025-06-30",
    "abstract": "Implicit neural representation (INR) methods for video compression have recently achieved visual quality and compression ratios that are competitive with traditional pipelines. However, due to the need for per-sample network training, the encoding speeds of these methods are too slow for practical adoption. We develop a library to allow us to disentangle and review the components of methods from the NeRV family, reframing their performance in terms of not only size-quality trade-offs, but also impacts on training time. We uncover principles for effective video INR design and propose a state-of-the-art configuration of these components, Rabbit NeRV (RNeRV). When all methods are given equal training time (equivalent to 300 NeRV epochs) for 7 different UVG videos at 1080p, RNeRV achieves +1.27% PSNR on average compared to the best-performing alternative for each video in our NeRV library. We then tackle the encoding speed issue head-on by investigating the viability of hyper-networks, which predict INR weights from video inputs, to disentangle training from encoding to allow for real-time encoding. We propose masking the weights of the predicted INR during training to allow for variable, higher quality compression, resulting in 1.7% improvements to both PSNR and MS-SSIM at 0.037 bpp on the UCF-101 dataset, and we increase hyper-network parameters by 0.4% for 2.5%/2.7% improvements to PSNR/MS-SSIM with equal bpp and similar speeds. Our project website is available at https://mgwillia.github.io/vinrb/ and our code is available at https://github.com/mgwillia/vinrb.",
    "authors": "Matthew Gwilliam, Roy Zhang, Namitha Padmanabhan, Hongyang Du, Abhinav Shrivastava",
    "id": "2506.24127v1",
    "x": -2.597033739089966,
    "y": 2.232078790664673,
    "z": 1.3753215074539185,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -2.597033739089966,
        "y": 2.232078790664673,
        "z": 1.3753215074539185
      },
      "starmap": {
        "x": 0.030916409566998482,
        "y": 0.0010213966015726328,
        "z": 0.02207065001130104
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Residual connection has been extensively studied and widely applied at the model architecture level. However, its potential in the more challenging data-centric approaches remains unexplored. In this work, we introduce the concept of Data Residual Matching for the first time, leveraging data-level skip connections to facilitate data generation and mitigate data information vanishing. This approach maintains a balance between newly acquired knowledge through pixel space optimization and existing core local information identification within raw data modalities, specifically for the dataset distillation task. Furthermore, by incorporating optimization-level refinements, our method significantly improves computational efficiency, achieving superior performance while reducing training time and peak GPU memory usage by 50%. Consequently, the proposed method Fast and Accurate Data Residual Matching for Dataset Distillation (FADRM) establishes a new state-of-the-art, demonstrating substantial improvements over existing methods across multiple dataset benchmarks in both efficiency and effectiveness. For instance, with ResNet-18 as the student model and a 0.8% compression ratio on ImageNet-1K, the method achieves 47.7% test accuracy in single-model dataset distillation and 50.0% in multi-model dataset distillation, surpassing RDED by +5.7% and outperforming state-of-the-art multi-model approaches, EDC and CV-DD, by +1.4% and +4.0%. Code is available at: https://github.com/Jiacheng8/FADRM.",
    "date": "2025-06-30",
    "id": "2506.24125v1",
    "title": "FADRM: Fast and Accurate Data Residual Matching for Dataset Distillation",
    "authors": "Jiacheng Cui, Xinyue Bi, Yaxin Luo, Xiaohan Zhao, Jiacheng Liu, Zhiqiang Shen",
    "x": -0.6097760796546936,
    "y": -0.5760597586631775,
    "z": -0.5171078443527222,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -0.6097760796546936,
        "y": -0.5760597586631775,
        "z": -0.5171078443527222
      },
      "starmap": {
        "x": 0.012218507938086987,
        "y": -0.018422095105051994,
        "z": -0.015128565020859241
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "Teaching Time Series to See and Speak: Forecasting with Aligned Visual and Textual Perspectives",
    "date": "2025-06-30",
    "id": "2506.24124v2",
    "abstract": "Time series forecasting traditionally relies on unimodal numerical inputs, which often struggle to capture high-level semantic patterns due to their dense and unstructured nature. While recent approaches have explored representing time series as text using large language models (LLMs), these methods remain limited by the discrete nature of token sequences and lack the perceptual intuition humans typically apply, such as interpreting visual patterns. In this paper, we propose a multimodal contrastive learning framework that transforms raw time series into structured visual and textual perspectives. Rather than using natural language or real-world images, we construct both modalities directly from numerical sequences. We then align these views in a shared semantic space via contrastive learning, enabling the model to capture richer and more complementary representations. Furthermore, we introduce a variate selection module that leverages the aligned representations to identify the most informative variables for multivariate forecasting. Extensive experiments on fifteen short-term and six long-term forecasting benchmarks demonstrate that our approach consistently outperforms strong unimodal and cross-modal baselines, highlighting the effectiveness of multimodal alignment in enhancing time series forecasting. Code is available at: https://github.com/Ironieser/TimesCLIP.",
    "authors": "Sixun Dong, Wei Fan, Teresa Wu, Yanjie Fu",
    "x": -0.6338099241256714,
    "y": 0.23305746912956238,
    "z": -0.4755445718765259,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -0.6338099241256714,
        "y": 0.23305746912956238,
        "z": -0.4755445718765259
      },
      "starmap": {
        "x": 0.014804225414991379,
        "y": -0.02825537510216236,
        "z": -0.018676523119211197
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.24123v1",
    "title": "Calligrapher: Freestyle Text Image Customization",
    "authors": "Yue Ma, Qingyan Bai, Hao Ouyang, Ka Leong Cheng, Qiuyu Wang, Hongyu Liu, Zichen Liu, Haofan Wang, Jingye Chen, Yujun Shen, Qifeng Chen",
    "date": "2025-06-30",
    "abstract": "We introduce Calligrapher, a novel diffusion-based framework that innovatively integrates advanced text customization with artistic typography for digital calligraphy and design applications. Addressing the challenges of precise style control and data dependency in typographic customization, our framework incorporates three key technical contributions. First, we develop a self-distillation mechanism that leverages the pre-trained text-to-image generative model itself alongside the large language model to automatically construct a style-centric typography benchmark. Second, we introduce a localized style injection framework via a trainable style encoder, which comprises both Qformer and linear layers, to extract robust style features from reference images. An in-context generation mechanism is also employed to directly embed reference images into the denoising process, further enhancing the refined alignment of target styles. Extensive quantitative and qualitative evaluations across diverse fonts and design contexts confirm Calligrapher's accurate reproduction of intricate stylistic details and precise glyph positioning. By automating high-quality, visually consistent typography, Calligrapher surpasses traditional models, empowering creative practitioners in digital art, branding, and contextual typographic design.",
    "x": -3.496447801589966,
    "y": 1.126227855682373,
    "z": -0.0800841748714447,
    "cluster_id": 2,
    "layouts": {
      "pacmap": {
        "x": -3.496447801589966,
        "y": 1.126227855682373,
        "z": -0.0800841748714447
      },
      "starmap": {
        "x": 0.02089037373661995,
        "y": -0.0007584632257930934,
        "z": 0.06982176005840302
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "id": "2506.24121v1",
    "abstract": "Recent advancements in diffusion generative models significantly advanced image, video, and 3D content creation from user-provided text prompts. However, the challenging problem of dynamic 3D content generation (text-to-4D) with diffusion guidance remains largely unexplored. In this paper, we introduce TextMesh4D, a novel framework for high-quality text-to-4D generation. Our approach leverages per-face Jacobians as a differentiable mesh representation and decomposes 4D generation into two stages: static object creation and dynamic motion synthesis. We further propose a flexibility-rigidity regularization term to stabilize Jacobian optimization under video diffusion priors, ensuring robust geometric performance. Experiments demonstrate that TextMesh4D achieves state-of-the-art results in terms of temporal consistency, structural fidelity, and visual realism. Moreover, TextMesh4D operates with a low GPU memory overhead-requiring only a single 24GB GPU-offering a cost-effective yet high-quality solution for text-driven 4D mesh generation. The code will be released to facilitate future research in text-to-4D generation.",
    "authors": "Sisi Dai, Xinxin Su, Boyan Wan, Ruizhen Hu, Kai Xu",
    "title": "TextMesh4D: High-Quality Text-to-4D Mesh Generation",
    "x": -2.5464820861816406,
    "y": 1.38588547706604,
    "z": -0.010197661817073822,
    "cluster_id": 2,
    "layouts": {
      "pacmap": {
        "x": -2.5464820861816406,
        "y": 1.38588547706604,
        "z": -0.010197661817073822
      },
      "starmap": {
        "x": 0.024098960682749748,
        "y": 0.0008539546397514641,
        "z": 0.07648847252130508
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Yuqing Wang, Shangding Gu",
    "title": "Data Uniformity Improves Training Efficiency and More, with a Convergence Framework Beyond the NTK Regime",
    "id": "2506.24120v1",
    "date": "2025-06-30",
    "abstract": "Data selection plays a crucial role in data-driven decision-making, including in large language models (LLMs), and is typically task-dependent. Properties such as data quality and diversity have been extensively studied and are known to enhance model performance. However, it remains unclear whether there exist other quantitative and general principles of data selection that can consistently improve performance, especially for complex tasks with limited prior knowledge. In this paper, we demonstrate that selecting more uniformly distributed data can improve training efficiency while enhancing performance. Specifically, we establish that more uniform (less biased) distribution leads to a larger minimum pairwise distance between data points, denoted by $h_{\\min}$, and prove that a smaller $h_{\\min}$ can slow down the training dynamics of gradient descent (GD). Moreover, we theoretically show that the approximation error of neural networks decreases as $h_{\\min}$ increases. Our analysis introduces a convergence framework for GD beyond the Neural Tangent Kernel (NTK) regime, applicable to a broad class of architectures, including transformers, without requiring Lipschitz smoothness. This framework further provides theoretical justification for the use of residual connections and function compositions in deep neural architectures. In the end, we conduct comprehensive experiments for supervised fine-tuning across various settings, including different optimization strategies, model sizes, and training datasets. The results consistently demonstrate that selecting data by maximizing pairwise distance significantly accelerates training and achieves comparable or better performance in LLMs across diverse datasets. Code and Datasets are available at the link: https://github.com/SafeRL-Lab/data-uniformity.",
    "x": 0.12319499254226685,
    "y": -1.6438088417053223,
    "z": -0.09571117907762527,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": 0.12319499254226685,
        "y": -1.6438088417053223,
        "z": -0.09571117907762527
      },
      "starmap": {
        "x": 0.009303887374699116,
        "y": -0.013468890450894833,
        "z": -0.012102030217647552
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Recent advances in reinforcement learning have shown that language models can develop sophisticated reasoning through training on tasks with verifiable rewards, but these approaches depend on human-curated problem-answer pairs and domain-specific reward engineering. We introduce SPIRAL, a self-play framework where models learn by playing multi-turn, zero-sum games against continuously improving versions of themselves, eliminating the need for human supervision. Through self-play, SPIRAL generates an infinite curriculum of progressively challenging problems as models must constantly adapt to stronger opponents. To enable this self-play training at scale, We implement a fully online, multi-turn, multi-agent reinforcement learning system for LLMs and propose role-conditioned advantage estimation (RAE) to stabilize multi-agent training. Using SPIRAL, self-play on zero-sum games produces reasoning capabilities that transfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6% improvement on math and 8.4% on general reasoning, outperforming SFT on 25,000 expert game trajectories. Analysis reveals that this transfer occurs through three cognitive patterns: systematic decomposition, expected value calculation, and case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple Negotiation) further enhances performance as each game develops distinct reasoning strengths. Applying SPIRAL to a strong reasoning model (DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These results demonstrate that zero-sum games naturally develop transferable reasoning capabilities, highlighting a promising direction for autonomous reasoning development.",
    "authors": "Bo Liu, Leon Guertler, Simon Yu, Zichen Liu, Penghui Qi, Daniel Balcells, Mickel Liu, Cheston Tan, Weiyan Shi, Min Lin, Wee Sun Lee, Natasha Jaques",
    "date": "2025-06-30",
    "title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning",
    "id": "2506.24119v2",
    "x": 3.046995162963867,
    "y": 1.5220801830291748,
    "z": 0.8224645256996155,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 3.046995162963867,
        "y": 1.5220801830291748,
        "z": 0.8224645256996155
      },
      "starmap": {
        "x": -0.024877911433577538,
        "y": 0.009958933107554913,
        "z": -0.0021373473573476076
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.24117v2",
    "abstract": "Identifying parallel passages in biblical Hebrew (BH) is central to biblical scholarship for understanding intertextual relationships. Traditional methods rely on manual comparison, a labor-intensive process prone to human error. This study evaluates the potential of pre-trained transformer-based language models, including E5, AlephBERT, MPNet, and LaBSE, for detecting textual parallels in the Hebrew Bible. Focusing on known parallels between Samuel/Kings and Chronicles, I assessed each model's capability to generate word embeddings distinguishing parallel from non-parallel passages. Using cosine similarity and Wasserstein Distance measures, I found that E5 and AlephBERT show promise; E5 excels in parallel detection, while AlephBERT demonstrates stronger non-parallel differentiation. These findings indicate that pre-trained models can enhance the efficiency and accuracy of detecting intertextual parallels in ancient texts, suggesting broader applications for ancient language studies.",
    "date": "2025-06-30",
    "title": "Intertextual Parallel Detection in Biblical Hebrew: A Transformer-Based Benchmark",
    "authors": "David M. Smiley",
    "x": 0.5921952128410339,
    "y": 1.4678826332092285,
    "z": -2.346433401107788,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 0.5921952128410339,
        "y": 1.4678826332092285,
        "z": -2.346433401107788
      },
      "starmap": {
        "x": -0.02100488916039467,
        "y": 0.006643673870712519,
        "z": 0.004404881503432989
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "Epona: Autoregressive Diffusion World Model for Autonomous Driving",
    "id": "2506.24113v1",
    "authors": "Kaiwen Zhang, Zhenyu Tang, Xiaotao Hu, Xingang Pan, Xiaoyang Guo, Yuan Liu, Jingwei Huang, Li Yuan, Qian Zhang, Xiao-Xiao Long, Xun Cao, Wei Yin",
    "abstract": "Diffusion models have demonstrated exceptional visual quality in video generation, making them promising for autonomous driving world modeling. However, existing video diffusion-based world models struggle with flexible-length, long-horizon predictions and integrating trajectory planning. This is because conventional video diffusion models rely on global joint distribution modeling of fixed-length frame sequences rather than sequentially constructing localized distributions at each timestep. In this work, we propose Epona, an autoregressive diffusion world model that enables localized spatiotemporal distribution modeling through two key innovations: 1) Decoupled spatiotemporal factorization that separates temporal dynamics modeling from fine-grained future world generation, and 2) Modular trajectory and video prediction that seamlessly integrate motion planning with visual modeling in an end-to-end framework. Our architecture enables high-resolution, long-duration generation while introducing a novel chain-of-forward training strategy to address error accumulation in autoregressive loops. Experimental results demonstrate state-of-the-art performance with 7.4\\% FVD improvement and minutes longer prediction duration compared to prior works. The learned world model further serves as a real-time motion planner, outperforming strong end-to-end planners on NAVSIM benchmarks. Code will be publicly available at \\href{https://github.com/Kevin-thu/Epona/}{https://github.com/Kevin-thu/Epona/}.",
    "date": "2025-06-30",
    "x": -2.310335636138916,
    "y": 1.5536154508590698,
    "z": 1.2491655349731445,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -2.310335636138916,
        "y": 1.5536154508590698,
        "z": 1.2491655349731445
      },
      "starmap": {
        "x": 0.020287055522203445,
        "y": -0.009580445475876331,
        "z": 0.031836215406656265
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Shai Yehezkel, Omer Dahary, Andrey Voynov, Daniel Cohen-Or",
    "date": "2025-06-30",
    "id": "2506.24108v1",
    "abstract": "Denoising diffusion models excel at generating high-quality images conditioned on text prompts, yet their effectiveness heavily relies on careful guidance during the sampling process. Classifier-Free Guidance (CFG) provides a widely used mechanism for steering generation by setting the guidance scale, which balances image quality and prompt alignment. However, the choice of the guidance scale has a critical impact on the convergence toward a visually appealing and prompt-adherent image. In this work, we propose an annealing guidance scheduler which dynamically adjusts the guidance scale over time based on the conditional noisy signal. By learning a scheduling policy, our method addresses the temperamental behavior of CFG. Empirical results demonstrate that our guidance scheduler significantly enhances image quality and alignment with the text prompt, advancing the performance of text-to-image generation. Notably, our novel scheduler requires no additional activations or memory consumption, and can seamlessly replace the common classifier-free guidance, offering an improved trade-off between prompt alignment and quality.",
    "title": "Navigating with Annealing Guidance Scale in Diffusion Space",
    "x": -2.526927947998047,
    "y": 0.4911927878856659,
    "z": -0.12082526832818985,
    "cluster_id": 2,
    "layouts": {
      "pacmap": {
        "x": -2.526927947998047,
        "y": 0.4911927878856659,
        "z": -0.12082526832818985
      },
      "starmap": {
        "x": 0.024675162509083748,
        "y": 0.00022097970941103995,
        "z": 0.06787395477294922
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "authors": "Yanhong Li, Ming Li, Karen Livescu, Jiawei Zhou",
    "id": "2506.24106v1",
    "abstract": "We show that a language model's ability to predict text is tightly linked to the breadth of its embedding space: models that spread their contextual representations more widely tend to achieve lower perplexity. Concretely, we find that representation dispersion - the average pairwise cosine distance among hidden vectors - strongly and negatively correlates with perplexity across diverse model families (LLaMA, Qwen, and others) and domains (Wikipedia, news, scientific abstracts). Beyond illustrating this link, we show how dispersion can be leveraged for a range of practical tasks without requiring labeled data. First, measuring dispersion on unlabeled text allows us to predict downstream accuracy in new domains, offering a data-efficient tool for model selection. Next, we find that identifying layers with higher dispersion pinpoints the best representations for retrieval-based methods such as kNN-LM, bypassing exhaustive layer-by-layer searches. Finally, we integrate a simple push-away objective into training, which increases dispersion in both single-domain and cross-domain scenarios and directly improves perplexity in each.",
    "title": "On the Predictive Power of Representation Dispersion in Language Models",
    "x": -0.2594945430755615,
    "y": -1.0505601167678833,
    "z": -2.111377716064453,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -0.2594945430755615,
        "y": -1.0505601167678833,
        "z": -2.111377716064453
      },
      "starmap": {
        "x": 0.009513420052826405,
        "y": -0.018662992864847183,
        "z": -0.015895845368504524
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "title": "DenseWorld-1M: Towards Detailed Dense Grounded Caption in the Real World",
    "abstract": "Multimodal Large Language Models (MLLMs) demonstrate a complex understanding of scenes, benefiting from large-scale and high-quality datasets. Most existing caption datasets lack the ground locations and relations for visual entities. Several grounded caption datasets face the problems of missing detailed descriptions, relations, and massive object descriptions on high-resolution images. To fill this gap for the community, we present DenseWorld-1M, the first massive, detailed, dense grounded caption dataset in the real world. We design a three-stage labeling pipeline, containing open-world perception, detailed object caption generation, and dense caption merging. The first stage obtains entity-level masks and labels. The second stage generates the object-level, detailed captions with the guidance of masks and labels from the first stage. The final stage merges object captions and masks into spatial and relational dense captions. To accelerate the labeling process and improve caption quality, we present two VLM models: the Detailed Region Caption model and the Spatial Caption Merging model. Extensive experiments on various settings, including vision-language understanding, visual grounding, and region caption generation, demonstrate the effectiveness of our DenseWorld-1M dataset and labeling models.",
    "id": "2506.24102v1",
    "authors": "Xiangtai Li, Tao Zhang, Yanwei Li, Haobo Yuan, Shihao Chen, Yikang Zhou, Jiahao Meng, Yueyi Sun, Shilin Xu, Lu Qi, Tianheng Cheng, Yi Lin, Zilong Huang, Wenhao Huang, Jiashi Feng, Guang Shi",
    "x": -1.5295004844665527,
    "y": 1.8513729572296143,
    "z": -1.2163444757461548,
    "cluster_id": 3,
    "layouts": {
      "pacmap": {
        "x": -1.5295004844665527,
        "y": 1.8513729572296143,
        "z": -1.2163444757461548
      },
      "starmap": {
        "x": 0.01951146498322487,
        "y": -0.03979681804776192,
        "z": -0.026586730033159256
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "While recent advances in Gaussian Splatting have enabled fast reconstruction of high-quality 3D scenes from images, extracting accurate surface meshes remains a challenge. Current approaches extract the surface through costly post-processing steps, resulting in the loss of fine geometric details or requiring significant time and leading to very dense meshes with millions of vertices. More fundamentally, the a posteriori conversion from a volumetric to a surface representation limits the ability of the final mesh to preserve all geometric structures captured during training. We present MILo, a novel Gaussian Splatting framework that bridges the gap between volumetric and surface representations by differentiably extracting a mesh from the 3D Gaussians. We design a fully differentiable procedure that constructs the mesh-including both vertex locations and connectivity-at every iteration directly from the parameters of the Gaussians, which are the only quantities optimized during training. Our method introduces three key technical contributions: a bidirectional consistency framework ensuring both representations-Gaussians and the extracted mesh-capture the same underlying geometry during training; an adaptive mesh extraction process performed at each training iteration, which uses Gaussians as differentiable pivots for Delaunay triangulation; a novel method for computing signed distance values from the 3D Gaussians that enables precise surface extraction while avoiding geometric erosion. Our approach can reconstruct complete scenes, including backgrounds, with state-of-the-art quality while requiring an order of magnitude fewer mesh vertices than previous methods. Due to their light weight and empty interior, our meshes are well suited for downstream applications such as physics simulations or animation.",
    "title": "MILo: Mesh-In-the-Loop Gaussian Splatting for Detailed and Efficient Surface Reconstruction",
    "id": "2506.24096v1",
    "authors": "Antoine Gu\u00e9don, Diego Gomez, Nissim Maruani, Bingchen Gong, George Drettakis, Maks Ovsjanikov",
    "date": "2025-06-30",
    "x": -2.5075721740722656,
    "y": 2.5742149353027344,
    "z": -0.32271620631217957,
    "cluster_id": 1,
    "layouts": {
      "pacmap": {
        "x": -2.5075721740722656,
        "y": 2.5742149353027344,
        "z": -0.32271620631217957
      },
      "starmap": {
        "x": 0.04956270009279251,
        "y": 0.06903336197137833,
        "z": -0.023339737206697464
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Paul Wachter, Lukas Niehaus, Julius Sch\u00f6ning",
    "abstract": "Synthetic data has emerged as a cost-effective alternative to real data for training artificial neural networks (ANN). However, the disparity between synthetic and real data results in a domain gap. That gap leads to poor performance and generalization of the trained ANN when applied to real-world scenarios. Several strategies have been developed to bridge this gap, which combine synthetic and real data, known as mixed training using hybrid datasets. While these strategies have been shown to mitigate the domain gap, a systematic evaluation of their generalizability and robustness across various tasks and architectures remains underexplored. To address this challenge, our study comprehensively analyzes two widely used mixing strategies on three prevalent architectures and three distinct hybrid datasets. From these datasets, we sample subsets with varying proportions of synthetic to real data to investigate the impact of synthetic and real components. The findings of this paper provide valuable insights into optimizing the use of synthetic data in the training process of any ANN, contributing to enhancing robustness and efficacy.",
    "date": "2025-06-30",
    "id": "2506.24093v1",
    "title": "Development of Hybrid Artificial Intelligence Training on Real and Synthetic Data: Benchmark on Two Mixed Training Strategies",
    "x": -0.09843840450048447,
    "y": -0.6859821677207947,
    "z": -0.21527719497680664,
    "cluster_id": 6,
    "layouts": {
      "pacmap": {
        "x": -0.09843840450048447,
        "y": -0.6859821677207947,
        "z": -0.21527719497680664
      },
      "starmap": {
        "x": 0.014577722176909447,
        "y": -0.023568512871861458,
        "z": -0.01629156619310379
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "MotionGPT3: Human Motion as a Second Modality",
    "id": "2506.24086v1",
    "abstract": "Though recent advances in multimodal models have demonstrated strong capabilities and opportunities in unified understanding and generation, the development of unified motion-language models remains underexplored. To enable such models with high-fidelity human motion, two core challenges must be addressed. The first is the reconstruction gap between the continuous motion modality and discrete representation in an autoregressive manner, and the second is the degradation of language intelligence during unified training. Inspired by the mixture of experts, we propose MotionGPT3, a bimodal motion-language model that treats human motion as a second modality, decoupling motion modeling via separate model parameters and enabling both effective cross-modal interaction and efficient multimodal scaling training. To preserve language intelligence, the text branch retains the original structure and parameters of the pretrained language model, while a new motion branch is integrated via a shared attention mechanism, enabling bidirectional information flow between two modalities. We first employ a motion Variational Autoencoder (VAE) to encode raw human motion into latent representations. Based on this continuous latent space, the motion branch predicts motion latents directly from intermediate hidden states using a diffusion head, bypassing discrete tokenization. Extensive experiments show that our approach achieves competitive performance on both motion understanding and generation tasks while preserving strong language capabilities, establishing a unified bimodal motion diffusion framework within an autoregressive manner.",
    "date": "2025-06-30",
    "authors": "Bingfan Zhu, Biao Jiang, Sunyi Wang, Shixiang Tang, Tao Chen, Linjie Luo, Youyi Zheng, Xin Chen",
    "x": -2.3854148387908936,
    "y": 1.0528134107589722,
    "z": 1.5092928409576416,
    "cluster_id": 2,
    "layouts": {
      "pacmap": {
        "x": -2.3854148387908936,
        "y": 1.0528134107589722,
        "z": 1.5092928409576416
      },
      "starmap": {
        "x": 0.02288205362856388,
        "y": -0.005182829685509205,
        "z": 0.06626185774803162
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "Imagine for Me: Creative Conceptual Blending of Real Images and Text via Blended Attention",
    "abstract": "Blending visual and textual concepts into a new visual concept is a unique and powerful trait of human beings that can fuel creativity. However, in practice, cross-modal conceptual blending for humans is prone to cognitive biases, like design fixation, which leads to local minima in the design space. In this paper, we propose a T2I diffusion adapter \"IT-Blender\" that can automate the blending process to enhance human creativity. Prior works related to cross-modal conceptual blending are limited in encoding a real image without loss of details or in disentangling the image and text inputs. To address these gaps, IT-Blender leverages pretrained diffusion models (SD and FLUX) to blend the latent representations of a clean reference image with those of the noisy generated image. Combined with our novel blended attention, IT-Blender encodes the real reference image without loss of details and blends the visual concept with the object specified by the text in a disentangled way. Our experiment results show that IT-Blender outperforms the baselines by a large margin in blending visual and textual concepts, shedding light on the new application of image generative models to augment human creativity.",
    "authors": "Wonwoong Cho, Yanxia Zhang, Yan-Ying Chen, David I. Inouye",
    "id": "2506.24085v1",
    "date": "2025-06-30",
    "x": -2.724341869354248,
    "y": 0.5532392859458923,
    "z": -0.21263518929481506,
    "cluster_id": 2,
    "layouts": {
      "pacmap": {
        "x": -2.724341869354248,
        "y": 0.5532392859458923,
        "z": -0.21263518929481506
      },
      "starmap": {
        "x": 0.02035597711801529,
        "y": -0.0021146934013813734,
        "z": 0.07615923881530762
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "id": "2506.24081v1",
    "authors": "Rahul Kumar, Wenqi Wei, Ying Mao, Junaid Farooq, Ying Wang, Juntao Chen",
    "title": "SQUASH: A SWAP-Based Quantum Attack to Sabotage Hybrid Quantum Neural Networks",
    "abstract": "We propose a circuit-level attack, SQUASH, a SWAP-Based Quantum Attack to sabotage Hybrid Quantum Neural Networks (HQNNs) for classification tasks. SQUASH is executed by inserting SWAP gate(s) into the variational quantum circuit of the victim HQNN. Unlike conventional noise-based or adversarial input attacks, SQUASH directly manipulates the circuit structure, leading to qubit misalignment and disrupting quantum state evolution. This attack is highly stealthy, as it does not require access to training data or introduce detectable perturbations in input states. Our results demonstrate that SQUASH significantly degrades classification performance, with untargeted SWAP attacks reducing accuracy by up to 74.08\\% and targeted SWAP attacks reducing target class accuracy by up to 79.78\\%. These findings reveal a critical vulnerability in HQNN implementations, underscoring the need for more resilient architectures against circuit-level adversarial interventions.",
    "x": 1.7792869806289673,
    "y": -1.6025795936584473,
    "z": 2.031818389892578,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": 1.7792869806289673,
        "y": -1.6025795936584473,
        "z": 2.031818389892578
      },
      "starmap": {
        "x": -0.006876571103930473,
        "y": -0.00011364060628693551,
        "z": 0.012930302880704403
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "id": "2506.24074v1",
    "authors": "Mayank V. Golhar, Lucas Sebastian Galeano Fretes, Loren Ayers, Venkata S. Akshintala, Taylor L. Bobrow, Nicholas J. Durr",
    "title": "C3VDv2 -- Colonoscopy 3D video dataset with enhanced realism",
    "abstract": "Computer vision techniques have the potential to improve the diagnostic performance of colonoscopy, but the lack of 3D colonoscopy datasets for training and validation hinders their development. This paper introduces C3VDv2, the second version (v2) of the high-definition Colonoscopy 3D Video Dataset, featuring enhanced realism designed to facilitate the quantitative evaluation of 3D colon reconstruction algorithms. 192 video sequences were captured by imaging 60 unique, high-fidelity silicone colon phantom segments. Ground truth depth, surface normals, optical flow, occlusion, six-degree-of-freedom pose, coverage maps, and 3D models are provided for 169 colonoscopy videos. Eight simulated screening colonoscopy videos acquired by a gastroenterologist are provided with ground truth poses. The dataset includes 15 videos featuring colon deformations for qualitative assessment. C3VDv2 emulates diverse and challenging scenarios for 3D reconstruction algorithms, including fecal debris, mucous pools, blood, debris obscuring the colonoscope lens, en-face views, and fast camera motion. The enhanced realism of C3VDv2 will allow for more robust and representative development and evaluation of 3D reconstruction algorithms.",
    "x": -2.0815606117248535,
    "y": 2.623276948928833,
    "z": -0.12276674807071686,
    "cluster_id": 1,
    "layouts": {
      "pacmap": {
        "x": -2.0815606117248535,
        "y": 2.623276948928833,
        "z": -0.12276674807071686
      },
      "starmap": {
        "x": 0.0522039532661438,
        "y": 0.07564296573400497,
        "z": -0.028658805415034294
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "abstract": "Frontier AI developers are relying on layers of safeguards to protect against catastrophic misuse of AI systems. Anthropic guards their latest Claude 4 Opus model using one such defense pipeline, and other frontier developers including Google DeepMind and OpenAI pledge to soon deploy similar defenses. However, the security of such pipelines is unclear, with limited prior work evaluating or attacking these pipelines. We address this gap by developing and red-teaming an open-source defense pipeline. First, we find that a novel few-shot-prompted input and output classifier outperforms state-of-the-art open-weight safeguard model ShieldGemma across three attacks and two datasets, reducing the attack success rate (ASR) to 0% on the catastrophic misuse dataset ClearHarm. Second, we introduce a STaged AttaCK (STACK) procedure that achieves 71% ASR on ClearHarm in a black-box attack against the few-shot-prompted classifier pipeline. Finally, we also evaluate STACK in a transfer setting, achieving 33% ASR, providing initial evidence that it is feasible to design attacks with no access to the target pipeline. We conclude by suggesting specific mitigations that developers could use to thwart staged attacks.",
    "title": "STACK: Adversarial Attacks on LLM Safeguard Pipelines",
    "id": "2506.24068v1",
    "authors": "Ian R. McKenzie, Oskar J. Hollinsworth, Tom Tseng, Xander Davies, Stephen Casper, Aaron D. Tucker, Robert Kirk, Adam Gleave",
    "x": 2.3425965309143066,
    "y": -1.5099033117294312,
    "z": 1.7794582843780518,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 2.3425965309143066,
        "y": -1.5099033117294312,
        "z": 1.7794582843780518
      },
      "starmap": {
        "x": -0.022122951224446297,
        "y": 0.008003335446119308,
        "z": 0.0042677102610468864
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Deng Li, Aming Wu, Yang Li, Yaowei Wang, Yahong Han",
    "abstract": "In practice, environments constantly change over time and space, posing significant challenges for object detectors trained based on a closed-set assumption, i.e., training and test data share the same distribution. To this end, continual test-time adaptation has attracted much attention, aiming to improve detectors' generalization by fine-tuning a few specific parameters, e.g., BatchNorm layers. However, based on a small number of test images, fine-tuning certain parameters may affect the representation ability of other fixed parameters, leading to performance degradation. Instead, we explore a new mechanism, i.e., converting the fine-tuning process to a specific-parameter generation. Particularly, we first design a dual-path LoRA-based domain-aware adapter that disentangles features into domain-invariant and domain-specific components, enabling efficient adaptation. Additionally, a conditional diffusion-based parameter generation mechanism is presented to synthesize the adapter's parameters based on the current environment, preventing the optimization from getting stuck in local optima. Finally, we propose a class-centered optimal transport alignment method to mitigate catastrophic forgetting. Extensive experiments conducted on various continuous domain adaptive object detection tasks demonstrate the effectiveness. Meanwhile, visualization results show that the representation extracted by the generated parameters can capture more object-related information and strengthen the generalization ability.",
    "title": "Continual Adaptation: Environment-Conditional Parameter Generation for Object Detection in Dynamic Scenarios",
    "id": "2506.24063v1",
    "date": "2025-06-30",
    "x": -0.6386643648147583,
    "y": -0.5648393630981445,
    "z": -0.6073185801506042,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -0.6386643648147583,
        "y": -0.5648393630981445,
        "z": -0.6073185801506042
      },
      "starmap": {
        "x": 0.010953351855278015,
        "y": -0.013790867291390896,
        "z": -0.016027161851525307
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "We introduce logit-gap steering, a fast jailbreak framework that casts the refusal-affirmation gap of RLHF-aligned language models as a single pass over the vocabulary. A forward-computable score blends gap reduction with lightweight proxies for KL penalty and reward shift, allowing a \"sort-sum-stop\" sweep to complete in under a second and return a short suffix--two orders of magnitude fewer model calls than beam or gradient attacks. The same suffix generalises to unseen prompts and scales from 0.5 B to 70 B checkpoints, lifting one-shot attack success from baseline levels to 80-100% while preserving topical coherence. Beyond efficiency, these suffixes expose sentence-boundary reward cliffs and other alignment artefacts, offering a lightweight probe into how safety tuning reshapes internal representations.",
    "title": "Logit-Gap Steering: Efficient Short-Suffix Jailbreaks for Aligned Large Language Models",
    "date": "2025-06-30",
    "id": "2506.24056v1",
    "authors": "Tung-Ling Li, Hongliang Liu",
    "x": 2.0059664249420166,
    "y": -0.9954387545585632,
    "z": 1.141040325164795,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 2.0059664249420166,
        "y": -0.9954387545585632,
        "z": 1.141040325164795
      },
      "starmap": {
        "x": -0.02062465064227581,
        "y": 0.007805591449141502,
        "z": 4.425698716659099e-05
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.24048v1",
    "abstract": "Consensus-based optimization (CBO) has established itself as an efficient gradient-free optimization scheme, with attractive mathematical properties, such as mean-field convergence results for non-convex loss functions. In this work, we study CBO in the context of closed-box adversarial attacks, which are imperceptible input perturbations that aim to fool a classifier, without accessing its gradient. Our contribution is to establish a connection between the so-called consensus hopping as introduced by Riedl et al. and natural evolution strategies (NES) commonly applied in the context of adversarial attacks and to rigorously relate both methods to gradient-based optimization schemes. Beyond that, we provide a comprehensive experimental study that shows that despite the conceptual similarities, CBO can outperform NES and other evolutionary strategies in certain scenarios.",
    "title": "Consensus-based optimization for closed-box adversarial attacks and a connection to evolution strategies",
    "authors": "Tim Roith, Leon Bungert, Philipp Wacker",
    "date": "2025-06-30",
    "x": 1.5233203172683716,
    "y": -2.4551844596862793,
    "z": 1.4867914915084839,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": 1.5233203172683716,
        "y": -2.4551844596862793,
        "z": 1.4867914915084839
      },
      "starmap": {
        "x": -0.0035285709891468287,
        "y": 0.0017417354974895716,
        "z": 0.022793980315327644
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "The proliferation of agentic Large Language Models (LLMs) on personal devices introduces a new class of workloads characterized by a dichotomy of objectives. Reactive tasks, initiated by users, demand immediate, low-latency responses, while proactive tasks operate invisibly and prioritize throughput. Existing on-device LLM engines, designed for isolated inferences, fail to efficiently manage these concurrent and conflicting requests on consumer-grade heterogeneous SoCs with CPU, integrated GPU, and NPU. This paper introduces Agent.xpu, an efficient serving system for agentic LLM workloads on memory-unified heterogeneous SoCs. With dedicated offline profiling, Agent.xpu first constructs a heterogeneous execution graph, which fuses and chunks model kernels for affinity-guided, elastic accelerator mapping with predictive kernel annotation. At runtime, its online scheduler enables fine-grained, kernel-level preemption to guarantee the responsiveness of reactive tasks. To maximize SoC utilization, it adopts slack-aware kernel backfill to opportunistically append proactive tasks, and mitigates NPU-iGPU contention via bandwidth-aware dispatch. Evaluation on an Intel Core Ultra SoC shows that Agent.xpu achieves 4.6$\\times$ lower latency for reactive tasks and sustains 1.6$\\times$-6.8$\\times$ higher throughput for proactive tasks compared to state-of-the-art inference engines.",
    "authors": "Xinming Wei, Jiahao Zhang, Haoran Li, Jiayu Chen, Rui Qu, Maoliang Li, Xiang Chen, Guojie Luo",
    "date": "2025-06-30",
    "id": "2506.24045v1",
    "title": "Agent.xpu: Efficient Scheduling of Agentic LLM Workloads on Heterogeneous SoC",
    "x": 2.6051204204559326,
    "y": 0.7112265825271606,
    "z": 2.193743944168091,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 2.6051204204559326,
        "y": 0.7112265825271606,
        "z": 2.193743944168091
      },
      "starmap": {
        "x": -0.02532380260527134,
        "y": 0.01032544020563364,
        "z": 0.0006937635480426252
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "The rapid progress of multimodal large language models (MLLM) has paved the way for Vision-Language-Action (VLA) paradigms, which integrate visual perception, natural language understanding, and control within a single policy. Researchers in autonomous driving are actively adapting these methods to the vehicle domain. Such models promise autonomous vehicles that can interpret high-level instructions, reason about complex traffic scenes, and make their own decisions. However, the literature remains fragmented and is rapidly expanding. This survey offers the first comprehensive overview of VLA for Autonomous Driving (VLA4AD). We (i) formalize the architectural building blocks shared across recent work, (ii) trace the evolution from early explainer to reasoning-centric VLA models, and (iii) compare over 20 representative models according to VLA's progress in the autonomous driving domain. We also consolidate existing datasets and benchmarks, highlighting protocols that jointly measure driving safety, accuracy, and explanation quality. Finally, we detail open challenges - robustness, real-time efficiency, and formal verification - and outline future directions of VLA4AD. This survey provides a concise yet complete reference for advancing interpretable socially aligned autonomous vehicles. Github repo is available at \\href{https://github.com/JohnsonJiang1996/Awesome-VLA4AD}{SicongJiang/Awesome-VLA4AD}.",
    "title": "A Survey on Vision-Language-Action Models for Autonomous Driving",
    "date": "2025-06-30",
    "authors": "Sicong Jiang, Zilin Huang, Kangan Qian, Ziang Luo, Tianze Zhu, Yang Zhong, Yihong Tang, Menglin Kong, Yunlong Wang, Siwen Jiao, Hao Ye, Zihao Sheng, Xin Zhao, Tuopu Wen, Zheng Fu, Sikai Chen, Kun Jiang, Diange Yang, Seongjin Choi, Lijun Sun",
    "id": "2506.24044v1",
    "x": 0.4837258458137512,
    "y": 0.6960026025772095,
    "z": 1.406287431716919,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 0.4837258458137512,
        "y": 0.6960026025772095,
        "z": 1.406287431716919
      },
      "starmap": {
        "x": -0.020719677209854126,
        "y": 0.007044288795441389,
        "z": -0.0007373870466835797
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "abstract": "In this paper, we explore provable acceleration of diffusion models without any additional retraining. Focusing on the task of approximating a target data distribution in $\\mathbb{R}^d$ to within $\\varepsilon$ total-variation distance, we propose a principled, training-free sampling algorithm that requires only the order of   $$ d^{1+2/K} \\varepsilon^{-1/K} $$   score function evaluations (up to log factor) in the presence of accurate scores, where $K$ is an arbitrarily large fixed integer. This result applies to a broad class of target data distributions, without the need for assumptions such as smoothness or log-concavity. Our theory is robust vis-a-vis inexact score estimation, degrading gracefully as the score estimation error increases -- without demanding higher-order smoothness on the score estimates as assumed in previous work. The proposed algorithm draws insight from high-order ODE solvers, leveraging high-order Lagrange interpolation and successive refinement to approximate the integral derived from the probability flow ODE.",
    "title": "Faster Diffusion Models via Higher-Order Approximation",
    "authors": "Gen Li, Yuchen Zhou, Yuting Wei, Yuxin Chen",
    "id": "2506.24042v1",
    "x": 1.22587251663208,
    "y": -3.0213887691497803,
    "z": -1.2873797416687012,
    "cluster_id": 8,
    "layouts": {
      "pacmap": {
        "x": 1.22587251663208,
        "y": -3.0213887691497803,
        "z": -1.2873797416687012
      },
      "starmap": {
        "x": 0.001727337366901338,
        "y": -0.00885343924164772,
        "z": -0.013987639918923378
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "Unsupervised Sparse Coding-based Spiking Neural Network for Real-time Spike Sorting",
    "date": "2025-06-30",
    "abstract": "Spike sorting is a crucial step in decoding multichannel extracellular neural signals, enabling the identification of individual neuronal activity. A key challenge in brain-machine interfaces (BMIs) is achieving real-time, low-power spike sorting at the edge while keeping high neural decoding performance. This study introduces the Neuromorphic Sparse Sorter (NSS), a compact two-layer spiking neural network optimized for efficient spike sorting. NSS leverages the Locally Competitive Algorithm (LCA) for sparse coding to extract relevant features from noisy events with reduced computational demands. NSS learns to sort detected spike waveforms in an online fashion and operates entirely unsupervised. To exploit multi-bit spike coding capabilities of neuromorphic platforms like Intel's Loihi 2, a custom neuron model was implemented, enabling flexible power-performance trade-offs via adjustable spike bit-widths. Evaluations on simulated and real-world tetrode signals with biological drift showed NSS outperformed established pipelines such as WaveClus3 and PCA+KMeans. With 2-bit graded spikes, NSS on Loihi 2 outperformed NSS implemented with leaky integrate-and-fire neuron and achieved an F1-score of 77% (+10% improvement) while consuming 8.6mW (+1.65mW) when tested on a drifting recording, with a computational processing time of 0.25ms (+60 us) per inference.",
    "id": "2506.24041v1",
    "authors": "Alexis Melot, Sean U. N. Wood, Yannick Coffinier, Pierre Yger, Fabien Alibart",
    "x": -0.6549350619316101,
    "y": -2.378612518310547,
    "z": 0.8650347590446472,
    "cluster_id": 7,
    "layouts": {
      "pacmap": {
        "x": -0.6549350619316101,
        "y": -2.378612518310547,
        "z": 0.8650347590446472
      },
      "starmap": {
        "x": 0.009161018766462803,
        "y": -0.016280926764011383,
        "z": -0.013499586842954159
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "id": "2506.24039v1",
    "title": "Foundation Models for Zero-Shot Segmentation of Scientific Images without AI-Ready Data",
    "abstract": "Zero-shot and prompt-based technologies capitalized on using frequently occurring images to transform visual reasoning tasks, which explains why such technologies struggle with valuable yet scarce scientific image sets. In this work, we propose Zenesis, a comprehensive no-code interactive platform designed to minimize barriers posed by data readiness for scientific images. We develop lightweight multi-modal adaptation techniques that enable zero-shot operation on raw scientific data, along with human-in-the-loop refinement and heuristic-based temporal enhancement options. We demonstrate the performance of our approach through comprehensive comparison and validation on challenging Focused Ion Beam Scanning Electron Microscopy (FIB-SEM) data of catalyst-loaded membranes. Zenesis significantly outperforms baseline methods, achieving an average accuracy of 0.947, an Intersection over Union (IOU) of 0.858, and a Dice score of 0.923 for amorphous catalyst samples and accuracy of 0.987, an IOU of 0.857, and a Dice score of 0.923 for crystalline samples. These results mark a substantial improvement over traditional methods like Otsu thresholding and even advanced models like Segment Anything Model (SAM) when used in isolation. Our results demonstrate that Zenesis is a powerful tool for scientific applications, particularly in fields where high-quality annotated datasets are unavailable, accelerating accurate analysis of experimental imaging.",
    "authors": "Shubhabrata Mukherjee, Jack Lang, Obeen Kwon, Iryna Zenyuk, Valerie Brogden, Adam Weber, Daniela Ushizima",
    "x": 1.6215746402740479,
    "y": -1.1065914630889893,
    "z": -0.39927005767822266,
    "cluster_id": 5,
    "layouts": {
      "pacmap": {
        "x": 1.6215746402740479,
        "y": -1.1065914630889893,
        "z": -0.39927005767822266
      },
      "starmap": {
        "x": 0.005967414937913418,
        "y": -0.01688193529844284,
        "z": -0.010114300064742565
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.24034v1",
    "authors": "George Webber, Alexander Hammers, Andrew P King, Andrew J Reader",
    "abstract": "Diffusion models (DMs) have recently been introduced as a regularizing prior for PET image reconstruction, integrating DMs trained on high-quality PET images with unsupervised schemes that condition on measured data. While these approaches have potential generalization advantages due to their independence from the scanner geometry and the injected activity level, they forgo the opportunity to explicitly model the interaction between the DM prior and noisy measurement data, potentially limiting reconstruction accuracy. To address this, we propose a supervised DM-based algorithm for PET reconstruction. Our method enforces the non-negativity of PET's Poisson likelihood model and accommodates the wide intensity range of PET images. Through experiments on realistic brain PET phantoms, we demonstrate that our approach outperforms or matches state-of-the-art deep learning-based methods quantitatively across a range of dose levels. We further conduct ablation studies to demonstrate the benefits of the proposed components in our model, as well as its dependence on training data, parameter count, and number of diffusion steps. Additionally, we show that our approach enables more accurate posterior sampling than unsupervised DM-based methods, suggesting improved uncertainty estimation. Finally, we extend our methodology to a practical approach for fully 3D PET and present example results from real [$^{18}$F]FDG brain PET data.",
    "date": "2025-06-30",
    "title": "Supervised Diffusion-Model-Based PET Image Reconstruction",
    "x": -3.228863000869751,
    "y": -1.1830949783325195,
    "z": -0.7150952219963074,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -3.228863000869751,
        "y": -1.1830949783325195,
        "z": -0.7150952219963074
      },
      "starmap": {
        "x": 0.023999430239200592,
        "y": -0.007722680456936359,
        "z": 0.013122945092618465
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Yongyi Wang, Wenxin Li",
    "abstract": "In the domain of algorithmic decision-making, non-Markovian dynamics manifest as a significant impediment, especially for paradigms such as Reinforcement Learning (RL), thereby exerting far-reaching consequences on the advancement and effectiveness of the associated systems. Nevertheless, the existing benchmarks are deficient in comprehensively assessing the capacity of decision algorithms to handle non-Markovian dynamics. To address this deficiency, we have devised a generalized methodology grounded in category theory. Notably, we established the category of Markov Decision Processes (MDP) and the category of non-Markovian Decision Processes (NMDP), and proved the equivalence relationship between them. This theoretical foundation provides a novel perspective for understanding and addressing non-Markovian dynamics. We further introduced non-Markovianity into decision-making problem settings via the History Aggregator for State (HAS). With HAS, we can precisely control the state dependency structure of decision-making problems in the time series. Our analysis demonstrates the effectiveness of our method in representing a broad range of non-Markovian dynamics. This approach facilitates a more rigorous and flexible evaluation of decision algorithms by testing them in problem settings where non-Markovian dynamics are explicitly constructed.",
    "title": "Constructing Non-Markovian Decision Process via History Aggregator",
    "id": "2506.24026v1",
    "date": "2025-06-30",
    "x": 2.086432695388794,
    "y": 0.08769107609987259,
    "z": 1.6236389875411987,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 2.086432695388794,
        "y": 0.08769107609987259,
        "z": 1.6236389875411987
      },
      "starmap": {
        "x": -0.02098187990486622,
        "y": 0.007303233724087477,
        "z": 0.0026091376785188913
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.24024v1",
    "title": "Post-processing of EEG-based Auditory Attention Decoding Decisions via Hidden Markov Models",
    "date": "2025-06-30",
    "abstract": "Auditory attention decoding (AAD) algorithms exploit brain signals, such as electroencephalography (EEG), to identify which speaker a listener is focusing on in a multi-speaker environment. While state-of-the-art AAD algorithms can identify the attended speaker on short time windows, their predictions are often too inaccurate for practical use. In this work, we propose augmenting AAD with a hidden Markov model (HMM) that models the temporal structure of attention. More specifically, the HMM relies on the fact that a subject is much less likely to switch attention than to keep attending the same speaker at any moment in time. We show how a HMM can significantly improve existing AAD algorithms in both causal (real-time) and non-causal (offline) settings. We further demonstrate that HMMs outperform existing postprocessing approaches in both accuracy and responsiveness, and explore how various factors such as window length, switching frequency, and AAD accuracy influence overall performance. The proposed method is computationally efficient, intuitive to use and applicable in both real-time and offline settings.",
    "authors": "Nicolas Heintz, Tom Francart, Alexander Bertrand",
    "x": -1.9632689952850342,
    "y": -0.8056257367134094,
    "z": 0.8889734745025635,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -1.9632689952850342,
        "y": -0.8056257367134094,
        "z": 0.8889734745025635
      },
      "starmap": {
        "x": 0.0060819219797849655,
        "y": -0.009320812299847603,
        "z": 0.005950209219008684
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "authors": "Hongxin Zhang, Zheyuan Zhang, Zeyuan Wang, Zunzhe Zhang, Lixing Fang, Qinhong Zhou, Chuang Gan",
    "abstract": "We introduce Ella, an embodied social agent capable of lifelong learning within a community in a 3D open world, where agents accumulate experiences and acquire knowledge through everyday visual observations and social interactions. At the core of Ella's capabilities is a structured, long-term multimodal memory system that stores, updates, and retrieves information effectively. It consists of a name-centric semantic memory for organizing acquired knowledge and a spatiotemporal episodic memory for capturing multimodal experiences. By integrating this lifelong memory system with foundation models, Ella retrieves relevant information for decision-making, plans daily activities, builds social relationships, and evolves autonomously while coexisting with other intelligent beings in the open world. We conduct capability-oriented evaluations in a dynamic 3D open world where 15 agents engage in social activities for days and are assessed with a suite of unseen controlled evaluations. Experimental results show that Ella can influence, lead, and cooperate with other agents well to achieve goals, showcasing its ability to learn effectively through observation and social interaction. Our findings highlight the transformative potential of combining structured memory systems with foundation models for advancing embodied intelligence. More videos can be found at https://umass-embodied-agi.github.io/Ella/.",
    "title": "Ella: Embodied Social Agents with Lifelong Memory",
    "id": "2506.24019v1",
    "x": 0.9312916398048401,
    "y": 1.4754587411880493,
    "z": 1.9309154748916626,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 0.9312916398048401,
        "y": 1.4754587411880493,
        "z": 1.9309154748916626
      },
      "starmap": {
        "x": -0.022701658308506012,
        "y": 0.00844052154570818,
        "z": 7.51124243834056e-05
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.24018v1",
    "title": "Bridging Theory and Practice in Link Representation with Graph Neural Networks",
    "abstract": "Graph Neural Networks (GNNs) are widely used to compute representations of node pairs for downstream tasks such as link prediction. Yet, theoretical understanding of their expressive power has focused almost entirely on graph-level representations. In this work, we shift the focus to links and provide the first comprehensive study of GNN expressiveness in link representation. We introduce a unifying framework, the $k_\\phi$-$k_\\rho$-$m$ framework, that subsumes existing message-passing link models and enables formal expressiveness comparisons. Using this framework, we derive a hierarchy of state-of-the-art methods and offer theoretical tools to analyze future architectures. To complement our analysis, we propose a synthetic evaluation protocol comprising the first benchmark specifically designed to assess link-level expressiveness. Finally, we ask: does expressiveness matter in practice? We use a graph symmetry metric that quantifies the difficulty of distinguishing links and show that while expressive models may underperform on standard benchmarks, they significantly outperform simpler ones as symmetry increases, highlighting the need for dataset-aware model selection.",
    "authors": "Veronica Lachi, Francesco Ferrini, Antonio Longa, Bruno Lepri, Andrea Passerini, Manfred Jaeger",
    "date": "2025-06-30",
    "x": -0.2029324471950531,
    "y": -1.3688634634017944,
    "z": -2.1303200721740723,
    "cluster_id": 6,
    "layouts": {
      "pacmap": {
        "x": -0.2029324471950531,
        "y": -1.3688634634017944,
        "z": -2.1303200721740723
      },
      "starmap": {
        "x": 0.014952927827835083,
        "y": -0.0159181896597147,
        "z": -0.024989470839500427
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Recent advances in large language models and vision-language models have led to growing interest in explainable evaluation metrics for image captioning. However, these metrics generate explanations without standardized criteria, and the overall quality of the generated explanations remains unverified. In this paper, we propose EXPERT, a reference-free evaluation metric that provides structured explanations based on three fundamental criteria: fluency, relevance, and descriptiveness. By constructing large-scale datasets of high-quality structured explanations, we develop a two-stage evaluation template to effectively supervise a vision-language model for both scoring and explanation generation. EXPERT achieves state-of-the-art results on benchmark datasets while providing significantly higher-quality explanations than existing metrics, as validated through comprehensive human evaluation. Our code and datasets are available at https://github.com/hjkim811/EXPERT.",
    "authors": "Hyunjong Kim, Sangyeop Kim, Jongheon Jeong, Yeongjae Cho, Sungzoon Cho",
    "date": "2025-06-30",
    "title": "EXPERT: An Explainable Image Captioning Evaluation Metric with Structured Explanations",
    "id": "2506.24016v1",
    "x": 1.564706802368164,
    "y": 1.0497945547103882,
    "z": -1.215714931488037,
    "cluster_id": 3,
    "layouts": {
      "pacmap": {
        "x": 1.564706802368164,
        "y": 1.0497945547103882,
        "z": -1.215714931488037
      },
      "starmap": {
        "x": 0.007809879723936319,
        "y": -0.02968260645866394,
        "z": -0.019031574949622154
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "Bridging Physical and Digital Worlds: Embodied Large AI for Future Wireless Systems",
    "date": "2025-06-30",
    "abstract": "Large artificial intelligence (AI) models offer revolutionary potential for future wireless systems, promising unprecedented capabilities in network optimization and performance. However, current paradigms largely overlook crucial physical interactions. This oversight means they primarily rely on offline datasets, leading to difficulties in handling real-time wireless dynamics and non-stationary environments. Furthermore, these models often lack the capability for active environmental probing. This paper proposes a fundamental paradigm shift towards wireless embodied large AI (WELAI), moving from passive observation to active embodiment. We first identify key challenges faced by existing models, then we explore the design principles and system structure of WELAI. Besides, we outline prospective applications in next-generation wireless. Finally, through an illustrative case study, we demonstrate the effectiveness of WELAI and point out promising research directions for realizing adaptive, robust, and autonomous wireless systems.",
    "id": "2506.24009v1",
    "authors": "Xinquan Wang, Fenghao Zhu, Zhaohui Yang, Chongwen Huang, Xiaoming Chen, Zhaoyang Zhang, Sami Muhaidat, M\u00e9rouane Debbah",
    "x": 3.9255552291870117,
    "y": 0.17244493961334229,
    "z": 1.1445101499557495,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 3.9255552291870117,
        "y": 0.17244493961334229,
        "z": 1.1445101499557495
      },
      "starmap": {
        "x": -0.025435656309127808,
        "y": 0.009025915525853634,
        "z": -0.0012629845878109336
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.24007v1",
    "title": "Minimax and Bayes Optimal Best-arm Identification: Adaptive Experimental Design for Treatment Choice",
    "authors": "Masahiro Kato",
    "abstract": "This study investigates adaptive experimental design for treatment choice, also known as fixed-budget best-arm identification. We consider an adaptive procedure consisting of a treatment-allocation phase followed by a treatment-choice phase, and we design an adaptive experiment for this setup to efficiently identify the best treatment arm, defined as the one with the highest expected outcome. In our designed experiment, the treatment-allocation phase consists of two stages. The first stage is a pilot phase, where we allocate each treatment arm uniformly with equal proportions to eliminate clearly suboptimal arms and estimate outcome variances. In the second stage, we allocate treatment arms in proportion to the variances estimated in the first stage. After the treatment-allocation phase, the procedure enters the treatment-choice phase, where we choose the treatment arm with the highest sample mean as our estimate of the best treatment arm. We prove that this single design is simultaneously asymptotically minimax and Bayes optimal for the simple regret, with upper bounds that match our lower bounds up to exact constants. Therefore, our designed experiment achieves the sharp efficiency limits without requiring separate tuning for minimax and Bayesian objectives.",
    "date": "2025-06-30",
    "x": 3.7712044715881348,
    "y": -0.1349545121192932,
    "z": -1.3880937099456787,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": 3.7712044715881348,
        "y": -0.1349545121192932,
        "z": -1.3880937099456787
      },
      "starmap": {
        "x": -0.009266051463782787,
        "y": -0.003943045623600483,
        "z": -0.006073419936001301
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "Large Language Models Don't Make Sense of Word Problems. A Scoping Review from a Mathematics Education Perspective",
    "authors": "Anselm R. Strohmaier, Wim Van Dooren, Kathrin Se\u00dfler, Brian Greer, Lieven Verschaffel",
    "abstract": "The progress of Large Language Models (LLMs) like ChatGPT raises the question of how they can be integrated into education. One hope is that they can support mathematics learning, including word-problem solving. Since LLMs can handle textual input with ease, they appear well-suited for solving mathematical word problems. Yet their real competence, whether they can make sense of the real-world context, and the implications for classrooms remain unclear. We conducted a scoping review from a mathematics-education perspective, including three parts: a technical overview, a systematic review of word problems used in research, and a state-of-the-art empirical evaluation of LLMs on mathematical word problems. First, in the technical overview, we contrast the conceptualization of word problems and their solution processes between LLMs and students. In computer-science research this is typically labeled mathematical reasoning, a term that does not align with usage in mathematics education. Second, our literature review of 213 studies shows that the most popular word-problem corpora are dominated by s-problems, which do not require a consideration of realities of their real-world context. Finally, our evaluation of GPT-3.5-turbo, GPT-4o-mini, GPT-4.1, and o3 on 287 word problems shows that most recent LLMs solve these s-problems with near-perfect accuracy, including a perfect score on 20 problems from PISA. LLMs still showed weaknesses in tackling problems where the real-world context is problematic or non-sensical. In sum, we argue based on all three aspects that LLMs have mastered a superficial solution process but do not make sense of word problems, which potentially limits their value as instructional tools in mathematics classrooms.",
    "id": "2506.24006v1",
    "date": "2025-06-30",
    "x": 3.5534236431121826,
    "y": 1.1944093704223633,
    "z": 0.019015315920114517,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 3.5534236431121826,
        "y": 1.1944093704223633,
        "z": 0.019015315920114517
      },
      "starmap": {
        "x": -0.022530727088451385,
        "y": 0.007417449727654457,
        "z": 1.7326397937722504e-05
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "Provably Efficient and Agile Randomized Q-Learning",
    "id": "2506.24005v1",
    "date": "2025-06-30",
    "abstract": "While Bayesian-based exploration often demonstrates superior empirical performance compared to bonus-based methods in model-based reinforcement learning (RL), its theoretical understanding remains limited for model-free settings. Existing provable algorithms either suffer from computational intractability or rely on stage-wise policy updates which reduce responsiveness and slow down the learning process. In this paper, we propose a novel variant of Q-learning algorithm, refereed to as RandomizedQ, which integrates sampling-based exploration with agile, step-wise, policy updates, for episodic tabular RL. We establish an $\\widetilde{O}(\\sqrt{H^5SAT})$ regret bound, where $S$ is the number of states, $A$ is the number of actions, $H$ is the episode length, and $T$ is the total number of episodes. In addition, we present a logarithmic regret bound under a mild positive sub-optimality condition on the optimal Q-function. Empirically, RandomizedQ exhibits outstanding performance compared to existing Q-learning variants with both bonus-based and Bayesian-based exploration on standard benchmarks.",
    "authors": "He Wang, Xingyu Xu, Yuejie Chi",
    "x": 0.8382835388183594,
    "y": -2.9523062705993652,
    "z": -0.18473678827285767,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 0.8382835388183594,
        "y": -2.9523062705993652,
        "z": -0.18473678827285767
      },
      "starmap": {
        "x": -0.018002144992351532,
        "y": 0.006164400838315487,
        "z": -0.001381243229843676
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "id": "2506.24003v1",
    "title": "ShapeKit",
    "authors": "Junqi Liu, Dongli He, Wenxuan Li, Ningyu Wang, Alan L. Yuille, Zongwei Zhou",
    "abstract": "In this paper, we present a practical approach to improve anatomical shape accuracy in whole-body medical segmentation. Our analysis shows that a shape-focused toolkit can enhance segmentation performance by over 8%, without the need for model re-training or fine-tuning. In comparison, modifications to model architecture typically lead to marginal gains of less than 3%. Motivated by this observation, we introduce ShapeKit, a flexible and easy-to-integrate toolkit designed to refine anatomical shapes. This work highlights the underappreciated value of shape-based tools and calls attention to their potential impact within the medical segmentation community.",
    "x": -1.9859299659729004,
    "y": 0.697422206401825,
    "z": -1.7358871698379517,
    "cluster_id": 4,
    "layouts": {
      "pacmap": {
        "x": -1.9859299659729004,
        "y": 0.697422206401825,
        "z": -1.7358871698379517
      },
      "starmap": {
        "x": 0.02635231241583824,
        "y": -0.03428548201918602,
        "z": -0.003955889958888292
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Lijun Sheng, Jian Liang, Ran He, Zilei Wang, Tieniu Tan",
    "date": "2025-06-30",
    "abstract": "Test-time adaptation (TTA) methods have gained significant attention for enhancing the performance of vision-language models (VLMs) such as CLIP during inference, without requiring additional labeled data. However, current TTA researches generally suffer from major limitations such as duplication of baseline results, limited evaluation metrics, inconsistent experimental settings, and insufficient analysis. These problems hinder fair comparisons between TTA methods and obscure their practical strengths and weaknesses. To address these challenges, we introduce TTA-VLM, a comprehensive benchmark for evaluating TTA methods on VLMs. Our benchmark implements 8 episodic TTA and 7 online TTA methods within a unified and reproducible framework, and evaluates them across 15 widely used datasets. Unlike prior studies focused solely on CLIP, we extend the evaluation to SigLIP--a model trained with a Sigmoid loss--and include training-time tuning methods such as CoOp, MaPLe, and TeCoA to assess generality. Beyond classification accuracy, TTA-VLM incorporates various evaluation metrics, including robustness, calibration, out-of-distribution detection, and stability, enabling a more holistic assessment of TTA methods. Through extensive experiments, we find that 1) existing TTA methods produce limited gains compared to the previous pioneering work; 2) current TTA methods exhibit poor collaboration with training-time fine-tuning methods; 3) accuracy gains frequently come at the cost of reduced model trustworthiness. We release TTA-VLM to provide fair comparison and comprehensive evaluation of TTA methods for VLMs, and we hope it encourages the community to develop more reliable and generalizable TTA strategies.",
    "id": "2506.24000v1",
    "title": "The Illusion of Progress? A Critical Look at Test-Time Adaptation for Vision-Language Models",
    "x": -0.03331425040960312,
    "y": 1.3430038690567017,
    "z": -1.1463713645935059,
    "cluster_id": 3,
    "layouts": {
      "pacmap": {
        "x": -0.03331425040960312,
        "y": 1.3430038690567017,
        "z": -1.1463713645935059
      },
      "starmap": {
        "x": 0.013351007364690304,
        "y": -0.03489279747009277,
        "z": -0.0270529817789793
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23998v1",
    "abstract": "Congenital heart disease (CHD) presents complex, lifelong challenges often underrepresented in traditional clinical metrics. While unstructured narratives offer rich insights into patient and caregiver experiences, manual thematic analysis (TA) remains labor-intensive and unscalable. We propose a fully automated large language model (LLM) pipeline that performs end-to-end TA on clinical narratives, which eliminates the need for manual coding or full transcript review. Our system employs a novel multi-agent framework, where specialized LLM agents assume roles to enhance theme quality and alignment with human analysis. To further improve thematic relevance, we optionally integrate reinforcement learning from human feedback (RLHF). This supports scalable, patient-centered analysis of large qualitative datasets and allows LLMs to be fine-tuned for specific clinical contexts.",
    "title": "Auto-TA: Towards Scalable Automated Thematic Analysis (TA) via Multi-Agent Large Language Models with Reinforcement Learning",
    "date": "2025-06-30",
    "authors": "Seungjun Yi, Joakim Nguyen, Huimin Xu, Terence Lim, Andrew Well, Mia Markey, Ying Ding",
    "x": 2.249415397644043,
    "y": 0.789721667766571,
    "z": 1.465947151184082,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 2.249415397644043,
        "y": 0.789721667766571,
        "z": 1.465947151184082
      },
      "starmap": {
        "x": -0.02267243154346943,
        "y": 0.007365480065345764,
        "z": 0.0006075297715142369
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23996v1",
    "date": "2025-06-30",
    "authors": "Juan Maro\u00f1as",
    "abstract": "This document shows how to obtain the Jacobian and Hessian matrices of the Kullback-Leibler divergence between two multivariate Gaussian distributions, using the first and second-order differentials. The presented derivations are based on the theory presented by \\cite{magnus99}. I've also got great inspiration from some of the derivations in \\cite{minka}.   Since I pretend to be at most didactic, the document is split into a summary of results and detailed derivations on each of the elements involved, with specific references to the tricks used in the derivations, and to many of the underlying concepts.",
    "title": "The Jacobian and Hessian of the Kullback-Leibler Divergence between Multivariate Gaussian Distributions (Technical Report)",
    "x": 0.9950539469718933,
    "y": -3.606011390686035,
    "z": -1.0096460580825806,
    "cluster_id": 6,
    "layouts": {
      "pacmap": {
        "x": 0.9950539469718933,
        "y": -3.606011390686035,
        "z": -1.0096460580825806
      },
      "starmap": {
        "x": 0.00102426134981215,
        "y": -0.0002619768783915788,
        "z": -0.01133685652166605
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23995v1",
    "title": "STCLocker: Deadlock Avoidance Testing for Autonomous Driving Systems",
    "abstract": "Autonomous Driving System (ADS) testing is essential to ensure the safety and reliability of autonomous vehicles (AVs) before deployment. However, existing techniques primarily focus on evaluating ADS functionalities in single-AV settings. As ADSs are increasingly deployed in multi-AV traffic, it becomes crucial to assess their cooperative performance, particularly regarding deadlocks, a fundamental coordination failure in which multiple AVs enter a circular waiting state indefinitely, resulting in motion planning failures. Despite its importance, the cooperative capability of ADSs to prevent deadlocks remains insufficiently underexplored. To address this gap, we propose the first dedicated Spatio-Temporal Conflict-Guided Deadlock Avoidance Testing technique, STCLocker, for generating DeadLock Scenarios (DLSs), where a group of AVs controlled by the ADS under test are in a circular wait state. STCLocker consists of three key components: Deadlock Oracle, Conflict Feedback, and Conflict-aware Scenario Generation. Deadlock Oracle provides a reliable black-box mechanism for detecting deadlock cycles among multiple AVs within a given scenario. Conflict Feedback and Conflict-aware Scenario Generation collaborate to actively guide AVs into simultaneous competition over spatial conflict resources (i.e., shared passing regions) and temporal competitive behaviors (i.e., reaching the conflict region at the same time), thereby increasing the effectiveness of generating conflict-prone deadlocks. We evaluate STCLocker on two types of ADSs: Roach, an end-to-end ADS, and OpenCDA, a module-based ADS supporting cooperative communication. Experimental results show that, on average, STCLocker generates more DLS than the best-performing baseline.",
    "date": "2025-06-30",
    "authors": "Mingfei Cheng, Renzhi Wang, Xiaofei Xie, Yuan Zhou, Lei Ma",
    "x": -0.1310063898563385,
    "y": 1.0840837955474854,
    "z": 1.4105980396270752,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": -0.1310063898563385,
        "y": 1.0840837955474854,
        "z": 1.4105980396270752
      },
      "starmap": {
        "x": -0.01965813897550106,
        "y": 0.008818701840937138,
        "z": 0.0005020195967517793
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "The international refugee crisis deepens, exposing millions of dis placed children to extreme psychological trauma. This research suggests a com pact, AI-based framework for processing unstructured refugee health data and distilling knowledge on child mental health. We compare two Retrieval-Aug mented Generation (RAG) pipelines, Zephyr-7B-beta and DeepSeek R1-7B, to determine how well they process challenging humanitarian datasets while avoid ing hallucination hazards. By combining cutting-edge AI methods with migration research and child psychology, this study presents a scalable strategy to assist policymakers, mental health practitioners, and humanitarian agencies to better assist displaced children and recognize their mental wellbeing. In total, both the models worked properly but significantly Deepseek R1 is superior to Zephyr with an accuracy of answer relevance 0.91",
    "authors": "Aditya Shrivastava, Komal Gupta, Shraddha Arora",
    "id": "2506.23992v1",
    "date": "2025-06-30",
    "title": "Harnessing AI Agents to Advance Research on Refugee Child Mental Health",
    "x": 3.929243564605713,
    "y": -0.43343496322631836,
    "z": -0.06408357620239258,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 3.929243564605713,
        "y": -0.43343496322631836,
        "z": -0.06408357620239258
      },
      "starmap": {
        "x": -0.02444026991724968,
        "y": 0.007802457083016634,
        "z": -0.001028063241392374
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "Machine Understanding of Scientific Language",
    "abstract": "Scientific information expresses human understanding of nature. This knowledge is largely disseminated in different forms of text, including scientific papers, news articles, and discourse among people on social media. While important for accelerating our pursuit of knowledge, not all scientific text is faithful to the underlying science. As the volume of this text has burgeoned online in recent years, it has become a problem of societal importance to be able to identify the faithfulness of a given piece of scientific text automatically. This thesis is concerned with the cultivation of datasets, methods, and tools for machine understanding of scientific language, in order to analyze and understand science communication at scale. To arrive at this, I present several contributions in three areas of natural language processing and machine learning: automatic fact checking, learning with limited data, and scientific text processing. These contributions include new methods and resources for identifying check-worthy claims, adversarial claim generation, multi-source domain adaptation, learning from crowd-sourced labels, cite-worthiness detection, zero-shot scientific fact checking, detecting exaggerated scientific claims, and modeling degrees of information change in science communication. Critically, I demonstrate how the research outputs of this thesis are useful for effectively learning from limited amounts of scientific text in order to identify misinformative scientific statements and generate new insights into the science communication process",
    "id": "2506.23990v1",
    "authors": "Dustin Wright",
    "date": "2025-06-30",
    "x": 1.8809243440628052,
    "y": -1.2939640283584595,
    "z": -0.6054525971412659,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 1.8809243440628052,
        "y": -1.2939640283584595,
        "z": -0.6054525971412659
      },
      "starmap": {
        "x": -0.007955607958137989,
        "y": -0.004769434221088886,
        "z": -0.006088537629693747
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "StyleDrive: Towards Driving-Style Aware Benchmarking of End-To-End Autonomous Driving",
    "authors": "Ruiyang Hao, Bowen Jing, Haibao Yu, Zaiqing Nie",
    "date": "2025-06-30",
    "id": "2506.23982v1",
    "abstract": "While personalization has been explored in traditional autonomous driving systems, it remains largely overlooked in end-to-end autonomous driving (E2EAD), despite its growing prominence. This gap is critical, as user-aligned behavior is essential for trust, comfort, and widespread adoption of autonomous vehicles. A core challenge is the lack of large-scale real-world datasets annotated with diverse and fine-grained driving preferences, hindering the development and evaluation of personalized E2EAD models. In this work, we present the first large-scale real-world dataset enriched with annotations capturing diverse driving preferences, establishing a foundation for personalization in E2EAD. We extract static environmental features from real-world road topology and infer dynamic contextual cues using a fine-tuned visual language model (VLM), enabling consistent and fine-grained scenario construction. Based on these scenarios, we derive objective preference annotations through behavioral distribution analysis and rule-based heuristics. To address the inherent subjectivity of driving style, we further employ the VLM to generate subjective annotations by jointly modeling scene semantics and driver behavior. Final high-quality labels are obtained through a human-in-the-loop verification process that fuses both perspectives. Building on this dataset, we propose the first benchmark for evaluating personalized E2EAD models. We assess several state-of-the-art models with and without preference conditioning, demonstrating that incorporating personalized preferences results in behavior more aligned with human driving. Our work lays the foundation for personalized E2EAD by providing a standardized platform to systematically integrate human preferences into data-driven E2EAD systems, catalyzing future research in human-centric autonomy.",
    "x": 0.7458223104476929,
    "y": 0.8178657293319702,
    "z": 1.6621614694595337,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 0.7458223104476929,
        "y": 0.8178657293319702,
        "z": 1.6621614694595337
      },
      "starmap": {
        "x": -0.021077316254377365,
        "y": 0.00811352301388979,
        "z": 0.0012928026262670755
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Renren Jin, Tianhao Shen, Xinwei Wu, Dan Shi, Haoran Sun, Wuwei Huang, Quandong Wang, Wei Liu, Jian Luan, Bin Wang, Deyi Xiong",
    "title": "TaP: A Taxonomy-Guided Framework for Automated and Scalable Preference Data Generation",
    "id": "2506.23979v1",
    "date": "2025-06-30",
    "abstract": "Conducting supervised fine-tuning and preference fine-tuning on large language models (LLMs) requires high-quality datasets to improve their ability to follow instructions and align with human preferences and values. However, constructing such datasets is resource-intensive, and most available datasets for supervised and preference fine-tuning are in English. To address these challenges, we propose the \\underline{\\textbf{Ta}}xonomy-Guided \\underline{\\textbf{P}}reference Data Generation (TaP) framework, which facilitates automated and scalable construction of preference datasets across various languages. TaP is grounded in a structured taxonomy that allows fine-grained control over dataset composition, thereby ensuring both diversity and comprehensive coverage. We employ TaP-generated datasets to perform supervised and preference fine-tuning on various LLMs. Experimental results demonstrate that LLMs trained on TaP-generated datasets outperform those trained on existing open-source datasets. Remarkably, LLMs trained on TaP-generated datasets surpass the performance of those trained on an open-source dataset that is 180 times larger.",
    "x": 1.092561960220337,
    "y": 0.73586505651474,
    "z": -0.5492644906044006,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 1.092561960220337,
        "y": 0.73586505651474,
        "z": -0.5492644906044006
      },
      "starmap": {
        "x": -0.019054749980568886,
        "y": 0.005328193306922913,
        "z": -0.0008882001275196671
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "id": "2506.23978v1",
    "abstract": "While the Internet's core infrastructure was designed to be open and universal, today's application layer is dominated by closed, proprietary platforms. Open and interoperable APIs require significant investment, and market leaders have little incentive to enable data exchange that could erode their user lock-in. We argue that LLM-based agents fundamentally disrupt this status quo. Agents can automatically translate between data formats and interact with interfaces designed for humans: this makes interoperability dramatically cheaper and effectively unavoidable. We name this shift universal interoperability: the ability for any two digital services to exchange data seamlessly using AI-mediated adapters. Universal interoperability undermines monopolistic behaviours and promotes data portability. However, it can also lead to new security risks and technical debt. Our position is that the ML community should embrace this development while building the appropriate frameworks to mitigate the downsides. By acting now, we can harness AI to restore user freedom and competitive markets without sacrificing security.",
    "title": "LLM Agents Are the Antidote to Walled Gardens",
    "authors": "Samuele Marro, Philip Torr",
    "x": 2.2040293216705322,
    "y": 0.5368093252182007,
    "z": 1.1094682216644287,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 2.2040293216705322,
        "y": 0.5368093252182007,
        "z": 1.1094682216644287
      },
      "starmap": {
        "x": -0.02384796552360058,
        "y": 0.008909937925636768,
        "z": 0.0010685642482712865
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "title": "A Scalable Approach for Safe and Robust Learning via Lipschitz-Constrained Networks",
    "id": "2506.23977v1",
    "abstract": "Certified robustness is a critical property for deploying neural networks (NN) in safety-critical applications. A principle approach to achieving such guarantees is to constrain the global Lipschitz constant of the network. However, accurate methods for Lipschitz-constrained training often suffer from non-convex formulations and poor scalability due to reliance on global semidefinite programs (SDPs). In this letter, we propose a convex training framework that enforces global Lipschitz constraints via semidefinite relaxation. By reparameterizing the NN using loop transformation, we derive a convex admissibility condition that enables tractable and certifiable training. While the resulting formulation guarantees robustness, its scalability is limited by the size of global SDP. To overcome this, we develop a randomized subspace linear matrix inequalities (RS-LMI) approach that decomposes the global constraints into sketched layerwise constraints projected onto low-dimensional subspaces, yielding a smooth and memory-efficient training objective. Empirical results on MNIST, CIFAR-10, and ImageNet demonstrate that the proposed framework achieves competitive accuracy with significantly improved Lipschitz bounds and runtime performance.",
    "authors": "Zain ul Abdeen, Vassilis Kekatos, Ming Jin",
    "x": 0.9046611189842224,
    "y": -2.5829854011535645,
    "z": 0.41289687156677246,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": 0.9046611189842224,
        "y": -2.5829854011535645,
        "z": 0.41289687156677246
      },
      "starmap": {
        "x": 0.018528269603848457,
        "y": 0.012005437165498734,
        "z": -0.016940729692578316
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "Toward Simple and Robust Contrastive Explanations for Image Classification by Leveraging Instance Similarity and Concept Relevance",
    "authors": "Yuliia Kaidashova, Bettina Finzel, Ute Schmid",
    "date": "2025-06-30",
    "abstract": "Understanding why a classification model prefers one class over another for an input instance is the challenge of contrastive explanation. This work implements concept-based contrastive explanations for image classification by leveraging the similarity of instance embeddings and relevance of human-understandable concepts used by a fine-tuned deep learning model. Our approach extracts concepts with their relevance score, computes contrasts for similar instances, and evaluates the resulting contrastive explanations based on explanation complexity. Robustness is tested for different image augmentations. Two research questions are addressed: (1) whether explanation complexity varies across different relevance ranges, and (2) whether explanation complexity remains consistent under image augmentations such as rotation and noise. The results confirm that for our experiments higher concept relevance leads to shorter, less complex explanations, while lower relevance results in longer, more diffuse explanations. Additionally, explanations show varying degrees of robustness. The discussion of these findings offers insights into the potential of building more interpretable and robust AI systems.",
    "id": "2506.23975v1",
    "x": 1.1220593452453613,
    "y": 0.6360726952552795,
    "z": -1.3930813074111938,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": 1.1220593452453613,
        "y": 0.6360726952552795,
        "z": -1.3930813074111938
      },
      "starmap": {
        "x": 0.002564895199611783,
        "y": -0.01739579252898693,
        "z": -0.0025698591489344835
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Boyue Xu, Ruichao Hou, Tongwei Ren, Gangshan Wu",
    "title": "Visual and Memory Dual Adapter for Multi-Modal Object Tracking",
    "date": "2025-06-30",
    "id": "2506.23972v1",
    "abstract": "Prompt-learning-based multi-modal trackers have achieved promising progress by employing lightweight visual adapters to incorporate auxiliary modality features into frozen foundation models. However, existing approaches often struggle to learn reliable prompts due to limited exploitation of critical cues across frequency and temporal domains. In this paper, we propose a novel visual and memory dual adapter (VMDA) to construct more robust and discriminative representations for multi-modal tracking. Specifically, we develop a simple but effective visual adapter that adaptively transfers discriminative cues from auxiliary modality to dominant modality by jointly modeling the frequency, spatial, and channel-wise features. Additionally, we design the memory adapter inspired by the human memory mechanism, which stores global temporal cues and performs dynamic update and retrieval operations to ensure the consistent propagation of reliable temporal information across video sequences. Extensive experiments demonstrate that our method achieves state-of-the-art performance on the various multi-modal tracking tasks, including RGB-Thermal, RGB-Depth, and RGB-Event tracking. Code and models are available at https://github.com/xuboyue1999/mmtrack.git.",
    "x": -1.8273963928222656,
    "y": 0.6800858378410339,
    "z": 0.8976575136184692,
    "cluster_id": 3,
    "layouts": {
      "pacmap": {
        "x": -1.8273963928222656,
        "y": 0.6800858378410339,
        "z": 0.8976575136184692
      },
      "starmap": {
        "x": 0.019204629585146904,
        "y": -0.037655942142009735,
        "z": -0.019031334668397903
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "UMA: A Family of Universal Models for Atoms",
    "authors": "Brandon M. Wood, Misko Dzamba, Xiang Fu, Meng Gao, Muhammed Shuaibi, Luis Barroso-Luque, Kareem Abdelmaqsoud, Vahe Gharakhanyan, John R. Kitchin, Daniel S. Levine, Kyle Michel, Anuroop Sriram, Taco Cohen, Abhishek Das, Ammar Rizvi, Sushree Jagriti Sahoo, Zachary W. Ulissi, C. Lawrence Zitnick",
    "date": "2025-06-30",
    "abstract": "The ability to quickly and accurately compute properties from atomic simulations is critical for advancing a large number of applications in chemistry and materials science including drug discovery, energy storage, and semiconductor manufacturing. To address this need, Meta FAIR presents a family of Universal Models for Atoms (UMA), designed to push the frontier of speed, accuracy, and generalization. UMA models are trained on half a billion unique 3D atomic structures (the largest training runs to date) by compiling data across multiple chemical domains, e.g. molecules, materials, and catalysts. We develop empirical scaling laws to help understand how to increase model capacity alongside dataset size to achieve the best accuracy. The UMA small and medium models utilize a novel architectural design we refer to as mixture of linear experts that enables increasing model capacity without sacrificing speed. For example, UMA-medium has 1.4B parameters but only ~50M active parameters per atomic structure. We evaluate UMA models on a diverse set of applications across multiple domains and find that, remarkably, a single model without any fine-tuning can perform similarly or better than specialized models. We are releasing the UMA code, weights, and associated data to accelerate computational workflows and enable the community to continue to build increasingly capable AI models.",
    "id": "2506.23971v1",
    "x": 1.8900200128555298,
    "y": 0.6202155947685242,
    "z": 1.16061270236969,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": 1.8900200128555298,
        "y": 0.6202155947685242,
        "z": 1.16061270236969
      },
      "starmap": {
        "x": -0.008417141623795033,
        "y": -0.006296409759670496,
        "z": -0.00866860430687666
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "abstract": "Network data conforms to a wide range of rules that arise from protocols, design principles, and deployment decisions (e.g., a packet's queuing delay must be less than its end-to-end delay). Formalizing such rules as logic constraints can (i) improve the quality of synthetic data, (ii) reduce the brittleness of machine learning (ML) models, and (iii) improve semantic understanding of network measurements. However, these benefits remain out of reach if rule extraction is manual or solely reliant on ML, as both approaches yield incomplete, unreliable, and/or inaccurate rules.   This paper formulates rule extraction as a constraint modeling problem and introduces NetNomos that learns propositional logic constraints directly from raw network measurements. Constraint modeling in this domain is uniquely challenging due to the scale of the data, the inherent learning complexity and passive environment, and the lack of ground truth supervision. NetNomos addresses these challenges via a lattice-based search structured by constraint specificity and succinctness. Our approach reduces learning complexity from superquadratic to logarithmic and enables efficient traversal in combinatorial search space.   Our evaluations on diverse network datasets show that NetNomos learns all benchmark rules, including those associated with as little as 0.01% of data points, in under three hours. In contrast, baseline methods discover less than 25% of the rules and require several days to run. Through three case studies, we show that: NetNomos (i) finds rule violations in the outputs of all seven synthetic traffic generators, hence can be used to assess and guide their generation process; (ii) detects semantic differences in traffic, hence can be used for anomaly detection; and (iii) automatically finds rules used for telemetry imputation, hence can support monitoring through inference.",
    "id": "2506.23964v1",
    "title": "Learning Constraints Directly from Network Data",
    "authors": "Hongyu H\u00e8, Minhao Jin, Maria Apostolaki",
    "x": 0.5375714898109436,
    "y": -1.736557960510254,
    "z": 1.3524469137191772,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 0.5375714898109436,
        "y": -1.736557960510254,
        "z": 1.3524469137191772
      },
      "starmap": {
        "x": -0.019627919420599937,
        "y": 0.006495482288300991,
        "z": -0.002326351823285222
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Vannkinh Nom, Souhail Bakkali, Muhammad Muzzamil Luqman, Mickael Coustaty, Jean-Marc Ogier",
    "date": "2025-06-30",
    "id": "2506.23963v1",
    "title": "Evaluating the Impact of Khmer Font Types on Text Recognition",
    "abstract": "Text recognition is significantly influenced by font types, especially for complex scripts like Khmer. The variety of Khmer fonts, each with its unique character structure, presents challenges for optical character recognition (OCR) systems. In this study, we evaluate the impact of 19 randomly selected Khmer font types on text recognition accuracy using Pytesseract. The fonts include Angkor, Battambang, Bayon, Bokor, Chenla, Dangrek, Freehand, Kh Kompong Chhnang, Kh SN Kampongsom, Khmer, Khmer CN Stueng Songke, Khmer Savuth Pen, Metal, Moul, Odor MeanChey, Preah Vihear, Siemreap, Sithi Manuss, and iSeth First. Our comparison of OCR performance across these fonts reveals that Khmer, Odor MeanChey, Siemreap, Sithi Manuss, and Battambang achieve high accuracy, while iSeth First, Bayon, and Dangrek perform poorly. This study underscores the critical importance of font selection in optimizing Khmer text recognition and provides valuable insights for developing more robust OCR systems.",
    "x": 0.6750814914703369,
    "y": 2.4756264686584473,
    "z": -0.965897798538208,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": 0.6750814914703369,
        "y": 2.4756264686584473,
        "z": -0.965897798538208
      },
      "starmap": {
        "x": 0.009784533642232418,
        "y": -0.013683514669537544,
        "z": 0.003480460261926055
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "ADReFT: Adaptive Decision Repair for Safe Autonomous Driving via Reinforcement Fine-Tuning",
    "date": "2025-06-30",
    "id": "2506.23960v1",
    "abstract": "Autonomous Driving Systems (ADSs) continue to face safety-critical risks due to the inherent limitations in their design and performance capabilities. Online repair plays a crucial role in mitigating such limitations, ensuring the runtime safety and reliability of ADSs. Existing online repair solutions enforce ADS compliance by transforming unacceptable trajectories into acceptable ones based on predefined specifications, such as rule-based constraints or training datasets. However, these approaches often lack generalizability, adaptability and tend to be overly conservative, resulting in ineffective repairs that not only fail to mitigate safety risks sufficiently but also degrade the overall driving experience. To address this issue, we propose Adaptive Decision Repair (ADReFT), a novel and effective repair method that identifies safety-critical states through offline learning from failed tests and generates appropriate mitigation actions to improve ADS safety. Specifically, ADReFT incorporates a transformer-based model with two joint heads, State Monitor and Decision Adapter, designed to capture complex driving environment interactions to evaluate state safety severity and generate adaptive repair actions. Given the absence of oracles for state safety identification, we first pretrain ADReFT using supervised learning with coarse annotations, i.e., labeling states preceding violations as positive samples and others as negative samples. It establishes ADReFT's foundational capability to mitigate safety-critical violations, though it may result in somewhat conservative mitigation strategies. Therefore, we subsequently finetune ADReFT using reinforcement learning to improve its initial capability and generate more precise and contextually appropriate repair decisions. Our evaluation results illustrate that ADReFT achieves better repair performance.",
    "authors": "Mingfei Cheng, Xiaofei Xie, Renzhi Wang, Yuan Zhou, Ming Hu",
    "x": 0.3703802227973938,
    "y": 0.11055746674537659,
    "z": 2.1082701683044434,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 0.3703802227973938,
        "y": 0.11055746674537659,
        "z": 2.1082701683044434
      },
      "starmap": {
        "x": -0.022493893280625343,
        "y": 0.008769557811319828,
        "z": 0.0005108111072331667
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "abstract": "Millions of people in African countries face barriers to accessing healthcare due to language and literacy gaps. This research tackles this challenge by transforming complex medical documents -- in this case, prosthetic device user manuals -- into accessible formats for underserved populations. This case study in cross-cultural translation is particularly pertinent/relevant for communities that receive donated prosthetic devices but may not receive the accompanying user documentation. Or, if available online, may only be available in formats (e.g., language and readability) that are inaccessible to local populations (e.g., English-language, high resource settings/cultural context). The approach is demonstrated using the widely spoken Pidgin dialect, but our open-source framework has been designed to enable rapid and easy extension to other languages/dialects. This work presents an AI-powered framework designed to process and translate complex medical documents, e.g., user manuals for prosthetic devices, into marginalised languages. The system enables users -- such as healthcare workers or patients -- to upload English-language medical equipment manuals, pose questions in their native language, and receive accurate, localised answers in real time. Technically, the system integrates a Retrieval-Augmented Generation (RAG) pipeline for processing and semantic understanding of the uploaded manuals. It then employs advanced Natural Language Processing (NLP) models for generative question-answering and multilingual translation. Beyond simple translation, it ensures accessibility to device instructions, treatment protocols, and safety information, empowering patients and clinicians to make informed healthcare decisions.",
    "title": "Bridging the Gap with Retrieval-Augmented Generation: Making Prosthetic Device User Manuals Available in Marginalised Languages",
    "authors": "Ikechukwu Ogbonna, Lesley Davidson, Soumya Banerjee, Abhishek Dasgupta, Laurence Kenney, Vikranth Harthikote Nagaraja",
    "id": "2506.23958v1",
    "x": 1.6967495679855347,
    "y": 1.1651524305343628,
    "z": -0.9615092873573303,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 1.6967495679855347,
        "y": 1.1651524305343628,
        "z": -0.9615092873573303
      },
      "starmap": {
        "x": -0.021372104063630104,
        "y": 0.007673466578125954,
        "z": 0.0033278586342930794
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Video stabilization is pivotal for video processing, as it removes unwanted shakiness while preserving the original user motion intent. Existing approaches, depending on the domain they operate, suffer from several issues (e.g. geometric distortions, excessive cropping, poor generalization) that degrade the user experience. To address these issues, we introduce \\textbf{GaVS}, a novel 3D-grounded approach that reformulates video stabilization as a temporally-consistent `local reconstruction and rendering' paradigm. Given 3D camera poses, we augment a reconstruction model to predict Gaussian Splatting primitives, and finetune it at test-time, with multi-view dynamics-aware photometric supervision and cross-frame regularization, to produce temporally-consistent local reconstructions. The model are then used to render each stabilized frame. We utilize a scene extrapolation module to avoid frame cropping. Our method is evaluated on a repurposed dataset, instilled with 3D-grounded information, covering samples with diverse camera motions and scene dynamics. Quantitatively, our method is competitive with or superior to state-of-the-art 2D and 2.5D approaches in terms of conventional task metrics and new geometry consistency. Qualitatively, our method produces noticeably better results compared to alternatives, validated by the user study.",
    "id": "2506.23957v1",
    "title": "GaVS: 3D-Grounded Video Stabilization via Temporally-Consistent Local Reconstruction and Rendering",
    "authors": "Zinuo You, Stamatios Georgoulis, Anpei Chen, Siyu Tang, Dengxin Dai",
    "date": "2025-06-30",
    "x": -2.277796506881714,
    "y": 2.4382379055023193,
    "z": -0.0017653327668085694,
    "cluster_id": 1,
    "layouts": {
      "pacmap": {
        "x": -2.277796506881714,
        "y": 2.4382379055023193,
        "z": -0.0017653327668085694
      },
      "starmap": {
        "x": 0.04679808393120766,
        "y": 0.06251025199890137,
        "z": -0.02319890819489956
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "AI systems increasingly support human decision-making across domains of professional, skill-based, and personal activity. While previous work has examined how AI might affect human autonomy globally, the effects of AI on domain-specific autonomy -- the capacity for self-governed action within defined realms of skill or expertise -- remain understudied. We analyze how AI decision-support systems affect two key components of domain-specific autonomy: skilled competence (the ability to make informed judgments within one's domain) and authentic value-formation (the capacity to form genuine domain-relevant values and preferences). By engaging with prior investigations and analyzing empirical cases across medical, financial, and educational domains, we demonstrate how the absence of reliable failure indicators and the potential for unconscious value shifts can erode domain-specific autonomy both immediately and over time. We then develop a constructive framework for autonomy-preserving AI support systems. We propose specific socio-technical design patterns -- including careful role specification, implementation of defeater mechanisms, and support for reflective practice -- that can help maintain domain-specific autonomy while leveraging AI capabilities. This framework provides concrete guidance for developing AI systems that enhance rather than diminish human agency within specialized domains of action.",
    "id": "2506.23952v2",
    "authors": "Stefan Buijsman, Sarah Carter, Juan Pablo Berm\u00fadez",
    "title": "Autonomy by Design: Preserving Human Autonomy in AI Decision-Support",
    "date": "2025-06-30",
    "x": 2.362375259399414,
    "y": -0.24133121967315674,
    "z": 1.0606871843338013,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 2.362375259399414,
        "y": -0.24133121967315674,
        "z": 1.0606871843338013
      },
      "starmap": {
        "x": -0.020695893093943596,
        "y": 0.005475196987390518,
        "z": -0.0007010773988440633
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Sparse Autoencoders (SAEs) have been successfully used to probe Large Language Models (LLMs) and extract interpretable concepts from their internal representations. These concepts are linear combinations of neuron activations that correspond to human-interpretable features. In this paper, we investigate the effectiveness of SAE-based explainability approaches for sentence classification, a domain where such methods have not been extensively explored. We present a novel SAE-based architecture tailored for text classification, leveraging a specialized classifier head and incorporating an activation rate sparsity loss. We benchmark this architecture against established methods such as ConceptShap, Independent Component Analysis, and other SAE-based concept extraction techniques. Our evaluation covers two classification benchmarks and four fine-tuned LLMs from the Pythia family. We further enrich our analysis with two novel metrics for measuring the precision of concept-based explanations, using an external sentence encoder. Our empirical results show that our architecture improves both the causality and interpretability of the extracted features.",
    "authors": "Mathis Le Bail, J\u00e9r\u00e9mie Dentan, Davide Buscaldi, Sonia Vanier",
    "date": "2025-06-30",
    "title": "Unveiling Decision-Making in LLMs for Text Classification : Extraction of influential and interpretable concepts with Sparse Autoencoders",
    "id": "2506.23951v1",
    "x": 1.6723440885543823,
    "y": 0.6510553956031799,
    "z": -1.634832739830017,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 1.6723440885543823,
        "y": 0.6510553956031799,
        "z": -1.634832739830017
      },
      "starmap": {
        "x": -0.014825067482888699,
        "y": 0.0035237055271863937,
        "z": -0.0008679297752678394
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Increasingly multi-purpose AI models, such as cutting-edge large language models or other 'general-purpose AI' (GPAI) models, 'foundation models,' generative AI models, and 'frontier models' (typically all referred to hereafter with the umbrella term 'GPAI/foundation models' except where greater specificity is needed), can provide many beneficial capabilities but also risks of adverse events with profound consequences. This document provides risk-management practices or controls for identifying, analyzing, and mitigating risks of GPAI/foundation models. We intend this document primarily for developers of large-scale, state-of-the-art GPAI/foundation models; others that can benefit from this guidance include downstream developers of end-use applications that build on a GPAI/foundation model. This document facilitates conformity with or use of leading AI risk management-related standards, adapting and building on the generic voluntary guidance in the NIST AI Risk Management Framework and ISO/IEC 23894, with a focus on the unique issues faced by developers of GPAI/foundation models.",
    "title": "AI Risk-Management Standards Profile for General-Purpose AI (GPAI) and Foundation Models",
    "id": "2506.23949v1",
    "authors": "Anthony M. Barrett, Jessica Newman, Brandie Nonnecke, Nada Madkour, Dan Hendrycks, Evan R. Murphy, Krystal Jackson, Deepika Raman",
    "date": "2025-06-30",
    "x": 3.2117319107055664,
    "y": -0.5142866373062134,
    "z": 1.2768605947494507,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 3.2117319107055664,
        "y": -0.5142866373062134,
        "z": 1.2768605947494507
      },
      "starmap": {
        "x": -0.022723553702235222,
        "y": 0.006975906901061535,
        "z": -0.001627536490559578
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23944v2",
    "date": "2025-06-30",
    "title": "Adapt Your Body: Mitigating Proprioception Shifts in Imitation Learning",
    "abstract": "Imitation learning models for robotic tasks typically rely on multi-modal inputs, such as RGB images, language, and proprioceptive states. While proprioception is intuitively important for decision-making and obstacle avoidance, simply incorporating all proprioceptive states leads to a surprising degradation in imitation learning performance. In this work, we identify the underlying issue as the proprioception shift problem, where the distributions of proprioceptive states diverge significantly between training and deployment. To address this challenge, we propose a domain adaptation framework that bridges the gap by utilizing rollout data collected during deployment. Using Wasserstein distance, we quantify the discrepancy between expert and rollout proprioceptive states and minimize this gap by adding noise to both sets of states, proportional to the Wasserstein distance. This strategy enhances robustness against proprioception shifts by aligning the training and deployment distributions. Experiments on robotic manipulation tasks demonstrate the efficacy of our method, enabling the imitation policy to leverage proprioception while mitigating its adverse effects. Our approach outperforms the naive solution which discards proprioception, and other baselines designed to address distributional shifts.",
    "authors": "Fuhang Kuang, Jiacheng You, Yingdong Hu, Tong Zhang, Chuan Wen, Yang Gao",
    "x": -0.6959169507026672,
    "y": -0.7632722854614258,
    "z": 2.1194121837615967,
    "cluster_id": 6,
    "layouts": {
      "pacmap": {
        "x": -0.6959169507026672,
        "y": -0.7632722854614258,
        "z": 2.1194121837615967
      },
      "starmap": {
        "x": 0.0066833109594881535,
        "y": -0.016554003581404686,
        "z": -0.010579557158052921
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "id": "2506.23940v2",
    "title": "Graft: Integrating the Domain Knowledge via Efficient Parameter Synergy for MLLMs",
    "authors": "Yang Dai, Jianxiang An, Tianwei Lin, Hongyang He, Hongzhe Huang, Wenqiao Zhang, Zheqi Lv, Siliang Tang, Yueting Zhuang",
    "abstract": "Multimodal Large Language Models (MLLMs) have achieved success across various domains. However, their applicability tends to degrade when confronted with different types of data inputs, especially for MLLMs that have been fine-tuned for specific tasks. Despite its importance, the study of knowledge sharing among domain-specific MLLMs--such as those trained for mathematics or code--remains largely underexplored. To address the fragmentation of knowledge across domain-specialized MLLMs, we propose a unified parameter integration framework that enables modular composition of expert capabilities. Our method is grounded in a novel Compatibility-Aware Parameter Splicing (CAPS) strategy, which leverages both local functional attribution and global information-theoretic signals to guide selective parameter fusion. By extending this mechanism to the low-rank adaptation layer granularity, we ensure efficient integration with minimal inference overhead. Furthermore, we introduce a domain compatibility scoring mechanism that quantifies inter-expert alignment at the activation level and correlates with downstream task utility. This principled fusion protocol allows the final model to synergize heterogeneous expertise while preserving structural modularity. Extensive evaluations across diverse multimodal benchmarks validate the effectiveness of our framework, offering a scalable path toward compositional, domain-adaptive MLLMs.",
    "x": -0.47711387276649475,
    "y": -0.9430398344993591,
    "z": -0.940859317779541,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -0.47711387276649475,
        "y": -0.9430398344993591,
        "z": -0.940859317779541
      },
      "starmap": {
        "x": 0.01216468308120966,
        "y": -0.025163136422634125,
        "z": -0.01673879101872444
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "As machine learning inferences increasingly move to edge devices, adapting to diverse computational capabilities, hardware, and memory constraints becomes more critical. Instead of relying on a pre-trained model fixed for all future inference queries across diverse edge devices, we argue that planning an inference pattern with a request-specific model tailored to the device's computational capacity, accuracy requirements, and time constraints is more cost-efficient and robust to diverse scenarios. To this end, we propose an accuracy-aware and workload-balanced inference system that integrates joint model quantization and inference partitioning. In this approach, the server dynamically responds to inference queries by sending a quantized model and adaptively sharing the inference workload with the device. Meanwhile, the device's computational power, channel capacity, and accuracy requirements are considered when deciding.   Furthermore, we introduce a new optimization framework for the inference system, incorporating joint model quantization and partitioning. Our approach optimizes layer-wise quantization bit width and partition points to minimize time consumption and cost while accounting for varying accuracy requirements of tasks through an accuracy degradation metric in our optimization model. To our knowledge, this work represents the first exploration of optimizing quantization layer-wise bit-width in the inference serving system, by introducing theoretical measurement of accuracy degradation. Simulation results demonstrate a substantial reduction in overall time and power consumption, with computation payloads decreasing by over 80% and accuracy degradation kept below 1%.",
    "date": "2025-06-30",
    "authors": "Xiangchen Li, Saeid Ghafouri, Bo Ji, Hans Vandierendonck, Deepu John, Dimitrios S. Nikolopoulos",
    "title": "QPART: Adaptive Model Quantization and Dynamic Workload Balancing for Accuracy-aware Edge Inference",
    "id": "2506.23934v1",
    "x": 0.05103996768593788,
    "y": -1.363227128982544,
    "z": 0.6199833154678345,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": 0.05103996768593788,
        "y": -1.363227128982544,
        "z": 0.6199833154678345
      },
      "starmap": {
        "x": 0.01017157081514597,
        "y": -0.018104059621691704,
        "z": -0.008490975946187973
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "Leveraging the Potential of Prompt Engineering for Hate Speech Detection in Low-Resource Languages",
    "authors": "Ruhina Tabasshum Prome, Tarikul Islam Tamiti, Anomadarshi Barua",
    "abstract": "The rapid expansion of social media leads to a marked increase in hate speech, which threatens personal lives and results in numerous hate crimes. Detecting hate speech presents several challenges: diverse dialects, frequent code-mixing, and the prevalence of misspelled words in user-generated content on social media platforms. Recent progress in hate speech detection is typically concentrated on high-resource languages. However, low-resource languages still face significant challenges due to the lack of large-scale, high-quality datasets. This paper investigates how we can overcome this limitation via prompt engineering on large language models (LLMs) focusing on low-resource Bengali language. We investigate six prompting strategies - zero-shot prompting, refusal suppression, flattering the classifier, multi-shot prompting, role prompting, and finally our innovative metaphor prompting to detect hate speech effectively in low-resource languages. We pioneer the metaphor prompting to circumvent the built-in safety mechanisms of LLMs that marks a significant departure from existing jailbreaking methods. We investigate all six different prompting strategies on the Llama2-7B model and compare the results extensively with three pre-trained word embeddings - GloVe, Word2Vec, and FastText for three different deep learning models - multilayer perceptron (MLP), convolutional neural network (CNN), and bidirectional gated recurrent unit (BiGRU). To prove the effectiveness of our metaphor prompting in the low-resource Bengali language, we also evaluate it in another low-resource language - Hindi, and two high-resource languages - English and German. The performance of all prompting techniques is evaluated using the F1 score, and environmental impact factor (IF), which measures CO$_2$ emissions, electricity usage, and computational time.",
    "date": "2025-06-30",
    "id": "2506.23930v1",
    "x": 1.6636875867843628,
    "y": 1.2300326824188232,
    "z": -0.6997349858283997,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 1.6636875867843628,
        "y": 1.2300326824188232,
        "z": -0.6997349858283997
      },
      "starmap": {
        "x": -0.02131281979382038,
        "y": 0.007257547229528427,
        "z": 0.004338009748607874
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "IMPACT: Inflectional Morphology Probes Across Complex Typologies",
    "date": "2025-06-30",
    "abstract": "Large Language Models (LLMs) have shown significant progress on various multilingual benchmarks and are increasingly used to generate and evaluate text in non-English languages. However, while they may produce fluent outputs, it remains unclear to what extent these models truly grasp the underlying linguistic complexity of those languages, particularly in morphology. To investigate this, we introduce IMPACT, a synthetically generated evaluation framework focused on inflectional morphology, which we publicly release, designed to evaluate LLM performance across five morphologically rich languages: Arabic, Russian, Finnish, Turkish, and Hebrew. IMPACT includes unit-test-style cases covering both shared and language-specific phenomena, from basic verb inflections (e.g., tense, number, gender) to unique features like Arabic's reverse gender agreement and vowel harmony in Finnish and Turkish. We assess eight multilingual LLMs that, despite strong English performance, struggle with other languages and uncommon morphological patterns, especially when judging ungrammatical examples. We also show that Chain of Thought and Thinking Models can degrade performance. Our work exposes gaps in LLMs' handling of linguistic complexity, pointing to clear room for improvement. To support further research, we publicly release the IMPACT framework.",
    "authors": "Mohammed J. Saeed, Tommi Vehvilainen, Evgeny Fedoseev, Sevil Caliskan, Tatiana Vodolazova",
    "id": "2506.23929v1",
    "x": 2.409320116043091,
    "y": 1.401036024093628,
    "z": -0.7440643310546875,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 2.409320116043091,
        "y": 1.401036024093628,
        "z": -0.7440643310546875
      },
      "starmap": {
        "x": -0.021747248247265816,
        "y": 0.0066717322915792465,
        "z": 0.0015928844222798944
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Junping Wang, Bicheng Wang, Yibo Xuea, Yuan Xie",
    "date": "2025-06-30",
    "title": "Industrial brain: a human-like autonomous neuro-symbolic cognitive decision-making system",
    "id": "2506.23926v1",
    "abstract": "Resilience non-equilibrium measurement, the ability to maintain fundamental functionality amidst failures and errors, is crucial for scientific management and engineering applications of industrial chain. The problem is particularly challenging when the number or types of multiple co-evolution of resilience (for example, randomly placed) are extremely chaos. Existing end-to-end deep learning ordinarily do not generalize well to unseen full-feld reconstruction of spatiotemporal co-evolution structure, and predict resilience of network topology, especially in multiple chaos data regimes typically seen in real-world applications. To address this challenge, here we propose industrial brain, a human-like autonomous cognitive decision-making and planning framework integrating higher-order activity-driven neuro network and CT-OODA symbolic reasoning to autonomous plan resilience directly from observational data of global variable. The industrial brain not only understands and model structure of node activity dynamics and network co-evolution topology without simplifying assumptions, and reveal the underlying laws hidden behind complex networks, but also enabling accurate resilience prediction, inference, and planning. Experimental results show that industrial brain significantly outperforms resilience prediction and planning methods, with an accurate improvement of up to 10.8\\% over GoT and OlaGPT framework and 11.03\\% over spectral dimension reduction. It also generalizes to unseen topologies and dynamics and maintains robust performance despite observational disturbances. Our findings suggest that industrial brain addresses an important gap in resilience prediction and planning for industrial chain.",
    "x": -1.953086256980896,
    "y": -2.220693826675415,
    "z": -0.6274946928024292,
    "cluster_id": 7,
    "layouts": {
      "pacmap": {
        "x": -1.953086256980896,
        "y": -2.220693826675415,
        "z": -0.6274946928024292
      },
      "starmap": {
        "x": 0.009754748083651066,
        "y": -0.020420189946889877,
        "z": -0.00825966615229845
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23924v1",
    "date": "2025-06-30",
    "authors": "Akshit Kumar, Tianyi Peng, Yuhang Wu, Assaf Zeevi",
    "abstract": "Large language models (LLMs) have exhibited expert-level capabilities across various domains. However, their abilities to solve problems in Operations Research (OR) -- the analysis and optimization of mathematical models derived from real-world problems or their verbal descriptions -- remain underexplored. In this work, we take a first step toward evaluating LLMs' abilities to solve stochastic modeling problems, a core class of OR problems characterized by uncertainty and typically involving tools from probability, statistics, and stochastic processes. We manually procure a representative set of graduate-level homework and doctoral qualification-exam problems and test LLMs' abilities to solve them. We further leverage SimOpt, an open-source library of simulation-optimization problems and solvers, to investigate LLMs' abilities to make real-world decisions under uncertainty. Our results show that, though a nontrivial amount of work is still needed to reliably automate the stochastic modeling pipeline in reality, state-of-the-art LLMs demonstrate proficiency on par with human experts in both classroom and practical settings. These findings highlight the potential of building AI agents that assist OR researchers and amplify the real-world impact of OR through automation.",
    "title": "Performance of LLMs on Stochastic Modeling Operations Research Problems: From Theory to Practice",
    "x": 3.1153111457824707,
    "y": 1.360306739807129,
    "z": -0.6601244211196899,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 3.1153111457824707,
        "y": 1.360306739807129,
        "z": -0.6601244211196899
      },
      "starmap": {
        "x": -0.022525126114487648,
        "y": 0.0069060600362718105,
        "z": -0.001560213859193027
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Resin infusion (RI) and resin transfer moulding (RTM) are critical processes for the manufacturing of high-performance fibre-reinforced polymer composites, particularly for large-scale applications such as wind turbine blades. Controlling the resin flow dynamics in these processes is critical to ensure the uniform impregnation of the fibre reinforcements, thereby preventing residual porosities and dry spots that impact the consequent structural integrity of the final component. This paper presents a reinforcement learning (RL) based strategy, established using process simulations, for synchronising the different resin flow fronts in an infusion scenario involving two resin inlets and a single outlet. Using Proximal Policy Optimisation (PPO), our approach addresses the challenge of managing the fluid dynamics in a partially observable environment. The results demonstrate the effectiveness of the RL approach in achieving an accurate flow convergence, highlighting its potential towards improving process control and product quality in composites manufacturing.",
    "title": "Reinforcement Learning for Synchronised Flow Control in a Dual-Gate Resin Infusion System",
    "date": "2025-06-30",
    "id": "2506.23923v1",
    "authors": "Miguel Camacho-S\u00e1nchez, Fernando Garc\u00eda-Torres, Jesper John Lisegaard, Roc\u00edo del Amor, Sankhya Mohanty, Valery Naranjo",
    "x": -1.821380376815796,
    "y": -1.0024127960205078,
    "z": 1.3191454410552979,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": -1.821380376815796,
        "y": -1.0024127960205078,
        "z": 1.3191454410552979
      },
      "starmap": {
        "x": -0.016212204471230507,
        "y": 0.00694868853315711,
        "z": 0.00823880359530449
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "We often attribute human characteristics to large language models (LLMs) and claim that they \"know\" certain things. LLMs have an internal probabilistic knowledge that represents information retained during training. How can we assess the veracity of this knowledge? We examine two common methods for probing the veracity of LLMs and discover several assumptions that are flawed. To address these flawed assumptions, we introduce sAwMIL (short for Sparse Aware Multiple-Instance Learning), a probing method that utilizes the internal activations of LLMs to separate statements into true, false, and neither. sAwMIL is based on multiple-instance learning and conformal prediction. We evaluate sAwMIL on 5 validity criteria across 16 open-source LLMs, including both default and chat-based variants, as well as on 3 new datasets. Among the insights we provide are: (1) the veracity signal is often concentrated in the third quarter of an LLM's depth; (2) truth and falsehood signals are not always symmetric; (3) linear probes perform better on chat models than on default models; (4) nonlinear probes may be required to capture veracity signals for some LLMs with reinforcement learning from human feedback or knowledge distillation; and (5) LLMs capture a third type of signal that is distinct from true and false and is neither true nor false. These findings provide a reliable method for verifying what LLMs \"know\" and how certain they are of their probabilistic internal knowledge.",
    "authors": "Germans Savcisens, Tina Eliassi-Rad",
    "title": "The Trilemma of Truth in Large Language Models",
    "id": "2506.23921v1",
    "date": "2025-06-30",
    "x": 2.8699004650115967,
    "y": 1.5684325695037842,
    "z": -0.3294176459312439,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 2.8699004650115967,
        "y": 1.5684325695037842,
        "z": -0.3294176459312439
      },
      "starmap": {
        "x": -0.023499680683016777,
        "y": 0.008071917109191418,
        "z": -0.0009209916461259127
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Recent progress in multimodal reasoning has been significantly advanced by textual Chain-of-Thought (CoT), a paradigm where models conduct reasoning within language. This text-centric approach, however, treats vision as a static, initial context, creating a fundamental \"semantic gap\" between rich perceptual data and discrete symbolic thought. Human cognition often transcends language, utilizing vision as a dynamic mental sketchpad. A similar evolution is now unfolding in AI, marking a fundamental paradigm shift from models that merely think about images to those that can truly think with images. This emerging paradigm is characterized by models leveraging visual information as intermediate steps in their thought process, transforming vision from a passive input into a dynamic, manipulable cognitive workspace. In this survey, we chart this evolution of intelligence along a trajectory of increasing cognitive autonomy, which unfolds across three key stages: from external tool exploration, through programmatic manipulation, to intrinsic imagination. To structure this rapidly evolving field, our survey makes four key contributions. (1) We establish the foundational principles of the think with image paradigm and its three-stage framework. (2) We provide a comprehensive review of the core methods that characterize each stage of this roadmap. (3) We analyze the critical landscape of evaluation benchmarks and transformative applications. (4) We identify significant challenges and outline promising future directions. By providing this structured overview, we aim to offer a clear roadmap for future research towards more powerful and human-aligned multimodal AI.",
    "date": "2025-06-30",
    "id": "2506.23918v2",
    "title": "Thinking with Images for Multimodal Reasoning: Foundations, Methods, and Future Frontiers",
    "authors": "Zhaochen Su, Peng Xia, Hangyu Guo, Zhenhua Liu, Yan Ma, Xiaoye Qu, Jiaqi Liu, Yanshu Li, Kaide Zeng, Zhengyuan Yang, Linjie Li, Yu Cheng, Heng Ji, Junxian He, Yi R. Fung",
    "x": 3.0388123989105225,
    "y": 0.9103766679763794,
    "z": 0.09974958002567291,
    "cluster_id": 3,
    "layouts": {
      "pacmap": {
        "x": 3.0388123989105225,
        "y": 0.9103766679763794,
        "z": 0.09974958002567291
      },
      "starmap": {
        "x": 0.0010364450281485915,
        "y": -0.021606912836432457,
        "z": -0.0168538186699152
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "abstract": "Deep learning (DL) methods are increasingly outperforming classical approaches in brain imaging, yet their generalizability across diverse imaging cohorts remains inadequately assessed. As age and sex are key neurobiological markers in clinical neuroscience, influencing brain structure and disease risk, this study evaluates three of the existing three-dimensional architectures, namely Simple Fully Connected Network (SFCN), DenseNet, and Shifted Window (Swin) Transformers, for age and sex prediction using T1-weighted MRI from four independent cohorts: UK Biobank (UKB, n=47,390), Dallas Lifespan Brain Study (DLBS, n=132), Parkinson's Progression Markers Initiative (PPMI, n=108 healthy controls), and Information eXtraction from Images (IXI, n=319). We found that SFCN consistently outperformed more complex architectures with AUC of 1.00 [1.00-1.00] in UKB (internal test set) and 0.85-0.91 in external test sets for sex classification. For the age prediction task, SFCN demonstrated a mean absolute error (MAE) of 2.66 (r=0.89) in UKB and 4.98-5.81 (r=0.55-0.70) across external datasets. Pairwise DeLong and Wilcoxon signed-rank tests with Bonferroni corrections confirmed SFCN's superiority over Swin Transformer across most cohorts (p<0.017, for three comparisons). Explainability analysis further demonstrates the regional consistency of model attention across cohorts and specific to each task. Our findings reveal that simpler convolutional networks outperform the denser and more complex attention-based DL architectures in brain image analysis by demonstrating better generalizability across different datasets.",
    "title": "Three-dimensional end-to-end deep learning for brain MRI analysis",
    "id": "2506.23916v1",
    "authors": "Radhika Juglan, Marta Ligero, Zunamys I. Carrero, Asier Rabasco, Tim Lenz, Leo Misera, Gregory Patrick Veldhuizen, Paul Kuntke, Hagen H. Kitzler, Sven Nebelung, Daniel Truhn, Jakob Nikolas Kather",
    "x": -2.1088600158691406,
    "y": -2.1677052974700928,
    "z": -0.5639172196388245,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -2.1088600158691406,
        "y": -2.1677052974700928,
        "z": -0.5639172196388245
      },
      "starmap": {
        "x": 0.01392294093966484,
        "y": -0.024465510621666908,
        "z": -0.008724537678062916
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "Learning robust parameter inference and density reconstruction in flyer plate impact experiments",
    "id": "2506.23914v1",
    "date": "2025-06-30",
    "abstract": "Estimating physical parameters or material properties from experimental observations is a common objective in many areas of physics and material science. In many experiments, especially in shock physics, radiography is the primary means of observing the system of interest. However, radiography does not provide direct access to key state variables, such as density, which prevents the application of traditional parameter estimation approaches. Here we focus on flyer plate impact experiments on porous materials, and resolving the underlying parameterized equation of state (EoS) and crush porosity model parameters given radiographic observation(s). We use machine learning as a tool to demonstrate with high confidence that using only high impact velocity data does not provide sufficient information to accurately infer both EoS and crush model parameters, even with fully resolved density fields or a dynamic sequence of images. We thus propose an observable data set consisting of low and high impact velocity experiments/simulations that capture different regimes of compaction and shock propagation, and proceed to introduce a generative machine learning approach which produces a posterior distribution of physical parameters directly from radiographs. We demonstrate the effectiveness of the approach in estimating parameters from simulated flyer plate impact experiments, and show that the obtained estimates of EoS and crush model parameters can then be used in hydrodynamic simulations to obtain accurate and physically admissible density reconstructions. Finally, we examine the robustness of the approach to model mismatches, and find that the learned approach can provide useful parameter estimates in the presence of out-of-distribution radiographic noise and previously unseen physics, thereby promoting a potential breakthrough in estimating material properties from experimental radiographic images.",
    "authors": "Evan Bell, Daniel A. Serino, Ben S. Southworth, Trevor Wilcox, Marc L. Klasky",
    "x": 0.2052449733018875,
    "y": -2.0673563480377197,
    "z": -1.2943016290664673,
    "cluster_id": 8,
    "layouts": {
      "pacmap": {
        "x": 0.2052449733018875,
        "y": -2.0673563480377197,
        "z": -1.2943016290664673
      },
      "starmap": {
        "x": 0.007744400762021542,
        "y": -0.018194185569882393,
        "z": -0.012206343933939934
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "This work addresses the challenge of malware classification using machine learning by developing a novel dataset labeled at both the malware type and family levels. Raw binaries were collected from sources such as VirusShare, VX Underground, and MalwareBazaar, and subsequently labeled with family information parsed from binary names and type-level labels integrated from ClarAVy. The dataset includes 14 malware types and 17 malware families, and was processed using a unified feature extraction pipeline based on static analysis, particularly extracting features from Portable Executable headers, to support advanced classification tasks. The evaluation was focused on three key classification tasks. In the binary classification of malware versus benign samples, Random Forest and XGBoost achieved high accuracy on the full datasets, reaching 98.5% for type-based detection and 98.98% for family-based detection. When using truncated datasets of 1,000 samples to assess performance under limited data conditions, both models still performed strongly, achieving 97.6% for type-based detection and 98.66% for family-based detection. For interclass classification, which distinguishes between malware types or families, the models reached up to 97.5% accuracy on type-level tasks and up to 93.7% on family-level tasks. In the multiclass classification setting, which assigns samples to the correct type or family, SVM achieved 81.1% accuracy on type labels, while Random Forest and XGBoost reached approximately 73.4% on family labels. The results highlight practical trade-offs between accuracy and computational cost, and demonstrate that labeling at both the type and family levels enables more fine-grained and insightful malware classification. The work establishes a robust foundation for future research on advanced malware detection and classification.",
    "title": "RawMal-TF: Raw Malware Dataset Labeled by Type and Family",
    "authors": "David B\u00e1lik, Martin Jure\u010dek, Mark Stamp",
    "date": "2025-06-30",
    "id": "2506.23909v1",
    "x": 1.8258222341537476,
    "y": 0.17724807560443878,
    "z": -1.621462345123291,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 1.8258222341537476,
        "y": 0.17724807560443878,
        "z": -1.621462345123291
      },
      "starmap": {
        "x": -0.017433542758226395,
        "y": 0.003649383317679167,
        "z": -0.0009991968981921673
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Sound deductive reasoning -- the ability to derive new knowledge from existing facts and rules -- is an indisputably desirable aspect of general intelligence. Despite the major advances of AI systems in areas such as math and science, especially since the introduction of transformer architectures, it is well-documented that even the most advanced frontier systems regularly and consistently falter on easily-solvable deductive reasoning tasks. Hence, these systems are unfit to fulfill the dream of achieving artificial general intelligence capable of sound deductive reasoning. We argue that their unsound behavior is a consequence of the statistical learning approach powering their development. To overcome this, we contend that to achieve reliable deductive reasoning in learning-based AI systems, researchers must fundamentally shift from optimizing for statistical performance against distributions on reasoning problems and algorithmic tasks to embracing the more ambitious exact learning paradigm, which demands correctness on all inputs. We argue that exact learning is both essential and possible, and that this ambitious objective should guide algorithm design.",
    "title": "Beyond Statistical Learning: Exact Learning Is Essential for General Intelligence",
    "date": "2025-06-30",
    "id": "2506.23908v1",
    "authors": "Andr\u00e1s Gy\u00f6rgy, Tor Lattimore, Nevena Lazi\u0107, Csaba Szepesv\u00e1ri",
    "x": 3.483381748199463,
    "y": 1.2757843732833862,
    "z": 0.6309567093849182,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 3.483381748199463,
        "y": 1.2757843732833862,
        "z": 0.6309567093849182
      },
      "starmap": {
        "x": -0.025093473494052887,
        "y": 0.010220300406217575,
        "z": -0.002247498370707035
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Hamza Rasaee, Taha Koleilat, Hassan Rivaz",
    "title": "GroundingDINO-US-SAM: Text-Prompted Multi-Organ Segmentation in Ultrasound with LoRA-Tuned Vision-Language Models",
    "date": "2025-06-30",
    "abstract": "Accurate and generalizable object segmentation in ultrasound imaging remains a significant challenge due to anatomical variability, diverse imaging protocols, and limited annotated data. In this study, we propose a prompt-driven vision-language model (VLM) that integrates Grounding DINO with SAM2 to enable object segmentation across multiple ultrasound organs. A total of 18 public ultrasound datasets, encompassing the breast, thyroid, liver, prostate, kidney, and paraspinal muscle, were utilized. These datasets were divided into 15 for fine-tuning and validation of Grounding DINO using Low Rank Adaptation (LoRA) to the ultrasound domain, and 3 were held out entirely for testing to evaluate performance in unseen distributions. Comprehensive experiments demonstrate that our approach outperforms state-of-the-art segmentation methods, including UniverSeg, MedSAM, MedCLIP-SAM, BiomedParse, and SAMUS on most seen datasets while maintaining strong performance on unseen datasets without additional fine-tuning. These results underscore the promise of VLMs in scalable and robust ultrasound image analysis, reducing dependence on large, organ-specific annotated datasets. We will publish our code on code.sonography.ai after acceptance.",
    "id": "2506.23903v1",
    "x": -2.0806498527526855,
    "y": 0.1397181898355484,
    "z": -1.2361501455307007,
    "cluster_id": 4,
    "layouts": {
      "pacmap": {
        "x": -2.0806498527526855,
        "y": 0.1397181898355484,
        "z": -1.2361501455307007
      },
      "starmap": {
        "x": 0.024333998560905457,
        "y": -0.03334872052073479,
        "z": -0.005145390518009663
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "PriOr-Flow: Enhancing Primitive Panoramic Optical Flow with Orthogonal View",
    "authors": "Longliang Liu, Miaojie Feng, Junda Cheng, Jijun Xiang, Xuan Zhu, Xin Yang",
    "date": "2025-06-30",
    "id": "2506.23897v2",
    "abstract": "Panoramic optical flow enables a comprehensive understanding of temporal dynamics across wide fields of view. However, severe distortions caused by sphere-to-plane projections, such as the equirectangular projection (ERP), significantly degrade the performance of conventional perspective-based optical flow methods, especially in polar regions. To address this challenge, we propose PriOr-Flow, a novel dual-branch framework that leverages the low-distortion nature of the orthogonal view to enhance optical flow estimation in these regions. Specifically, we introduce the Dual-Cost Collaborative Lookup (DCCL) operator, which jointly retrieves correlation information from both the primitive and orthogonal cost volumes, effectively mitigating distortion noise during cost volume construction. Furthermore, our Ortho-Driven Distortion Compensation (ODDC) module iteratively refines motion features from both branches, further suppressing polar distortions. Extensive experiments demonstrate that PriOr-Flow is compatible with various perspective-based iterative optical flow methods and consistently achieves state-of-the-art performance on publicly available panoramic optical flow datasets, setting a new benchmark for wide-field motion estimation. The code is publicly available at: https://github.com/longliangLiu/PriOr-Flow.",
    "x": -2.4763734340667725,
    "y": 0.9832692742347717,
    "z": 1.3142187595367432,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -2.4763734340667725,
        "y": 0.9832692742347717,
        "z": 1.3142187595367432
      },
      "starmap": {
        "x": 0.016014231368899345,
        "y": -0.001467822934500873,
        "z": 0.043225016444921494
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23888v1",
    "title": "Advancing Multi-Step Mathematical Reasoning in Large Language Models through Multi-Layered Self-Reflection with Auto-Prompting",
    "authors": "Andr\u00e9 de Souza Loureiro, Jorge Valverde-Rebaza, Julieta Noguez, David Escarcega, Ricardo Marcacini",
    "date": "2025-06-30",
    "abstract": "Recent advancements in Large Language Models (LLMs) have significantly improved their problem-solving capabilities. However, these models still struggle when faced with complex multi-step reasoning tasks. In this paper, we propose the Multi-Layered Self-Reflection with Auto-Prompting (MAPS) framework, a novel approach designed to enhance multi-step mathematical reasoning in LLMs by integrating techniques such as Chain of Thought (CoT), Self-Reflection, and Auto-Prompting. Unlike traditional static prompting methods, MAPS employs an iterative refinement process. Initially, the model generates a solution using CoT prompting. When errors are detected, an adaptive self-reflection mechanism identifies and analyzes them, generating tailored prompts to guide corrections. These dynamically adjusted prompts enable the model to iteratively refine its reasoning. Experiments on four well-established benchmarks across multiple LLMs show that MAPS significantly outperforms standard CoT and achieves competitive results with reasoning-optimized models. In addition, MAPS enables general-purpose LLMs to reach performance levels comparable to specialized reasoning models. While deeper reflection layers improve accuracy, they also increase token usage and costs. To balance this trade-off, MAPS strategically limits reflection depth, ensuring an optimal balance between cost and reasoning performance.",
    "x": 3.0042178630828857,
    "y": 1.629494547843933,
    "z": 0.18556855618953705,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 3.0042178630828857,
        "y": 1.629494547843933,
        "z": 0.18556855618953705
      },
      "starmap": {
        "x": -0.023707890883088112,
        "y": 0.009593592025339603,
        "z": -0.002200953895226121
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Reihaneh Zohrabi, Hosein Hasani, Mahdieh Soleymani Baghshah, Anna Rohrbach, Marcus Rohrbach, Mohammad Hossein Rohban",
    "title": "Spurious-Aware Prototype Refinement for Reliable Out-of-Distribution Detection",
    "abstract": "Out-of-distribution (OOD) detection is crucial for ensuring the reliability and safety of machine learning models in real-world applications, where they frequently face data distributions unseen during training. Despite progress, existing methods are often vulnerable to spurious correlations that mislead models and compromise robustness. To address this, we propose SPROD, a novel prototype-based OOD detection approach that explicitly addresses the challenge posed by unknown spurious correlations. Our post-hoc method refines class prototypes to mitigate bias from spurious features without additional data or hyperparameter tuning, and is broadly applicable across diverse backbones and OOD detection settings. We conduct a comprehensive spurious correlation OOD detection benchmarking, comparing our method against existing approaches and demonstrating its superior performance across challenging OOD datasets, such as CelebA, Waterbirds, UrbanCars, Spurious Imagenet, and the newly introduced Animals MetaCoCo. On average, SPROD improves AUROC by 4.7% and FPR@95 by 9.3% over the second best.",
    "date": "2025-06-30",
    "id": "2506.23881v1",
    "x": -0.38504186272621155,
    "y": 1.010672688484192,
    "z": -2.2835123538970947,
    "cluster_id": 6,
    "layouts": {
      "pacmap": {
        "x": -0.38504186272621155,
        "y": 1.010672688484192,
        "z": -2.2835123538970947
      },
      "starmap": {
        "x": 0.01866741292178631,
        "y": 0.002259453060105443,
        "z": -0.022455044090747833
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "The chain of thought is fundamental in Transformers, which is to perform step-by-step reasoning. Besides what intermediate steps work, the order of these steps critically affects the difficulty of the reasoning. This study addresses a novel task of unraveling chain of thought - reordering decoder input tokens to a learning-friendly sequence for Transformers to learn arithmetic tasks. The proposed pipeline first trains a Transformer on a mixture of target sequences arranged in different orders and then identifies benign orders as those with fast loss drops in the early stage. As the search space grows factorially with sequence length, we propose a two-stage hierarchical approach for inter- and intra-block reordering. Experiments on four order-sensitive arithmetic tasks show that our method identifies a learning-friendly order out of a few billion candidates. Notably, on the multiplication task, it recovered the reverse-digit order reported in prior studies.",
    "date": "2025-06-30",
    "authors": "Yuta Sato, Kazuhiko Kawamoto, Hiroshi Kera",
    "id": "2506.23875v1",
    "title": "Chain of Thought in Order: Discovering Learning-Friendly Orders for Arithmetic",
    "x": 3.257401943206787,
    "y": 1.8136647939682007,
    "z": -0.057503245770931244,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 3.257401943206787,
        "y": 1.8136647939682007,
        "z": -0.057503245770931244
      },
      "starmap": {
        "x": -0.015483902767300606,
        "y": 0.003126264549791813,
        "z": -0.004997559357434511
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "title": "Emergent musical properties of a transformer under contrastive self-supervised learning",
    "abstract": "In music information retrieval (MIR), contrastive self-supervised learning for general-purpose representation models is effective for global tasks such as automatic tagging. However, for local tasks such as chord estimation, it is widely assumed that contrastively trained general-purpose self-supervised models are inadequate and that more sophisticated SSL is necessary; e.g., masked modeling. Our paper challenges this assumption by revealing the potential of contrastive SSL paired with a transformer in local MIR tasks. We consider a lightweight vision transformer with one-dimensional patches in the time--frequency domain (ViT-1D) and train it with simple contrastive SSL through normalized temperature-scaled cross-entropy loss (NT-Xent). Although NT-Xent operates only over the class token, we observe that, potentially thanks to weight sharing, informative musical properties emerge in ViT-1D's sequence tokens. On global tasks, the temporal average of class and sequence tokens offers a performance increase compared to the class token alone, showing useful properties in the sequence tokens. On local tasks, sequence tokens perform unexpectedly well, despite not being specifically trained for. Furthermore, high-level musical features such as onsets emerge from layer-wise attention maps and self-similarity matrices show different layers capture different musical dimensions. Our paper does not focus on improving performance but advances the musical interpretation of transformers and sheds light on some overlooked abilities of contrastive SSL paired with transformers for sequence modeling in MIR.",
    "authors": "Yuexuan Kong, Gabriel Meseguer-Brocal, Vincent Lostanlen, Mathieu Lagrange, Romain Hennequin",
    "id": "2506.23873v1",
    "x": -1.700160264968872,
    "y": -3.0531136989593506,
    "z": -0.412495493888855,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -1.700160264968872,
        "y": -3.0531136989593506,
        "z": -0.412495493888855
      },
      "starmap": {
        "x": 0.004780408926308155,
        "y": -0.017351489514112473,
        "z": -0.015997683629393578
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Living plants, while contributing to ecological balance and climate regulation, also function as natural sensors capable of transmitting information about their internal physiological states and surrounding conditions. This rich source of data provides potential for applications in environmental monitoring and precision agriculture. With integration into biohybrid systems, we establish novel channels of physiological signal flow between living plants and artificial devices. We equipped *Hedera helix* with a plant-wearable device called PhytoNode to continuously record the plant's electrophysiological activity. We deployed plants in an uncontrolled outdoor environment to map electrophysiological patterns to environmental conditions. Over five months, we collected data that we analyzed using state-of-the-art and automated machine learning (AutoML). Our classification models achieve high performance, reaching macro F1 scores of up to 95 percent in binary tasks. AutoML approaches outperformed manual tuning, and selecting subsets of statistical features further improved accuracy. Our biohybrid living system monitors the electrophysiology of plants in harsh, real-world conditions. This work advances scalable, self-sustaining, and plant-integrated living biohybrid systems for sustainable environmental monitoring.",
    "authors": "Eduard Buss, Till Aust, Heiko Hamann",
    "id": "2506.23872v1",
    "title": "When Plants Respond: Electrophysiology and Machine Learning for Green Monitoring Systems",
    "date": "2025-06-30",
    "x": 1.2123136520385742,
    "y": -1.6594700813293457,
    "z": 0.012492837384343147,
    "cluster_id": 7,
    "layouts": {
      "pacmap": {
        "x": 1.2123136520385742,
        "y": -1.6594700813293457,
        "z": 0.012492837384343147
      },
      "starmap": {
        "x": 0.005221680738031864,
        "y": -0.009309462271630764,
        "z": -0.012523737736046314
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "abstract": "We study the capabilities of generative autoregressive transformer models trained on large amounts of symbolic solo-piano transcriptions. After first pretraining on approximately 60,000 hours of music, we use a comparatively smaller, high-quality subset, to finetune models to produce musical continuations, perform symbolic classification tasks, and produce general-purpose contrastive MIDI embeddings by adapting the SimCLR framework to symbolic music. When evaluating piano continuation coherence, our generative model outperforms leading symbolic generation techniques and remains competitive with proprietary audio generation models. On MIR classification benchmarks, frozen representations from our contrastive model achieve state-of-the-art results in linear probe experiments, while direct finetuning demonstrates the generalizability of pretrained representations, often requiring only a few hundred labeled examples to specialize to downstream tasks.",
    "id": "2506.23869v1",
    "authors": "Louis Bradshaw, Honglu Fan, Alexander Spangher, Stella Biderman, Simon Colton",
    "title": "Scaling Self-Supervised Representation Learning for Symbolic Piano Performance",
    "x": -1.6478934288024902,
    "y": 0.17290711402893066,
    "z": 0.591451108455658,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -1.6478934288024902,
        "y": 0.17290711402893066,
        "z": 0.591451108455658
      },
      "starmap": {
        "x": 0.0003947983495891094,
        "y": -0.01285428274422884,
        "z": -0.009030056186020374
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Seyed Mahed Mousavi, Edoardo Cecchinato, Lucia Hornikova, Giuseppe Riccardi",
    "date": "2025-06-30",
    "abstract": "We conduct a systematic audit of three widely used reasoning benchmarks, SocialIQa, FauxPas-EAI, and ToMi, and uncover pervasive flaws in both benchmark items and evaluation methodology. Using five LLMs (GPT-{3, 3.5, 4, o1}, and LLaMA 3.1) as diagnostic tools, we identify structural, semantic, and pragmatic issues in benchmark design (e.g., duplicated items, ambiguous wording, and implausible answers), as well as scoring procedures that prioritize output form over reasoning process. Through systematic human annotation and re-evaluation on cleaned benchmark subsets, we find that model scores often improve not due to due to erratic surface wording variations and not to improved reasoning. Infact, further analyses show that model performance is highly sensitive to minor input variations such as context availability and phrasing, revealing that high scores may reflect alignment with format-specific cues rather than consistent inference based on the input. These findings challenge the validity of current benchmark-based claims about reasoning in LLMs, and highlight the need for evaluation protocols that assess reasoning as a process of drawing inference from available information, rather than as static output selection. We release audited data and evaluation tools to support more interpretable and diagnostic assessments of model reasoning.",
    "title": "Garbage In, Reasoning Out? Why Benchmark Scores are Unreliable and What to Do About It",
    "id": "2506.23864v1",
    "x": 3.0671544075012207,
    "y": 1.4937825202941895,
    "z": 0.4829496741294861,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 3.0671544075012207,
        "y": 1.4937825202941895,
        "z": 0.4829496741294861
      },
      "starmap": {
        "x": -0.02420167066156864,
        "y": 0.009992740117013454,
        "z": -0.00250598625279963
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "abstract": "Multi-view 3D reconstruction remains a core challenge in computer vision. Recent methods, such as DUST3R and its successors, directly regress pointmaps from image pairs without relying on known scene geometry or camera parameters. However, the performance of these models is constrained by the diversity and scale of available training data. In this work, we introduce Puzzles, a data augmentation strategy that synthesizes an unbounded volume of high-quality posed video-depth data from a single image or video clip. By simulating diverse camera trajectories and realistic scene geometry through targeted image transformations, Puzzles significantly enhances data variety. Extensive experiments show that integrating Puzzles into existing video-based 3D reconstruction pipelines consistently boosts performance without modifying the underlying network architecture. Notably, models trained on only ten percent of the original data augmented with Puzzles still achieve accuracy comparable to those trained on the full dataset. Code is available at https://jiahao-ma.github.io/puzzles/.",
    "title": "Puzzles: Unbounded Video-Depth Augmentation for Scalable End-to-End 3D Reconstruction",
    "authors": "Jiahao Ma, Lei Wang, Miaomiao liu, David Ahmedt-Aristizabal, Chuong Nguyen",
    "id": "2506.23863v1",
    "x": -2.366896152496338,
    "y": 2.440518617630005,
    "z": 0.21022531390190125,
    "cluster_id": 1,
    "layouts": {
      "pacmap": {
        "x": -2.366896152496338,
        "y": 2.440518617630005,
        "z": 0.21022531390190125
      },
      "starmap": {
        "x": 0.05010084807872772,
        "y": 0.06822982430458069,
        "z": -0.022470945492386818
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "The quadratic complexity of full attention mechanisms poses a significant bottleneck for Video Diffusion Models (VDMs) aiming to generate long-duration, high-resolution videos. While various sparse attention methods have been proposed, many are designed as training-free inference accelerators or do not optimally capture the unique spatio-temporal characteristics inherent in video data when trained natively. This paper introduces Video Mixture of Block Attention (VMoBA), a novel sparse attention mechanism specifically adapted for VDMs. Motivated by an in-depth analysis of attention patterns within pre-trained video transformers, which revealed strong spatio-temporal locality, varying query importance, and head-specific concentration levels, VMoBA enhances the original MoBA framework with three key modifications: (1) a layer-wise recurrent block partition scheme (1D-2D-3D) to dynamically adapt to diverse spatio-temporal attention patterns and improve efficiency; (2) global block selection to prioritize the most salient query-key block interactions across an entire attention head; and (3) threshold-based block selection to dynamically determine the number of attended blocks based on their cumulative similarity. Extensive experiments demonstrate that VMoBA significantly accelerates the training of VDMs on longer sequences, achieving 2.92x FLOPs and 1.48x latency speedup, while attaining comparable or even superior generation quality to full attention. Furthermore, VMoBA exhibits competitive performance in training-free inference, offering 2.40x FLOPs and 1.35x latency speedup for high-res video generation.",
    "id": "2506.23858v1",
    "authors": "Jianzong Wu, Liang Hou, Haotian Yang, Xin Tao, Ye Tian, Pengfei Wan, Di Zhang, Yunhai Tong",
    "date": "2025-06-30",
    "title": "VMoBA: Mixture-of-Block Attention for Video Diffusion Models",
    "x": -2.2649195194244385,
    "y": 1.9405919313430786,
    "z": 1.4056240320205688,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -2.2649195194244385,
        "y": 1.9405919313430786,
        "z": 1.4056240320205688
      },
      "starmap": {
        "x": 0.02305472642183304,
        "y": -0.013450310565531254,
        "z": 0.017734171822667122
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23856v1",
    "title": "A Closer Look at Conditional Prompt Tuning for Vision-Language Models",
    "authors": "Ji Zhang, Shihan Wu, Lianli Gao, Jingkuan Song, Nicu Sebe, Heng Tao Shen",
    "date": "2025-06-30",
    "abstract": "Despite the great promise of Prompt Tuning (PT) in adapting large Vision-Language Pretrained Models (VLPMs) to downstream tasks, they often struggle to overcome the Base-New Tradeoff (BNT) dilemma: as VLPMs are better tuned to a base task, their ability to generalize to new tasks diminishes. Recent work on conditional PT addresses this problem by replacing static prompts with dynamic Visual Image Information (VII)-conditioned prompts, improving the model's generalization to new tasks to some extent. In this work, we first identify a critical issue with existing conditional PT methods: using VII as the \"condition\" of prompts yields suboptimal performance, and even random noise-conditioned prompts can outperform the VII-conditioned counterparts. On further analysis, we find that learning dynamic prompts conditioned on Textual Class Information (TCI) is the key to solving the BNT problem. Motivated by this, we then propose Class-adaptive Prompt Tuning (CaPT), which enables fast adaptation of tuned models to new classes by learning TCI-conditioned prompts from base classes. Remarkably, CaPT can be used as a plugin to mitigate the BNT problem for existing unconditional PT schemes. Extensive experiments on 11 datasets show that CaPT consistently improves the performance of five strong unconditional PT baselines with negligible additional computational cost. Additionally, by integrating CaPT with our recently proposed DePT framework, we devise a new conditional PT approach, termed DeCaPT, which outperforms the H ACC of the state-of-the-art conditional PT scheme by 3.49%, averaged over the 11 datasets. Code: https://github.com/Koorye/CaPT.",
    "x": -2.0911142826080322,
    "y": -0.6518874168395996,
    "z": -0.5144690275192261,
    "cluster_id": 3,
    "layouts": {
      "pacmap": {
        "x": -2.0911142826080322,
        "y": -0.6518874168395996,
        "z": -0.5144690275192261
      },
      "starmap": {
        "x": 0.018602518364787102,
        "y": -0.03680954873561859,
        "z": -0.018419399857521057
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "The analysis of the privacy properties of Privacy-Preserving Ads APIs is an area of research that has received strong interest from academics, industry, and regulators. Despite this interest, the empirical study of these methods is hindered by the lack of publicly available data. Reliable empirical analysis of the privacy properties of an API, in fact, requires access to a dataset consisting of realistic API outputs; however, privacy concerns prevent the general release of such data to the public.   In this work, we develop a novel methodology to construct synthetic API outputs that are simultaneously realistic enough to enable accurate study and provide strong privacy protections. We focus on one Privacy-Preserving Ads APIs: the Topics API, part of Google Chrome's Privacy Sandbox. We developed a methodology to generate a differentially-private dataset that closely matches the re-identification risk properties of the real Topics API data. The use of differential privacy provides strong theoretical bounds on the leakage of private user information from this release.   Our methodology is based on first computing a large number of differentially-private statistics describing how output API traces evolve over time. Then, we design a parameterized distribution over sequences of API traces and optimize its parameters so that they closely match the statistics obtained. Finally, we create the synthetic data by drawing from this distribution.   Our work is complemented by an open-source release of the anonymized dataset obtained by this methodology. We hope this will enable external researchers to analyze the API in-depth and replicate prior and future work on a realistic large-scale dataset. We believe that this work will contribute to fostering transparency regarding the privacy properties of Privacy-Preserving Ads APIs.",
    "id": "2506.23855v1",
    "authors": "Travis Dick, Alessandro Epasto, Adel Javanmard, Josh Karlin, Andres Munoz Medina, Vahab Mirrokni, Sergei Vassilvitskii, Peilin Zhong",
    "title": "Differentially Private Synthetic Data Release for Topics API Outputs",
    "date": "2025-06-30",
    "x": 0.7461509704589844,
    "y": 0.35665377974510193,
    "z": -0.3864622414112091,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 0.7461509704589844,
        "y": 0.35665377974510193,
        "z": -0.3864622414112091
      },
      "starmap": {
        "x": -0.017867710441350937,
        "y": 0.00290630548261106,
        "z": -0.0017583289882168174
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Neural surface reconstruction faces persistent challenges in reconciling geometric fidelity with photometric consistency under complex scene conditions. We present HiNeuS, a unified framework that holistically addresses three core limitations in existing approaches: multi-view radiance inconsistency, missing keypoints in textureless regions, and structural degradation from over-enforced Eikonal constraints during joint optimization. To resolve these issues through a unified pipeline, we introduce: 1) Differential visibility verification through SDF-guided ray tracing, resolving reflection ambiguities via continuous occlusion modeling; 2) Planar-conformal regularization via ray-aligned geometry patches that enforce local surface coherence while preserving sharp edges through adaptive appearance weighting; and 3) Physically-grounded Eikonal relaxation that dynamically modulates geometric constraints based on local radiance gradients, enabling detail preservation without sacrificing global regularity. Unlike prior methods that handle these aspects through sequential optimizations or isolated modules, our approach achieves cohesive integration where appearance-geometry constraints evolve synergistically throughout training. Comprehensive evaluations across synthetic and real-world datasets demonstrate state-of-the-art performance, including a 21.4% reduction in Chamfer distance over reflection-aware baselines and 2.32 dB PSNR improvement against neural rendering counterparts. Qualitative analyses reveal superior capability in recovering specular instruments, urban layouts with centimeter-scale infrastructure, and low-textured surfaces without local patch collapse. The method's generalizability is further validated through successful application to inverse rendering tasks, including material decomposition and view-consistent relighting.",
    "id": "2506.23854v1",
    "date": "2025-06-30",
    "authors": "Yida Wang, Xueyang Zhang, Kun Zhan, Peng Jia, Xianpeng Lang",
    "title": "HiNeuS: High-fidelity Neural Surface Mitigating Low-texture and Reflective Ambiguity",
    "x": -1.823315143585205,
    "y": 2.6518568992614746,
    "z": -0.6251187920570374,
    "cluster_id": 1,
    "layouts": {
      "pacmap": {
        "x": -1.823315143585205,
        "y": 2.6518568992614746,
        "z": -0.6251187920570374
      },
      "starmap": {
        "x": 0.04671498015522957,
        "y": 0.06415851414203644,
        "z": -0.025367414578795433
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Jianing Jin, Jiangyong Ying, Huiyu Duan, Liu Yang, Sijing Wu, Yunhao Li, Yushuo Zheng, Xiongkuo Min, Guangtao Zhai",
    "abstract": "As camera-equipped robotic platforms become increasingly integrated into daily life, robotic-generated videos have begun to appear on streaming media platforms, enabling us to envision a future where humans and robots coexist. We innovatively propose the concept of Robotic-Generated Content (RGC) to term these videos generated from egocentric perspective of robots. The perceptual quality of RGC videos is critical in human-robot interaction scenarios, and RGC videos exhibit unique distortions and visual requirements that differ markedly from those of professionally-generated content (PGC) videos and user-generated content (UGC) videos. However, dedicated research on quality assessment of RGC videos is still lacking. To address this gap and to support broader robotic applications, we establish the first Robotic-Generated Content Database (RGCD), which contains a total of 2,100 videos drawn from three robot categories and sourced from diverse platforms. A subjective VQA experiment is conducted subsequently to assess human visual perception of robotic-generated videos. Finally, we conduct a benchmark experiment to evaluate the performance of 11 state-of-the-art VQA models on our database. Experimental results reveal significant limitations in existing VQA models when applied to complex, robotic-generated content, highlighting a critical need for RGC-specific VQA models. Our RGCD is publicly available at: https://github.com/IntMeGroup/RGC-VQA.",
    "title": "RGC-VQA: An Exploration Database for Robotic-Generated Video Quality Assessment",
    "date": "2025-06-30",
    "id": "2506.23852v1",
    "x": -0.7509053349494934,
    "y": 2.526620864868164,
    "z": 1.0975255966186523,
    "cluster_id": 2,
    "layouts": {
      "pacmap": {
        "x": -0.7509053349494934,
        "y": 2.526620864868164,
        "z": 1.0975255966186523
      },
      "starmap": {
        "x": 0.013914857059717178,
        "y": -0.003190512303262949,
        "z": 0.04880191385746002
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "While sparse autoencoders (SAEs) have generated significant excitement, a series of negative results have added to skepticism about their usefulness. Here, we establish a conceptual distinction that reconciles competing narratives surrounding SAEs. We argue that while SAEs may be less effective for acting on known concepts, SAEs are powerful tools for discovering unknown concepts. This distinction cleanly separates existing negative and positive results, and suggests several classes of SAE applications. Specifically, we outline use cases for SAEs in (i) ML interpretability, explainability, fairness, auditing, and safety, and (ii) social and health sciences.",
    "authors": "Kenny Peng, Rajiv Movva, Jon Kleinberg, Emma Pierson, Nikhil Garg",
    "id": "2506.23845v1",
    "title": "Use Sparse Autoencoders to Discover Unknown Concepts, Not to Act on Known Concepts",
    "date": "2025-06-30",
    "x": 2.838954448699951,
    "y": 0.047764554619789124,
    "z": -1.4991074800491333,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 2.838954448699951,
        "y": 0.047764554619789124,
        "z": -1.4991074800491333
      },
      "starmap": {
        "x": -0.020179908722639084,
        "y": 0.005545084364712238,
        "z": 0.0006076773861423135
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Hang Su, Jun Luo, Chang Liu, Xiao Yang, Yichi Zhang, Yinpeng Dong, Jun Zhu",
    "abstract": "Recent advances in large language models (LLMs) have catalyzed the rise of autonomous AI agents capable of perceiving, reasoning, and acting in dynamic, open-ended environments. These large-model agents mark a paradigm shift from static inference systems to interactive, memory-augmented entities. While these capabilities significantly expand the functional scope of AI, they also introduce qualitatively novel security risks - such as memory poisoning, tool misuse, reward hacking, and emergent misalignment - that extend beyond the threat models of conventional systems or standalone LLMs. In this survey, we first examine the structural foundations and key capabilities that underpin increasing levels of agent autonomy, including long-term memory retention, modular tool use, recursive planning, and reflective reasoning. We then analyze the corresponding security vulnerabilities across the agent stack, identifying failure modes such as deferred decision hazards, irreversible tool chains, and deceptive behaviors arising from internal state drift or value misalignment. These risks are traced to architectural fragilities that emerge across perception, cognition, memory, and action modules. To address these challenges, we systematically review recent defense strategies deployed at different autonomy layers, including input sanitization, memory lifecycle control, constrained decision-making, structured tool invocation, and introspective reflection. We introduce the Reflective Risk-Aware Agent Architecture (R2A2), a unified cognitive framework grounded in Constrained Markov Decision Processes (CMDPs), which incorporates risk-aware world modeling, meta-policy adaptation, and joint reward-risk optimization to enable principled, proactive safety across the agent's decision-making loop.",
    "title": "A Survey on Autonomy-Induced Security Risks in Large Model-Based Agents",
    "id": "2506.23844v1",
    "date": "2025-06-30",
    "x": 2.489037036895752,
    "y": 0.696882963180542,
    "z": 1.2733350992202759,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 2.489037036895752,
        "y": 0.696882963180542,
        "z": 1.2733350992202759
      },
      "starmap": {
        "x": -0.023740451782941818,
        "y": 0.008695783093571663,
        "z": -3.3389740565326065e-05
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23843v1",
    "date": "2025-06-30",
    "title": "EFPI: Elastic Formation and Position Identification in Football (Soccer) using Template Matching and Linear Assignment",
    "abstract": "Understanding team formations and player positioning is crucial for tactical analysis in football (soccer). This paper presents a flexible method for formation recognition and player position assignment in football using predefined static formation templates and cost minimization from spatiotemporal tracking data, called EFPI. Our approach employs linear sum assignment to optimally match players to positions within a set of template formations by minimizing the total distance between actual player locations and template positions, subsequently selecting the formation with the lowest assignment cost. To improve accuracy, we scale actual player positions to match the dimensions of these formation templates in both width and length. While the method functions effectively on individual frames, it extends naturally to larger game segments such as complete periods, possession sequences or specific intervals (e.g. 10 second intervals, 5 minute intervals etc.). Additionally, we incorporate an optional stability parameter that prevents unnecessary formation changes when assignment costs differ only marginally between time segments. EFPI is available as open-source code through the unravelsports Python package.",
    "authors": "Joris Bekkers",
    "x": 2.428518772125244,
    "y": -1.5699849128723145,
    "z": -2.2605137825012207,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": 2.428518772125244,
        "y": -1.5699849128723145,
        "z": -2.2605137825012207
      },
      "starmap": {
        "x": -0.011285361833870411,
        "y": -0.00023696402786299586,
        "z": -0.007407506927847862
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23840v1",
    "abstract": "Large Reasoning Models (LRMs) excel at solving complex problems but face an overthinking dilemma. When handling simple tasks, they often produce verbose responses overloaded with thinking tokens (e.g., wait, however). These tokens trigger unnecessary high-level reasoning behaviors like reflection and backtracking, reducing efficiency. In this work, our pilot study reveals that these thinking-token-induced behaviors are not essential for effective problem-solving and may even hinder correct reasoning within constrained token budgets. We identify this phenomenon as the thinking trap. To mitigate this issue, we propose Dual Policy Preference Optimization (DuP-PO), a novel algorithm featuring: (1) A rollout sampling strategy that guarantees balanced exposure to responses with and without thinking tokens; (2) A fine-grained advantage control technique to dynamically regulate the prediction of target tokens; (3) A policy shaping method ensuring stable gradient contributions from thinking tokens. Experimental results on five popular math reasoning benchmarks show that DuP-PO performs well on the popular LRM, which significantly improves their token efficiency during reasoning, while achieving superior performance of the base model.",
    "date": "2025-06-30",
    "authors": "Bowen Ding, Yuhan Chen, Futing Wang, Lingfeng Ming, Tao Lin",
    "title": "Do Thinking Tokens Help or Trap? Towards More Efficient Large Reasoning Model",
    "x": 3.2853775024414062,
    "y": 1.9280211925506592,
    "z": 0.4578287899494171,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 3.2853775024414062,
        "y": 1.9280211925506592,
        "z": 0.4578287899494171
      },
      "starmap": {
        "x": -0.02265145629644394,
        "y": 0.008955688215792179,
        "z": -0.00137474259827286
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "abstract": "We consider centralized distributed optimization in the classical federated learning setup, where $n$ workers jointly find an $\\varepsilon$-stationary point of an $L$-smooth, $d$-dimensional nonconvex function $f$, having access only to unbiased stochastic gradients with variance $\\sigma^2$. Each worker requires at most $h$ seconds to compute a stochastic gradient, and the communication times from the server to the workers and from the workers to the server are $\\tau_{s}$ and $\\tau_{w}$ seconds per coordinate, respectively. One of the main motivations for distributed optimization is to achieve scalability with respect to $n$. For instance, it is well known that the distributed version of SGD has a variance-dependent runtime term $\\frac{h \\sigma^2 L \\Delta}{n \\varepsilon^2},$ which improves with the number of workers $n,$ where $\\Delta = f(x^0) - f^*,$ and $x^0 \\in R^d$ is the starting point. Similarly, using unbiased sparsification compressors, it is possible to reduce both the variance-dependent runtime term and the communication runtime term. However, once we account for the communication from the server to the workers $\\tau_{s}$, we prove that it becomes infeasible to design a method using unbiased random sparsification compressors that scales both the server-side communication runtime term $\\tau_{s} d \\frac{L \\Delta}{\\varepsilon}$ and the variance-dependent runtime term $\\frac{h \\sigma^2 L \\Delta}{\\varepsilon^2},$ better than poly-logarithmically in $n$, even in the homogeneous (i.i.d.) case, where all workers access the same distribution. To establish this result, we construct a new \"worst-case\" function and develop a new lower bound framework that reduces the analysis to the concentration of a random sum, for which we prove a concentration bound. These results reveal fundamental limitations in scaling distributed optimization, even under the homogeneous assumption.",
    "authors": "Alexander Tyurin",
    "title": "Proving the Limited Scalability of Centralized Distributed Optimization via a New Lower Bound Construction",
    "id": "2506.23836v1",
    "x": 1.737401008605957,
    "y": -2.7633633613586426,
    "z": -0.7480682134628296,
    "cluster_id": 6,
    "layouts": {
      "pacmap": {
        "x": 1.737401008605957,
        "y": -2.7633633613586426,
        "z": -0.7480682134628296
      },
      "starmap": {
        "x": 0.012070901691913605,
        "y": -0.014131906442344189,
        "z": -0.02021278440952301
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "authors": "Ziwei Chen, Ziling Liu, Zitong Huang, Mingqi Gao, Feng Zheng",
    "title": "Refine Any Object in Any Scene",
    "id": "2506.23835v1",
    "abstract": "Viewpoint missing of objects is common in scene reconstruction, as camera paths typically prioritize capturing the overall scene structure rather than individual objects. This makes it highly challenging to achieve high-fidelity object-level modeling while maintaining accurate scene-level representation. Addressing this issue is critical for advancing downstream tasks requiring detailed object understanding and appearance modeling. In this paper, we introduce Refine Any object In any ScenE (RAISE), a novel 3D enhancement framework that leverages 3D generative priors to recover fine-grained object geometry and appearance under missing views. Starting from substituting degraded objects with proxies, via a 3D generative model with strong 3D understanding, RAISE progressively refines geometry and texture by aligning each proxy to its degraded counterpart in 7-DOF pose, followed by correcting spatial and appearance inconsistencies via registration-constrained enhancement. This two-stage refinement ensures the high-fidelity geometry and appearance of the original object in unseen views while maintaining consistency in spatial positioning, observed geometry, and appearance. Extensive experiments on challenging benchmarks show that RAISE significantly outperforms state-of-the-art methods in both novel view synthesis and geometry completion tasks. RAISE is made publicly available at https://github.com/PolySummit/RAISE.",
    "x": -2.017219305038452,
    "y": 2.365605354309082,
    "z": -0.6345205307006836,
    "cluster_id": 1,
    "layouts": {
      "pacmap": {
        "x": -2.017219305038452,
        "y": 2.365605354309082,
        "z": -0.6345205307006836
      },
      "starmap": {
        "x": 0.04959528520703316,
        "y": 0.06846657395362854,
        "z": -0.025485290214419365
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23833v1",
    "title": "PointSSIM: A novel low dimensional resolution invariant image-to-image comparison metric",
    "authors": "Oscar Ovanger, Ragnar Hauge, Jacob Skauvold, Michael J. Pyrcz, Jo Eidsvik",
    "date": "2025-06-30",
    "abstract": "This paper presents PointSSIM, a novel low-dimensional image-to-image comparison metric that is resolution invariant. Drawing inspiration from the structural similarity index measure and mathematical morphology, PointSSIM enables robust comparison across binary images of varying resolutions by transforming them into marked point pattern representations. The key features of the image, referred to as anchor points, are extracted from binary images by identifying locally adaptive maxima from the minimal distance transform. Image comparisons are then performed using a summary vector, capturing intensity, connectivity, complexity, and structural attributes. Results show that this approach provides an efficient and reliable method for image comparison, particularly suited to applications requiring structural analysis across different resolutions.",
    "x": -3.254197120666504,
    "y": 0.5380045771598816,
    "z": -0.2783653438091278,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -3.254197120666504,
        "y": 0.5380045771598816,
        "z": -0.2783653438091278
      },
      "starmap": {
        "x": 0.025637056678533554,
        "y": 0.0013735737884417176,
        "z": 0.046016741544008255
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "authors": "Ronit D. Gross, Tal Halevi, Ella Koresh, Yarden Tzach, Ido Kanter",
    "id": "2506.23832v1",
    "abstract": "The emergence of spontaneous symmetry breaking among a few heads of multi-head attention (MHA) across transformer blocks in classification tasks was recently demonstrated through the quantification of single-nodal performance (SNP). This finding indicates that each head focuses its attention on a subset of labels through cooperation among its SNPs. This underlying learning mechanism is generalized to large-scale MHA (LS-MHA) using a single matrix value representing single-head performance (SHP), analogous to single-filter performance in convolutional neural networks (CNNs). The results indicate that each SHP matrix comprises multiple unit clusters such that each label being explicitly recognized by a few heads with negligible noise. This leads to an increased signal-to-noise ratio (SNR) along the transformer blocks, thereby improving classification accuracy. These features give rise to several distinct vision transformer (ViT) architectures that achieve the same accuracy but differ in their LS-MHA structures. As a result, their soft committee yields superior accuracy, an outcome not typically observed in CNNs which rely on hundreds of filters. In addition, a significant reduction in latency is achieved without affecting the accuracy by replacing the initial transformer blocks with convolutional layers. This substitution accelerates early-stage learning, which is then improved by subsequent transformer layers. The extension of this learning mechanism to natural language processing tasks, based on quantitative differences between CNNs and ViT architectures, has the potential to yield new insights in deep learning. The findings are demonstrated using compact convolutional transformer architectures trained on the CIFAR-100 dataset.",
    "title": "Low-latency vision transformers via large-scale multi-head attention",
    "x": -1.7666398286819458,
    "y": -2.742114305496216,
    "z": 0.031394001096487045,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -1.7666398286819458,
        "y": -2.742114305496216,
        "z": 0.031394001096487045
      },
      "starmap": {
        "x": 0.014681598171591759,
        "y": -0.011677857488393784,
        "z": -0.002486254321411252
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Spatial transcriptomics (ST) provides crucial insights into tissue micro-environments, but is limited to its high cost and complexity. As an alternative, predicting gene expression from pathology whole slide images (WSI) is gaining increasing attention. However, existing methods typically rely on single patches or a single pathology modality, neglecting the complex spatial and molecular interactions between target and neighboring information (e.g., gene co-expression). This leads to a failure in establishing connections among adjacent regions and capturing intricate cross-modal relationships. To address these issues, we propose NH2ST, a framework that integrates spatial context and both pathology and gene modalities for gene expression prediction. Our model comprises a query branch and a neighbor branch to process paired target patch and gene data and their neighboring regions, where cross-attention and contrastive learning are employed to capture intrinsic associations and ensure alignments between pathology and gene expression. Extensive experiments on six datasets demonstrate that our model consistently outperforms existing methods, achieving over 20% in PCC metrics. Codes are available at https://github.com/MCPathology/NH2ST",
    "id": "2506.23827v1",
    "date": "2025-06-30",
    "authors": "Mingcheng Qu, Yuncong Wu, Donglin Di, Yue Gao, Tonghua Su, Yang Song, Lei Fan",
    "title": "Spatially Gene Expression Prediction using Dual-Scale Contrastive Learning",
    "x": -2.1091043949127197,
    "y": -1.3175796270370483,
    "z": -0.653645932674408,
    "cluster_id": 4,
    "layouts": {
      "pacmap": {
        "x": -2.1091043949127197,
        "y": -1.3175796270370483,
        "z": -0.653645932674408
      },
      "starmap": {
        "x": 0.016010774299502373,
        "y": -0.02497970499098301,
        "z": -0.0012602441711351275
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "Towards the \"Digital Me\": A vision of authentic Conversational Agents powered by personal Human Digital Twins",
    "authors": "Llu\u00eds C. Coll, Martin W. Lauer-Schmaltz, Philip Cash, John P. Hansen, Anja Maier",
    "abstract": "Human Digital Twins (HDTs) have traditionally been conceptualized as data-driven models designed to support decision-making across various domains. However, recent advancements in conversational AI open new possibilities for HDTs to function as authentic, interactive digital counterparts of individuals. This paper introduces a novel HDT system architecture that integrates large language models with dynamically updated personal data, enabling it to mirror an individual's conversational style, memories, and behaviors. To achieve this, our approach implements context-aware memory retrieval, neural plasticity-inspired consolidation, and adaptive learning mechanisms, creating a more natural and evolving digital persona. The resulting system does not only replicate an individual's unique conversational style depending on who they are speaking with, but also enriches responses with dynamically captured personal experiences, opinions, and memories. While this marks a significant step toward developing authentic virtual counterparts, it also raises critical ethical concerns regarding privacy, accountability, and the long-term implications of persistent digital identities. This study contributes to the field of HDTs by describing our novel system architecture, demonstrating its capabilities, and discussing future directions and emerging challenges to ensure the responsible and ethical development of HDTs.",
    "id": "2506.23826v1",
    "date": "2025-06-30",
    "x": 2.426466703414917,
    "y": 0.20322467386722565,
    "z": 0.2753705084323883,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": 2.426466703414917,
        "y": 0.20322467386722565,
        "z": 0.2753705084323883
      },
      "starmap": {
        "x": -0.0030624608043581247,
        "y": 0.0015844939043745399,
        "z": 0.028458762913942337
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Benefiting from the advances in large language models and cross-modal alignment, existing multimodal large language models have achieved prominent performance in image and short video understanding. However, the understanding of long videos is still challenging, as their long-context nature results in significant computational and memory overhead. Most existing work treats long videos in the same way as short videos, which is inefficient for real-world applications and hard to generalize to even longer videos. To address these issues, we propose Flash-VStream, an efficient video language model capable of processing extremely long videos and responding to user queries in real time. Particularly, we design a Flash Memory module, containing a low-capacity context memory to aggregate long-context temporal information and model the distribution of information density, and a high-capacity augmentation memory to retrieve detailed spatial information based on this distribution. Compared to existing models, Flash-VStream achieves significant reductions in inference latency. Extensive experiments on long video benchmarks and comprehensive video benchmarks, i.e., EgoSchema, MLVU, LVBench, MVBench and Video-MME, demonstrate the state-of-the-art performance and outstanding efficiency of our method. Code is available at https://github.com/IVGSZ/Flash-VStream.",
    "date": "2025-06-30",
    "authors": "Haoji Zhang, Yiqin Wang, Yansong Tang, Yong Liu, Jiashi Feng, Xiaojie Jin",
    "title": "Flash-VStream: Efficient Real-Time Understanding for Long Video Streams",
    "id": "2506.23825v1",
    "x": -2.1283693313598633,
    "y": 2.194078207015991,
    "z": 1.4218772649765015,
    "cluster_id": 3,
    "layouts": {
      "pacmap": {
        "x": -2.1283693313598633,
        "y": 2.194078207015991,
        "z": 1.4218772649765015
      },
      "starmap": {
        "x": 0.018396025523543358,
        "y": -0.021371005102992058,
        "z": 0.007858448661863804
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "The development of semi-supervised learning (SSL) has in recent years largely focused on the development of new consistency regularization or entropy minimization approaches, often resulting in models with complex training strategies to obtain the desired results. In this work, we instead propose a novel approach that explicitly incorporates the underlying clustering assumption in SSL through extending a recently proposed differentiable clustering module. Leveraging annotated data to guide the cluster centroids results in a simple end-to-end trainable deep SSL approach. We demonstrate that the proposed model improves the performance over the supervised-only baseline and show that our framework can be used in conjunction with other SSL methods to further boost their performance.",
    "title": "Supercm: Revisiting Clustering for Semi-Supervised Learning",
    "authors": "Durgesh Singh, Ahcene Boubekki, Robert Jenssen, Michael C. Kampffmeyer",
    "id": "2506.23824v1",
    "date": "2025-06-30",
    "x": -1.5407638549804688,
    "y": -2.9399938583374023,
    "z": -0.7267757654190063,
    "cluster_id": 5,
    "layouts": {
      "pacmap": {
        "x": -1.5407638549804688,
        "y": -2.9399938583374023,
        "z": -0.7267757654190063
      },
      "starmap": {
        "x": 0.0068178256042301655,
        "y": -0.0225526075810194,
        "z": -0.023575516417622566
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23822v1",
    "abstract": "Large-scale vision-language models (VLMs), such as CLIP, have achieved remarkable success in zero-shot learning (ZSL) by leveraging large-scale visual-text pair datasets. However, these methods often lack interpretability, as they compute the similarity between an entire query image and the embedded category words, making it difficult to explain their predictions. One approach to address this issue is to develop interpretable models by integrating language, where classifiers are built using discrete attributes, similar to human perception. This introduces a new challenge: how to effectively align local visual features with corresponding attributes based on pre-trained VLMs. To tackle this, we propose LaZSL, a locally-aligned vision-language model for interpretable ZSL. LaZSL employs local visual-semantic alignment via optimal transport to perform interaction between visual regions and their associated attributes, facilitating effective alignment and providing interpretable similarity without the need for additional training. Extensive experiments demonstrate that our method offers several advantages, including enhanced interpretability, improved accuracy, and strong domain generalization. Codes available at: https://github.com/shiming-chen/LaZSL.",
    "title": "Interpretable Zero-Shot Learning with Locally-Aligned Vision-Language Model",
    "date": "2025-06-30",
    "authors": "Shiming Chen, Bowen Duan, Salman Khan, Fahad Shahbaz Khan",
    "x": -0.20928405225276947,
    "y": 1.2372925281524658,
    "z": -0.5358638763427734,
    "cluster_id": 3,
    "layouts": {
      "pacmap": {
        "x": -0.20928405225276947,
        "y": 1.2372925281524658,
        "z": -0.5358638763427734
      },
      "starmap": {
        "x": 0.014736013486981392,
        "y": -0.03728080540895462,
        "z": -0.02578721195459366
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "The influence of Artificial Intelligence (AI), and specifically Large Language Models (LLM), on education is continuously increasing. These models are frequently used by students, giving rise to the question whether current forms of assessment are still a valid way to evaluate student performance and comprehension. The theoretical framework developed in this paper is grounded in Constructive Alignment (CA) theory and Bloom's taxonomy for defining learning objectives. We argue that AI influences learning objectives of different Bloom levels in a different way, and assessment has to be adopted accordingly. Furthermore, in line with Bloom's vision, formative and summative assessment should be aligned on whether the use of AI is permitted or not.   Although lecturers tend to agree that education and assessment need to be adapted to the presence of AI, a strong bias exists on the extent to which lecturers want to allow for AI in assessment. This bias is caused by a lecturer's familiarity with AI and specifically whether they use it themselves. To avoid this bias, we propose structured guidelines on a university or faculty level, to foster alignment among the staff. Besides that, we argue that teaching staff should be trained on the capabilities and limitations of AI tools. In this way, they are better able to adapt their assessment methods.",
    "title": "The Impact of AI on Educational Assessment: A Framework for Constructive Alignment",
    "date": "2025-06-30",
    "authors": "Patrick Stokkink",
    "id": "2506.23815v2",
    "x": 3.3962674140930176,
    "y": -0.25258970260620117,
    "z": 0.7490864396095276,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 3.3962674140930176,
        "y": -0.25258970260620117,
        "z": 0.7490864396095276
      },
      "starmap": {
        "x": -0.025293856859207153,
        "y": 0.008900998160243034,
        "z": -0.00043156376341357827
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "MadCLIP: Few-shot Medical Anomaly Detection with CLIP",
    "date": "2025-06-30",
    "authors": "Mahshid Shiri, Cigdem Beyan, Vittorio Murino",
    "id": "2506.23810v1",
    "abstract": "An innovative few-shot anomaly detection approach is presented, leveraging the pre-trained CLIP model for medical data, and adapting it for both image-level anomaly classification (AC) and pixel-level anomaly segmentation (AS). A dual-branch design is proposed to separately capture normal and abnormal features through learnable adapters in the CLIP vision encoder. To improve semantic alignment, learnable text prompts are employed to link visual features. Furthermore, SigLIP loss is applied to effectively handle the many-to-one relationship between images and unpaired text prompts, showcasing its adaptation in the medical field for the first time. Our approach is validated on multiple modalities, demonstrating superior performance over existing methods for AC and AS, in both same-dataset and cross-dataset evaluations. Unlike prior work, it does not rely on synthetic data or memory banks, and an ablation study confirms the contribution of each component. The code is available at https://github.com/mahshid1998/MadCLIP.",
    "x": -0.964225172996521,
    "y": 0.13553455471992493,
    "z": -1.1896748542785645,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -0.964225172996521,
        "y": 0.13553455471992493,
        "z": -1.1896748542785645
      },
      "starmap": {
        "x": 0.016419991850852966,
        "y": -0.023455264046788216,
        "z": -0.02171611599624157
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Carl Olsson, Amanda Nilsson",
    "abstract": "A recent series of works has shown that initialization-free BA can be achieved using pseudo Object Space Error (pOSE) as a surrogate objective. The initial reconstruction-step optimizes an objective where all terms are projectively invariant and it cannot incorporate knowledge of the camera calibration. As a result, the solution is only determined up to a projective transformation of the scene and the process requires more data for successful reconstruction.   In contrast, we present a method that is able to use the known camera calibration thereby producing near metric solutions, that is, reconstructions that are accurate up to a similarity transformation. To achieve this we introduce pairwise relative rotation estimates that carry information about camera calibration. These are only invariant to similarity transformations, thus encouraging solutions that preserve metric features of the real scene. Our method can be seen as integrating rotation averaging into the pOSE framework striving towards initialization-free calibrated SfM.   Our experimental evaluation shows that we are able to reliably optimize our objective, achieving convergence to the global minimum with high probability from random starting solutions, resulting in accurate near metric reconstructions.",
    "title": "Towards Initialization-free Calibrated Bundle Adjustment",
    "id": "2506.23808v1",
    "date": "2025-06-30",
    "x": -2.1645472049713135,
    "y": 2.6086580753326416,
    "z": -0.4923575222492218,
    "cluster_id": 1,
    "layouts": {
      "pacmap": {
        "x": -2.1645472049713135,
        "y": 2.6086580753326416,
        "z": -0.4923575222492218
      },
      "starmap": {
        "x": 0.04801604524254799,
        "y": 0.06522603332996368,
        "z": -0.02361162006855011
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23803v1",
    "authors": "Dmitry Kovalev",
    "abstract": "In this paper, we revisit stochastic gradient descent (SGD) with AdaGrad-type preconditioning. Our contributions are twofold. First, we develop a unified convergence analysis of SGD with adaptive preconditioning under anisotropic or matrix smoothness and noise assumptions. This allows us to recover state-of-the-art convergence results for several popular adaptive gradient methods, including AdaGrad-Norm, AdaGrad, and ASGO/One-sided Shampoo. In addition, we establish the fundamental connection between two recently proposed algorithms, Scion and DASGO, and provide the first theoretical guarantees for the latter. Second, we show that the convergence of methods like AdaGrad and DASGO can be provably accelerated beyond the best-known rates using Nesterov momentum. Consequently, we obtain the first theoretical justification that AdaGrad-type algorithms can simultaneously benefit from both diagonal preconditioning and momentum, which may provide an ultimate explanation for the practical efficiency of Adam.",
    "title": "SGD with Adaptive Preconditioning: Unified Analysis and Momentum Acceleration",
    "date": "2025-06-30",
    "x": 1.355614423751831,
    "y": -3.0385375022888184,
    "z": 1.164699912071228,
    "cluster_id": 8,
    "layouts": {
      "pacmap": {
        "x": 1.355614423751831,
        "y": -3.0385375022888184,
        "z": 1.164699912071228
      },
      "starmap": {
        "x": 0.005692875944077969,
        "y": -0.006546473130583763,
        "z": -0.00259357038885355
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "Adaptive Out-of-Control Point Pattern Detection in Sequential Random Finite Set Observations",
    "abstract": "In this work we introduce a novel adaptive anomaly detection framework specifically designed for monitoring sequential random finite set (RFS) observations. Our approach effectively distinguishes between In-Control data (normal) and Out-Of-Control data (anomalies) by detecting deviations from the expected statistical behavior of the process. The primary contributions of this study include the development of an innovative RFS-based framework that not only learns the normal behavior of the data-generating process online but also dynamically adapts to behavioral shifts to accurately identify abnormal point patterns. To achieve this, we introduce a new class of RFS-based posterior distributions, named Power Discounting Posteriors (PD), which facilitate adaptation to systematic changes in data while enabling anomaly detection of point pattern data through a novel predictive posterior density function. The effectiveness of the proposed approach is demonstrated by extensive qualitative and quantitative simulation experiments.",
    "date": "2025-06-30",
    "authors": "Konstantinos Bourazas, Savvas Papaioannou, Panayiotis Kolios",
    "id": "2506.23802v1",
    "x": -0.13563932478427887,
    "y": -1.0185596942901611,
    "z": -1.364222526550293,
    "cluster_id": 6,
    "layouts": {
      "pacmap": {
        "x": -0.13563932478427887,
        "y": -1.0185596942901611,
        "z": -1.364222526550293
      },
      "starmap": {
        "x": 0.011640594340860844,
        "y": -0.013507354073226452,
        "z": -0.02454218827188015
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Super-resolution (SR) techniques can enhance the spatial resolution of remote sensing images by utilizing low-resolution (LR) images to reconstruct high-resolution (HR) images, enabling more efficient large-scale earth observation applications. While single-image super-resolution (SISR) methods have shown progress, reference-based super-resolution (RefSR) offers superior performance by incorporating historical HR images alongside current LR observations. However, existing RefSR methods struggle with real-world complexities, such as cross-sensor resolution gap and significant land cover changes, often leading to under-generation or over-reliance on reference image. To address these challenges, we propose CRefDiff, a novel controllable reference-based diffusion model for real-world remote sensing image SR. To address the under-generation problem, CRefDiff is built upon the pretrained Stable Diffusion model, leveraging its powerful generative prior to produce accurate structures and textures. To mitigate over-reliance on the reference, we introduce a dual-branch fusion mechanism that adaptively integrates both local and global information from the reference image. Moreover, this novel dual-branch design enables reference strength control during inference, enhancing interactivity and flexibility of the model. Finally, a strategy named Better Start is proposed to significantly reduce the number of denoising steps, thereby accelerating the inference process. To support further research, we introduce Real-RefRSSRD, a new real-world RefSR dataset for remote sensing images, consisting of HR NAIP and LR Sentinel-2 image pairs with diverse land cover changes and significant temporal gaps. Extensive experiments on Real-RefRSSRD show that CRefDiff achieves state-of-the-art performance across various metrics and improves downstream tasks such as scene classification and semantic segmentation.",
    "title": "Controllable Reference-Based Real-World Remote Sensing Image Super-Resolution with Generative Diffusion Priors",
    "id": "2506.23801v1",
    "date": "2025-06-30",
    "authors": "Ce Wang, Wanjie Sun",
    "x": -3.5303306579589844,
    "y": 0.3735714554786682,
    "z": -0.022655026987195015,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -3.5303306579589844,
        "y": 0.3735714554786682,
        "z": -0.022655026987195015
      },
      "starmap": {
        "x": 0.02157788723707199,
        "y": -0.010914350859820843,
        "z": 0.03971388563513756
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Predictive coding networks trained with equilibrium propagation are neural models that perform inference through an iterative energy minimization process. Previous studies have demonstrated their effectiveness in shallow architectures, but show significant performance degradation when depth exceeds five to seven layers. In this work, we show that the reason behind this degradation is due to exponentially imbalanced errors between layers during weight updates, and predictions from the previous layer not being effective in guiding updates in deeper layers. We address the first issue by introducing two novel methods to optimize the latent variables that use precision-weighting to re-balance the distribution of energy among layers during the `relaxation phase', and the second issue by proposing a novel weight update mechanism that reduces error accumulation in deeper layers. Empirically, we test our methods on a large number of image classification tasks, resulting in large improvements in test accuracy across networks with more than seven layers, with performances comparable to those of backprop on similar models. These findings suggest that a better understanding of the relaxation phase is important to train models using equilibrium propagation at scale, and open new possibilities for their application in complex tasks.",
    "id": "2506.23800v2",
    "date": "2025-06-30",
    "authors": "Chang Qi, Matteo Forasassi, Thomas Lukasiewicz, Tommaso Salvatori",
    "title": "Towards the Training of Deeper Predictive Coding Neural Networks",
    "x": 0.2978358864784241,
    "y": -2.4546093940734863,
    "z": 0.43823036551475525,
    "cluster_id": 7,
    "layouts": {
      "pacmap": {
        "x": 0.2978358864784241,
        "y": -2.4546093940734863,
        "z": 0.43823036551475525
      },
      "starmap": {
        "x": 0.007804587949067354,
        "y": -0.011126467026770115,
        "z": -0.01093847956508398
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23799v1",
    "title": "KAIROS: Scalable Model-Agnostic Data Valuation",
    "abstract": "Training data increasingly shapes not only model accuracy but also regulatory compliance and market valuation of AI assets. Yet existing valuation methods remain inadequate: model-based techniques depend on a single fitted model and inherit its biases, while algorithm-based approaches such as Data Shapley require costly retrainings at web scale. Recent Wasserstein-based model-agnostic methods rely on approximations that misrank examples relative to their true leave-one-out (LOO) utility. We introduce KAIROS, a scalable, model-agnostic valuation framework that assigns each example a distributional influence score: its contribution to the Maximum Mean Discrepancy (MMD) between the empirical training distribution and a clean reference set. Unlike Wasserstein surrogates, our MMD-based influence admits a closed-form solution that faithfully approximates the exact LOO ranking within $O(1/N^2)$ error, requires no retraining, and naturally extends to conditional kernels for unified label- and feature-error detection. Moreover, KAIROS supports efficient online updates: when a new batch of size m arrives, all scores can be updated in $O(mN)$ time, delivering up to 50x speedup without compromising ranking quality. Empirical evaluations on noise, mislabeling, and poisoning benchmarks show that KAIROS consistently outperforms state-of-the-art model-, Shapley-, and Wasserstein-based baselines in both accuracy and runtime. We provide rigorous theoretical guarantees, including symmetry for reproducible rankings and density-separation for interpretable thresholds.",
    "authors": "Jiongli Zhu, Parjanya Prajakta Prashant, Alex Cloninger, Babak Salimi",
    "date": "2025-06-30",
    "x": 0.3651466369628906,
    "y": -2.6182610988616943,
    "z": -0.5927938222885132,
    "cluster_id": 6,
    "layouts": {
      "pacmap": {
        "x": 0.3651466369628906,
        "y": -2.6182610988616943,
        "z": -0.5927938222885132
      },
      "starmap": {
        "x": 0.006485482212156057,
        "y": -0.01408479269593954,
        "z": -0.015875589102506638
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Anton Andreychuk, Konstantin Yakovlev, Aleksandr Panov, Alexey Skrynnik",
    "title": "Advancing Learnable Multi-Agent Pathfinding Solvers with Active Fine-Tuning",
    "date": "2025-06-30",
    "abstract": "Multi-agent pathfinding (MAPF) is a common abstraction of multi-robot trajectory planning problems, where multiple homogeneous robots simultaneously move in the shared environment. While solving MAPF optimally has been proven to be NP-hard, scalable, and efficient, solvers are vital for real-world applications like logistics, search-and-rescue, etc. To this end, decentralized suboptimal MAPF solvers that leverage machine learning have come on stage. Building on the success of the recently introduced MAPF-GPT, a pure imitation learning solver, we introduce MAPF-GPT-DDG. This novel approach effectively fine-tunes the pre-trained MAPF model using centralized expert data. Leveraging a novel delta-data generation mechanism, MAPF-GPT-DDG accelerates training while significantly improving performance at test time. Our experiments demonstrate that MAPF-GPT-DDG surpasses all existing learning-based MAPF solvers, including the original MAPF-GPT, regarding solution quality across many testing scenarios. Remarkably, it can work with MAPF instances involving up to 1 million agents in a single environment, setting a new milestone for scalability in MAPF domains.",
    "id": "2506.23793v1",
    "x": 4.001147270202637,
    "y": 1.4612236022949219,
    "z": 1.372039556503296,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 4.001147270202637,
        "y": 1.4612236022949219,
        "z": 1.372039556503296
      },
      "starmap": {
        "x": -0.022397387772798538,
        "y": 0.008481204509735107,
        "z": -0.000402848090743646
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "abstract": "We propose VisTex-OVLM, a novel image prompted object detection method that introduces visual textualization -- a process that projects a few visual exemplars into the text feature space to enhance Object-level Vision-Language Models' (OVLMs) capability in detecting rare categories that are difficult to describe textually and nearly absent from their pre-training data, while preserving their pre-trained object-text alignment. Specifically, VisTex-OVLM leverages multi-scale textualizing blocks and a multi-stage fusion strategy to integrate visual information from visual exemplars, generating textualized visual tokens that effectively guide OVLMs alongside text prompts. Unlike previous methods, our method maintains the original architecture of OVLM, maintaining its generalization capabilities while enhancing performance in few-shot settings. VisTex-OVLM demonstrates superior performance across open-set datasets which have minimal overlap with OVLM's pre-training data and achieves state-of-the-art results on few-shot benchmarks PASCAL VOC and MSCOCO. The code will be released at https://github.com/WitGotFlg/VisTex-OVLM.",
    "id": "2506.23785v1",
    "authors": "Yongjian Wu, Yang Zhou, Jiya Saiyin, Bingzheng Wei, Yan Xu",
    "title": "Visual Textualization for Image Prompted Object Detection",
    "x": -0.8436928391456604,
    "y": 1.2435591220855713,
    "z": -0.89570152759552,
    "cluster_id": 3,
    "layouts": {
      "pacmap": {
        "x": -0.8436928391456604,
        "y": 1.2435591220855713,
        "z": -0.89570152759552
      },
      "starmap": {
        "x": 0.01776971109211445,
        "y": -0.041329313069581985,
        "z": -0.026458928361535072
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "title": "When GNNs Met a Word Equations Solver: Learning to Rank Equations (Extended Technical Report)",
    "id": "2506.23784v1",
    "authors": "Parosh Aziz Abdulla, Mohamed Faouzi Atig, Julie Cailler, Chencheng Liang, Philipp R\u00fcmmer",
    "abstract": "Nielsen transformation is a standard approach for solving word equations: by repeatedly splitting equations and applying simplification steps, equations are rewritten until a solution is reached. When solving a conjunction of word equations in this way, the performance of the solver will depend considerably on the order in which equations are processed. In this work, the use of Graph Neural Networks (GNNs) for ranking word equations before and during the solving process is explored. For this, a novel graph-based representation for word equations is presented, preserving global information across conjuncts, enabling the GNN to have a holistic view during ranking. To handle the variable number of conjuncts, three approaches to adapt a multi-classification task to the problem of ranking equations are proposed. The training of the GNN is done with the help of minimum unsatisfiable subsets (MUSes) of word equations. The experimental results show that, compared to state-of-the-art string solvers, the new framework solves more problems in benchmarks where each variable appears at most once in each equation.",
    "x": 0.6562943458557129,
    "y": -3.1259124279022217,
    "z": -0.8773239850997925,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 0.6562943458557129,
        "y": -3.1259124279022217,
        "z": -0.8773239850997925
      },
      "starmap": {
        "x": -0.01282338984310627,
        "y": 0.00045625309576280415,
        "z": -0.00621027871966362
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23783v1",
    "authors": "Shiao Wang, Ju Huang, Qingchuan Ma, Jinfeng Gao, Chunyi Xu, Xiao Wang, Lan Chen, Bo Jiang",
    "abstract": "Combining traditional RGB cameras with bio-inspired event cameras for robust object tracking has garnered increasing attention in recent years. However, most existing multimodal tracking algorithms depend heavily on high-complexity Vision Transformer architectures for feature extraction and fusion across modalities. This not only leads to substantial computational overhead but also limits the effectiveness of cross-modal interactions. In this paper, we propose an efficient RGB-Event object tracking framework based on the linear-complexity Vision Mamba network, termed Mamba-FETrack V2. Specifically, we first design a lightweight Prompt Generator that utilizes embedded features from each modality, together with a shared prompt pool, to dynamically generate modality-specific learnable prompt vectors. These prompts, along with the modality-specific embedded features, are then fed into a Vision Mamba-based FEMamba backbone, which facilitates prompt-guided feature extraction, cross-modal interaction, and fusion in a unified manner. Finally, the fused representations are passed to the tracking head for accurate target localization. Extensive experimental evaluations on multiple RGB-Event tracking benchmarks, including short-term COESOT dataset and long-term datasets, i.e., FE108 and FELT V2, demonstrate the superior performance and efficiency of the proposed tracking framework. The source code and pre-trained models will be released on https://github.com/Event-AHU/Mamba_FETrack",
    "title": "Mamba-FETrack V2: Revisiting State Space Model for Frame-Event based Visual Object Tracking",
    "date": "2025-06-30",
    "x": -1.6801793575286865,
    "y": 0.17848704755306244,
    "z": 0.9544088244438171,
    "cluster_id": 3,
    "layouts": {
      "pacmap": {
        "x": -1.6801793575286865,
        "y": 0.17848704755306244,
        "z": 0.9544088244438171
      },
      "starmap": {
        "x": 0.02028808370232582,
        "y": -0.0375717431306839,
        "z": -0.024494372308254242
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23782v1",
    "title": "Calibrating Graph Neural Networks with Wavelet-Aware Temperature Scaling",
    "authors": "Xiaoyang Li, Linwei Tao, Haohui Lu, Minjing Dong, Junbin Gao, Chang Xu",
    "abstract": "Graph Neural Networks (GNNs) have demonstrated strong predictive performance on relational data; however, their confidence estimates often misalign with actual predictive correctness, posing significant limitations for deployment in safety-critical settings. While existing graph-aware calibration methods seek to mitigate this limitation, they primarily depend on coarse one-hop statistics, such as neighbor-predicted confidence, or latent node embeddings, thereby neglecting the fine-grained structural heterogeneity inherent in graph topology. In this work, we propose Wavelet-Aware Temperature Scaling (WATS), a post-hoc calibration framework that assigns node-specific temperatures based on tunable heat-kernel graph wavelet features. Specifically, WATS harnesses the scalability and topology sensitivity of graph wavelets to refine confidence estimates, all without necessitating model retraining or access to neighboring logits or predictions. Extensive evaluations across seven benchmark datasets with varying graph structures and two GNN backbones demonstrate that WATS achieves the lowest Expected Calibration Error (ECE) among all compared methods, outperforming both classical and graph-specific baselines by up to 42.3\\% in ECE and reducing calibration variance by 17.24\\% on average compared with graph-specific methods. Moreover, WATS remains computationally efficient, scaling well across graphs of diverse sizes and densities. Code will be released based on publication.",
    "date": "2025-06-30",
    "x": 0.9764096140861511,
    "y": -1.7757511138916016,
    "z": -1.5777734518051147,
    "cluster_id": 6,
    "layouts": {
      "pacmap": {
        "x": 0.9764096140861511,
        "y": -1.7757511138916016,
        "z": -1.5777734518051147
      },
      "starmap": {
        "x": 0.014541770331561565,
        "y": -0.00816766731441021,
        "z": -0.02497659996151924
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Jari Peeperkorn, Johannes De Smedt, Jochen De Weerdt",
    "abstract": "Process discovery algorithms automatically extract process models from event logs, but high variability often results in complex and hard-to-understand models. To mitigate this issue, trace clustering techniques group process executions into clusters, each represented by a simpler and more understandable process model. Model-driven trace clustering improves on this by assigning traces to clusters based on their conformity to cluster-specific process models. However, most existing clustering techniques rely on either no process model discovery, or non-stochastic models, neglecting the frequency or probability of activities and transitions, thereby limiting their capability to capture real-world execution dynamics. We propose a novel model-driven trace clustering method that optimizes stochastic process models within each cluster. Our approach uses entropic relevance, a stochastic conformance metric based on directly-follows probabilities, to guide trace assignment. This allows clustering decisions to consider both structural alignment with a cluster's process model and the likelihood that a trace originates from a given stochastic process model. The method is computationally efficient, scales linearly with input size, and improves model interpretability by producing clusters with clearer control-flow patterns. Extensive experiments on public real-life datasets show that our method outperforms existing alternatives in representing process behavior and reveals how clustering performance rankings can shift when stochasticity is considered.",
    "title": "Model-driven Stochastic Trace Clustering",
    "id": "2506.23776v1",
    "date": "2025-06-30",
    "x": -1.1824489831924438,
    "y": -2.645209550857544,
    "z": -0.17502787709236145,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -1.1824489831924438,
        "y": -2.645209550857544,
        "z": -0.17502787709236145
      },
      "starmap": {
        "x": 0.005903960671275854,
        "y": -0.017320752143859863,
        "z": -0.02028985135257244
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23773v1",
    "date": "2025-06-30",
    "title": "BayesL: Towards a Logical Framework for Bayesian Networks",
    "abstract": "We introduce BayesL, a novel logical framework for specifying, querying, and verifying the behaviour of Bayesian networks (BNs). BayesL (pronounced \"Basil\") is a structured language that allows for the creation of queries over BNs. It facilitates versatile reasoning concerning causal and evidence-based relationships, and permits comprehensive what-if scenario evaluations without the need for manual modifications to the model.",
    "authors": "Stefano M. Nicoletti, Mari\u00eblle Stoelinga",
    "x": 2.459115505218506,
    "y": 2.2143971920013428,
    "z": 1.3897764682769775,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 2.459115505218506,
        "y": 2.2143971920013428,
        "z": 1.3897764682769775
      },
      "starmap": {
        "x": -0.02379688061773777,
        "y": 0.009919990785419941,
        "z": -0.0019362189341336489
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Reinforcement Learning (RL) is increasingly used in autonomous driving (AD) and shows clear advantages. However, most RL-based AD methods overlook policy structure design. An RL policy that only outputs short-timescale vehicle control commands results in fluctuating driving behavior due to fluctuations in network outputs, while one that only outputs long-timescale driving goals cannot achieve unified optimality of driving behavior and control. Therefore, we propose a multi-timescale hierarchical reinforcement learning approach. Our approach adopts a hierarchical policy structure, where high- and low-level RL policies are unified-trained to produce long-timescale motion guidance and short-timescale control commands, respectively. Therein, motion guidance is explicitly represented by hybrid actions to capture multimodal driving behaviors on structured road and support incremental low-level extend-state updates. Additionally, a hierarchical safety mechanism is designed to ensure multi-timescale safety. Evaluation in simulator-based and HighD dataset-based highway multi-lane scenarios demonstrates that our approach significantly improves AD performance, effectively increasing driving efficiency, action consistency and safety.",
    "authors": "Guizhe Jin, Zhuoren Li, Bo Leng, Ran Yu, Lu Xiong",
    "date": "2025-06-30",
    "id": "2506.23771v1",
    "title": "Multi-Timescale Hierarchical Reinforcement Learning for Unified Behavior and Control of Autonomous Driving",
    "x": 0.32900533080101013,
    "y": 0.5271496772766113,
    "z": 2.196451187133789,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 0.32900533080101013,
        "y": 0.5271496772766113,
        "z": 2.196451187133789
      },
      "starmap": {
        "x": -0.021464001387357712,
        "y": 0.008797602728009224,
        "z": 0.0017823444213718176
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23767v1",
    "title": "Explainable AI for Comprehensive Risk Assessment for Financial Reports: A Lightweight Hierarchical Transformer Network Approach",
    "abstract": "Every publicly traded U.S. company files an annual 10-K report containing critical insights into financial health and risk. We propose Tiny eXplainable Risk Assessor (TinyXRA), a lightweight and explainable transformer-based model that automatically assesses company risk from these reports. Unlike prior work that relies solely on the standard deviation of excess returns (adjusted for the Fama-French model), which indiscriminately penalizes both upside and downside risk, TinyXRA incorporates skewness, kurtosis, and the Sortino ratio for more comprehensive risk assessment. We leverage TinyBERT as our encoder to efficiently process lengthy financial documents, coupled with a novel dynamic, attention-based word cloud mechanism that provides intuitive risk visualization while filtering irrelevant terms. This lightweight design ensures scalable deployment across diverse computing environments with real-time processing capabilities for thousands of financial documents which is essential for production systems with constrained computational resources. We employ triplet loss for risk quartile classification, improving over pairwise loss approaches in existing literature by capturing both the direction and magnitude of risk differences. Our TinyXRA achieves state-of-the-art predictive accuracy across seven test years on a dataset spanning 2013-2024, while providing transparent and interpretable risk assessments. We conduct comprehensive ablation studies to evaluate our contributions and assess model explanations both quantitatively by systematically removing highly attended words and sentences, and qualitatively by examining explanation coherence. The paper concludes with findings, practical implications, limitations, and future research directions.",
    "date": "2025-06-30",
    "authors": "Xue Wen Tan, Stanley Kok",
    "x": 1.197445273399353,
    "y": -0.49319010972976685,
    "z": -1.2604739665985107,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": 1.197445273399353,
        "y": -0.49319010972976685,
        "z": -1.2604739665985107
      },
      "starmap": {
        "x": -0.006649772170931101,
        "y": -0.00921852421015501,
        "z": -0.007711478974670172
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "The rapid advancement of large language models (LLMs) has redefined artificial intelligence (AI), pushing the boundaries of AI research and enabling unbounded possibilities for both academia and the industry. However, LLM development faces increasingly complex challenges throughout its lifecycle, yet no existing research systematically explores these challenges and solutions from the perspective of software engineering (SE) approaches. To fill the gap, we systematically analyze research status throughout the LLM development lifecycle, divided into six phases: requirements engineering, dataset construction, model development and enhancement, testing and evaluation, deployment and operations, and maintenance and evolution. We then conclude by identifying the key challenges for each phase and presenting potential research directions to address these challenges. In general, we provide valuable insights from an SE perspective to facilitate future advances in LLM development.",
    "authors": "Hongzhou Rao, Yanjie Zhao, Xinyi Hou, Shenao Wang, Haoyu Wang",
    "id": "2506.23762v1",
    "title": "Software Engineering for Large Language Models: Research Status, Challenges and the Road Ahead",
    "date": "2025-06-30",
    "x": 3.288479804992676,
    "y": 0.2610183358192444,
    "z": 0.6599526405334473,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 3.288479804992676,
        "y": 0.2610183358192444,
        "z": 0.6599526405334473
      },
      "starmap": {
        "x": -0.024178821593523026,
        "y": 0.008398042991757393,
        "z": -0.0005567498155869544
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Zheng Fang, Xiaoming Qi, Chun-Mei Feng, Jialun Pei, Weixin Si, Yueming Jin",
    "id": "2506.23759v1",
    "date": "2025-06-30",
    "title": "Spatio-Temporal Representation Decoupling and Enhancement for Federated Instrument Segmentation in Surgical Videos",
    "abstract": "Surgical instrument segmentation under Federated Learning (FL) is a promising direction, which enables multiple surgical sites to collaboratively train the model without centralizing datasets. However, there exist very limited FL works in surgical data science, and FL methods for other modalities do not consider inherent characteristics in surgical domain: i) different scenarios show diverse anatomical backgrounds while highly similar instrument representation; ii) there exist surgical simulators which promote large-scale synthetic data generation with minimal efforts. In this paper, we propose a novel Personalized FL scheme, Spatio-Temporal Representation Decoupling and Enhancement (FedST), which wisely leverages surgical domain knowledge during both local-site and global-server training to boost segmentation. Concretely, our model embraces a Representation Separation and Cooperation (RSC) mechanism in local-site training, which decouples the query embedding layer to be trained privately, to encode respective backgrounds. Meanwhile, other parameters are optimized globally to capture the consistent representations of instruments, including the temporal layer to capture similar motion patterns. A textual-guided channel selection is further designed to highlight site-specific features, facilitating model adapta tion to each site. Moreover, in global-server training, we propose Synthesis-based Explicit Representation Quantification (SERQ), which defines an explicit representation target based on synthetic data to synchronize the model convergence during fusion for improving model generalization.",
    "x": -0.9085124731063843,
    "y": -0.39639779925346375,
    "z": 0.5896303057670593,
    "cluster_id": 4,
    "layouts": {
      "pacmap": {
        "x": -0.9085124731063843,
        "y": -0.39639779925346375,
        "z": 0.5896303057670593
      },
      "starmap": {
        "x": 0.02325814962387085,
        "y": -0.009443655610084534,
        "z": -0.014835835434496403
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23757v1",
    "title": "Training of Spiking Neural Networks with Expectation-Propagation",
    "abstract": "In this paper, we propose a unifying message-passing framework for training spiking neural networks (SNNs) using Expectation-Propagation. Our gradient-free method is capable of learning the marginal distributions of network parameters and simultaneously marginalizes nuisance parameters, such as the outputs of hidden layers. This framework allows for the first time, training of discrete and continuous weights, for deterministic and stochastic spiking networks, using batches of training samples. Although its convergence is not ensured, the algorithm converges in practice faster than gradient-based methods, without requiring a large number of passes through the training data. The classification and regression results presented pave the way for new efficient training methods for deep Bayesian networks.",
    "authors": "Dan Yao, Steve McLaughlin, Yoann Altmann",
    "date": "2025-06-30",
    "x": 0.36462777853012085,
    "y": -2.5737364292144775,
    "z": 0.11992065608501434,
    "cluster_id": 7,
    "layouts": {
      "pacmap": {
        "x": 0.36462777853012085,
        "y": -2.5737364292144775,
        "z": 0.11992065608501434
      },
      "starmap": {
        "x": 0.008655251935124397,
        "y": -0.008382987231016159,
        "z": -0.012826180085539818
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "title": "Can We Challenge Open-Vocabulary Object Detectors with Generated Content in Street Scenes?",
    "id": "2506.23751v1",
    "authors": "Annika M\u00fctze, Sadia Ilyas, Christian D\u00f6rpelkus, Matthias Rottmann",
    "abstract": "Open-vocabulary object detectors such as Grounding DINO are trained on vast and diverse data, achieving remarkable performance on challenging datasets. Due to that, it is unclear where to find their limitations, which is of major concern when using in safety-critical applications. Real-world data does not provide sufficient control, required for a rigorous evaluation of model generalization. In contrast, synthetically generated data allows to systematically explore the boundaries of model competence/generalization. In this work, we address two research questions: 1) Can we challenge open-vocabulary object detectors with generated image content? 2) Can we find systematic failure modes of those models? To address these questions, we design two automated pipelines using stable diffusion to inpaint unusual objects with high diversity in semantics, by sampling multiple substantives from WordNet and ChatGPT. On the synthetically generated data, we evaluate and compare multiple open-vocabulary object detectors as well as a classical object detector. The synthetic data is derived from two real-world datasets, namely LostAndFound, a challenging out-of-distribution (OOD) detection benchmark, and the NuImages dataset. Our results indicate that inpainting can challenge open-vocabulary object detectors in terms of overlooking objects. Additionally, we find a strong dependence of open-vocabulary models on object location, rather than on object semantics. This provides a systematic approach to challenge open-vocabulary models and gives valuable insights on how data could be acquired to effectively improve these models.",
    "x": -0.37485432624816895,
    "y": 1.1598637104034424,
    "z": -1.024061679840088,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -0.37485432624816895,
        "y": 1.1598637104034424,
        "z": -1.024061679840088
      },
      "starmap": {
        "x": 0.022491997107863426,
        "y": 0.001249102409929037,
        "z": -0.01797470450401306
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23743v2",
    "date": "2025-06-30",
    "authors": "Tiziano Labruna, Simone Gallo, Giovanni Da San Martino",
    "abstract": "Positional bias in binary question answering occurs when a model systematically favors one choice over another based solely on the ordering of presented options. In this study, we quantify and analyze positional bias across five large language models under varying degrees of answer uncertainty. We re-adapted the SQuAD-it dataset by adding an extra incorrect answer option and then created multiple versions with progressively less context and more out-of-context answers, yielding datasets that range from low to high uncertainty. Additionally, we evaluate two naturally higher-uncertainty benchmarks: (1) WebGPT - question pairs with unequal human-assigned quality scores, and (2) Winning Arguments - where models predict the more persuasive argument in Reddit's r/ChangeMyView exchanges. Across each dataset, the order of the \"correct\" (or higher-quality/persuasive) option is systematically flipped (first placed in position 1, then in position 2) to compute both Preference Fairness and Position Consistency. We observe that positional bias is nearly absent under low-uncertainty conditions, but grows exponentially when it becomes doubtful to decide which option is correct.",
    "title": "Positional Bias in Binary Question Answering: How Uncertainty Shapes Model Preferences",
    "x": 2.6071667671203613,
    "y": 0.16140063107013702,
    "z": -1.0296931266784668,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 2.6071667671203613,
        "y": 0.16140063107013702,
        "z": -1.0296931266784668
      },
      "starmap": {
        "x": -0.02135504223406315,
        "y": 0.006539571564644575,
        "z": 0.00104984559584409
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "abstract": "The Gene-pool Optimal Mixing EA (GOMEA) family of EAs offers a specific means to exploit problem-specific knowledge through linkage learning, i.e., inter-variable dependency detection, expressed using subsets of variables, that should undergo joint variation. Such knowledge can be exploited if faster fitness evaluations are possible when only a few variables are changed in a solution, enabling large speed-ups. The recent-most version of Real-Valued GOMEA (RV-GOMEA) can learn a conditional linkage model during optimization using fitness-based linkage learning, enabling fine-grained dependency exploitation in learning and sampling a Gaussian distribution. However, while the most efficient Gaussian-based EAs, like NES and CMA-ES, employ incremental learning of the Gaussian distribution rather than performing full re-estimation every generation, the recent-most RV-GOMEA version does not employ such incremental learning. In this paper, we therefore study whether incremental distribution estimation can lead to efficiency enhancements of RV-GOMEA. We consider various benchmark problems with varying degrees of overlapping dependencies. We find that, compared to RV-GOMEA and VKD-CMA-ES, the required number of evaluations to reach high-quality solutions can be reduced by a factor of up to 1.5 if population sizes are tuned problem-specifically, while a reduction by a factor of 2-3 can be achieved with generic population-sizing guidelines.",
    "id": "2506.23738v1",
    "title": "More Efficient Real-Valued Gray-Box Optimization through Incremental Distribution Estimation in RV-GOMEA",
    "authors": "Renzo J. Scholman, Tanja Alderliesten, Peter A. N. Bosman",
    "x": 1.192925214767456,
    "y": -3.416076183319092,
    "z": -1.6248050928115845,
    "cluster_id": 6,
    "layouts": {
      "pacmap": {
        "x": 1.192925214767456,
        "y": -3.416076183319092,
        "z": -1.6248050928115845
      },
      "starmap": {
        "x": 0.013031866401433945,
        "y": -0.008613171987235546,
        "z": -0.019942190498113632
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23735v1",
    "abstract": "Large language models (LLMs) have shown remarkable performance on various tasks, but existing evaluation benchmarks are often static and insufficient to fully assess their robustness and generalization in realistic scenarios. Prior work using evolutionary or adversarial data augmentation has improved evaluation diversity but lacks systematic control over perturbation types and multi-step complexity, limiting comprehensive robustness analysis. To address these gaps, we propose AutoEvoEval, an evolution-based evaluation framework for close-ended tasks such as multi-choice question answering. AutoEvoEval introduces 22 interpretable atomic evolution operations and supports multi-round compositions, enabling controlled generation of diverse, challenging, and realistic test samples. We conduct extensive experiments addressing four research questions on a broad set of open- and closed-source LLMs. Our results show that atomic operations cause an average accuracy drop of 7.283\\%, with structure-disrupting or misleading semantic edits causing the largest declines. Model sensitivities vary significantly for the same perturbation, and combining multiple evolution steps amplifies adversarial effects by up to 52.932\\%. These findings suggest current benchmarks may overestimate true model generalization and emphasize the need for evolution-aware robustness evaluation. Code and resources are available at: https://github.com/SYSUSELab/AutoEvoEval.",
    "authors": "JiaRu Wu, Mingwei Liu",
    "title": "AutoEvoEval: An Automated Framework for Evolving Close-Ended LLM Evaluation Data",
    "date": "2025-06-30",
    "x": 2.495082139968872,
    "y": 1.205701231956482,
    "z": -0.30361416935920715,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 2.495082139968872,
        "y": 1.205701231956482,
        "z": -0.30361416935920715
      },
      "starmap": {
        "x": -0.020316962152719498,
        "y": 0.0056793405674397945,
        "z": -0.001089148223400116
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "title": "Marker Gene Method : Identifying Stable Solutions in a Dynamic Environment",
    "id": "2506.23734v1",
    "authors": "Hao Shi, Xi Li, Fangfang Xie",
    "abstract": "Competitive Co-evolutionary Algorithms (CCEAs) are often hampered by complex dynamics like intransitivity and the Red Queen effect, leading to unstable convergence. To counter these challenges, this paper introduces the Marker Gene Method (MGM), a framework that establishes stability by using a 'marker gene' as a dynamic benchmark and an adaptive weighting mechanism to balance exploration and exploitation. We provide rigorous mathematical proofs demonstrating that MGM creates strong attractors near Nash Equilibria within the Strictly Competitive Game framework. Empirically, MGM demonstrates its efficacy across a spectrum of challenges: it stabilizes the canonical Rock-Paper-Scissors game, significantly improves the performance of C-RMOEA/D on ZDT benchmarks, and, when augmented with a Memory Pool (MP) extension, it successfully tames the notoriously pathological Shapley Biased Game. This work presents a theoretically sound and empirically validated framework that substantially enhances the stability and robustness of CCEAs in complex competitive environments.",
    "x": 1.678356647491455,
    "y": -2.6145663261413574,
    "z": 1.130256175994873,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 1.678356647491455,
        "y": -2.6145663261413574,
        "z": 1.130256175994873
      },
      "starmap": {
        "x": -0.022500189021229744,
        "y": 0.008047462441027164,
        "z": -0.001675904612056911
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23731v1",
    "abstract": "Image generative models have become increasingly popular, but training them requires large datasets that are costly to collect and curate. To circumvent these costs, some parties may exploit existing models by using the generated images as training data for their own models. In general, watermarking is a valuable tool for detecting unauthorized use of generated images. However, when these images are used to train a new model, watermarking can only enable detection if the watermark persists through training and remains identifiable in the outputs of the newly trained model - a property known as radioactivity. We analyze the radioactivity of watermarks in images generated by diffusion models (DMs) and image autoregressive models (IARs). We find that existing watermarking methods for DMs fail to retain radioactivity, as watermarks are either erased during encoding into the latent space or lost in the noising-denoising process (during the training in the latent space). Meanwhile, despite IARs having recently surpassed DMs in image generation quality and efficiency, no radioactive watermarking methods have been proposed for them. To overcome this limitation, we propose the first watermarking method tailored for IARs and with radioactivity in mind - drawing inspiration from techniques in large language models (LLMs), which share IARs' autoregressive paradigm. Our extensive experimental evaluation highlights our method's effectiveness in preserving radioactivity within IARs, enabling robust provenance tracking, and preventing unauthorized use of their generated images.",
    "title": "Radioactive Watermarks in Diffusion and Autoregressive Image Generative Models",
    "authors": "Michel Meintz, Jan Dubi\u0144ski, Franziska Boenisch, Adam Dziedzic",
    "date": "2025-06-30",
    "x": -3.139075517654419,
    "y": 0.12451973557472229,
    "z": -1.0022801160812378,
    "cluster_id": 2,
    "layouts": {
      "pacmap": {
        "x": -3.139075517654419,
        "y": 0.12451973557472229,
        "z": -1.0022801160812378
      },
      "starmap": {
        "x": 0.020740708336234093,
        "y": -0.004874178674072027,
        "z": 0.06620756536722183
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "Proteus-ID: ID-Consistent and Motion-Coherent Video Customization",
    "abstract": "Video identity customization seeks to synthesize realistic, temporally coherent videos of a specific subject, given a single reference image and a text prompt. This task presents two core challenges: (1) maintaining identity consistency while aligning with the described appearance and actions, and (2) generating natural, fluid motion without unrealistic stiffness. To address these challenges, we introduce Proteus-ID, a novel diffusion-based framework for identity-consistent and motion-coherent video customization. First, we propose a Multimodal Identity Fusion (MIF) module that unifies visual and textual cues into a joint identity representation using a Q-Former, providing coherent guidance to the diffusion model and eliminating modality imbalance. Second, we present a Time-Aware Identity Injection (TAII) mechanism that dynamically modulates identity conditioning across denoising steps, improving fine-detail reconstruction. Third, we propose Adaptive Motion Learning (AML), a self-supervised strategy that reweights the training loss based on optical-flow-derived motion heatmaps, enhancing motion realism without requiring additional inputs. To support this task, we construct Proteus-Bench, a high-quality dataset comprising 200K curated clips for training and 150 individuals from diverse professions and ethnicities for evaluation. Extensive experiments demonstrate that Proteus-ID outperforms prior methods in identity preservation, text alignment, and motion quality, establishing a new benchmark for video identity customization. Codes and data are publicly available at https://grenoble-zhang.github.io/Proteus-ID/.",
    "date": "2025-06-30",
    "id": "2506.23729v1",
    "authors": "Guiyu Zhang, Chen Shi, Zijian Jiang, Xunzhi Xiang, Jingjing Qian, Shaoshuai Shi, Li Jiang",
    "x": -2.706345319747925,
    "y": 1.5094648599624634,
    "z": 1.3706543445587158,
    "cluster_id": 2,
    "layouts": {
      "pacmap": {
        "x": -2.706345319747925,
        "y": 1.5094648599624634,
        "z": 1.3706543445587158
      },
      "starmap": {
        "x": 0.024625170975923538,
        "y": -0.003607583697885275,
        "z": 0.06652051955461502
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Bartlomiej Sobieski, Matthew Tivnan, Yuang Wang, Siyeop Yoon, Pengfei Jin, Dufan Wu, Quanzheng Li, Przemyslaw Biecek",
    "date": "2025-06-30",
    "id": "2506.23726v1",
    "title": "System-Embedded Diffusion Bridge Models",
    "abstract": "Solving inverse problems -- recovering signals from incomplete or noisy measurements -- is fundamental in science and engineering. Score-based generative models (SGMs) have recently emerged as a powerful framework for this task. Two main paradigms have formed: unsupervised approaches that adapt pretrained generative models to inverse problems, and supervised bridge methods that train stochastic processes conditioned on paired clean and corrupted data. While the former typically assume knowledge of the measurement model, the latter have largely overlooked this structural information. We introduce System embedded Diffusion Bridge Models (SDBs), a new class of supervised bridge methods that explicitly embed the known linear measurement system into the coefficients of a matrix-valued SDE. This principled integration yields consistent improvements across diverse linear inverse problems and demonstrates robust generalization under system misspecification between training and deployment, offering a promising solution to real-world applications.",
    "x": -2.0037126541137695,
    "y": -2.412184476852417,
    "z": -0.8826814293861389,
    "cluster_id": 8,
    "layouts": {
      "pacmap": {
        "x": -2.0037126541137695,
        "y": -2.412184476852417,
        "z": -0.8826814293861389
      },
      "starmap": {
        "x": 0.0050169131718575954,
        "y": -0.01649928279221058,
        "z": -0.007260942831635475
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Vision-Language Models (VLMs) are increasingly pivotal for generalist robot manipulation, enabling tasks such as physical reasoning, policy generation, and failure detection. However, their proficiency in these high-level applications often assumes a deep understanding of low-level physical prerequisites, a capability that remains largely unverified. For robots to perform actions reliably, they must comprehend intrinsic object properties (e.g., material, weight), action affordances (e.g., graspable, stackable), and physical constraints (e.g., stability, reachability, or an object's state, such as being closed). Despite the widespread use of VLMs in manipulation tasks, we argue that off-the-shelf models may lack this granular, physically grounded understanding, as such prerequisites are often overlooked during training.   To address this critical gap, we introduce PAC Bench, a comprehensive benchmark designed to systematically evaluate VLMs on their understanding of core Properties, Affordances, and Constraints (PAC) from a task executability perspective. PAC Bench features a diverse dataset with over 30,000 annotations, comprising 673 real-world images (115 object classes, 15 property types, and 1 to 3 affordances defined per class), 100 real-world humanoid-view scenarios, and 120 unique simulated constraint scenarios across four tasks.   Our evaluations reveal significant gaps in the ability of current VLMs to grasp fundamental physical concepts, highlighting limitations in their suitability for reliable robot manipulation and pointing to key areas for targeted research. PAC Bench also serves as a standardized benchmark for rigorously evaluating physical reasoning in VLMs and guiding the development of more robust, physically grounded models for robotic applications.   Project Page: https://pacbench.github.io/",
    "authors": "Atharva Gundawar, Som Sagar, Ransalu Senanayake",
    "title": "PAC Bench: Do Foundation Models Understand Prerequisites for Executing Manipulation Policies?",
    "date": "2025-06-30",
    "id": "2506.23725v1",
    "x": 0.06208314746618271,
    "y": 2.174260377883911,
    "z": -0.10109089314937592,
    "cluster_id": 3,
    "layouts": {
      "pacmap": {
        "x": 0.06208314746618271,
        "y": 2.174260377883911,
        "z": -0.10109089314937592
      },
      "starmap": {
        "x": 0.013534024357795715,
        "y": -0.035638537257909775,
        "z": -0.02611796371638775
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "When Small Guides Large: Cross-Model Co-Learning for Test-Time Adaptation",
    "abstract": "Test-time Adaptation (TTA) adapts a given model to testing domain data with potential domain shifts through online unsupervised learning, yielding impressive performance. However, to date, existing TTA methods primarily focus on single-model adaptation. In this work, we investigate an intriguing question: how does cross-model knowledge influence the TTA process? Our findings reveal that, in TTA's unsupervised online setting, each model can provide complementary, confident knowledge to the others, even when there are substantial differences in model size. For instance, a smaller model like MobileViT (10.6M parameters) can effectively guide a larger model like ViT-Base (86.6M parameters). In light of this, we propose COCA, a Cross-Model Co-Learning framework for TTA, which mainly consists of two main strategies. 1) Co-adaptation adaptively integrates complementary knowledge from other models throughout the TTA process, reducing individual model biases. 2) Self-adaptation enhances each model's unique strengths via unsupervised learning, enabling diverse adaptation to the target domain. Extensive experiments show that COCA, which can also serve as a plug-and-play module, significantly boosts existing SOTAs, on models with various sizes--including ResNets, ViTs, and Mobile-ViTs--via cross-model co-learned TTA. For example, with Mobile-ViT's guidance, COCA raises ViT-Base's average adaptation accuracy on ImageNet-C from 51.7% to 64.5%. The code is publicly available at https://github.com/ycarobot/COCA.",
    "authors": "Chang'an Yi, Xiaohui Deng, Guohao Chen, Yan Zhou, Qinghua Lu, Shuaicheng Niu",
    "date": "2025-06-30",
    "id": "2506.23724v1",
    "x": -0.9315291047096252,
    "y": -1.7068567276000977,
    "z": -1.0527461767196655,
    "cluster_id": 5,
    "layouts": {
      "pacmap": {
        "x": -0.9315291047096252,
        "y": -1.7068567276000977,
        "z": -1.0527461767196655
      },
      "starmap": {
        "x": 0.015349267981946468,
        "y": -0.026605218648910522,
        "z": -0.021469196304678917
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Gijs Luijten, Roberto Maria Scardigno, Lisle Faray de Paiva, Peter Hoyer, Jens Kleesiek, Domenico Buongiorno, Vitoantonio Bevilacqua, Jan Egger",
    "abstract": "Ultrasound (US) is widely accessible and radiation-free but has a steep learning curve due to its dynamic nature and non-standard imaging planes. Additionally, the constant need to shift focus between the US screen and the patient poses a challenge. To address these issues, we integrate deep learning (DL)-based semantic segmentation for real-time (RT) automated kidney volumetric measurements, which are essential for clinical assessment but are traditionally time-consuming and prone to fatigue. This automation allows clinicians to concentrate on image interpretation rather than manual measurements. Complementing DL, augmented reality (AR) enhances the usability of US by projecting the display directly into the clinician's field of view, improving ergonomics and reducing the cognitive load associated with screen-to-patient transitions. Two AR-DL-assisted US pipelines on HoloLens-2 are proposed: one streams directly via the application programming interface for a wireless setup, while the other supports any US device with video output for broader accessibility. We evaluate RT feasibility and accuracy using the Open Kidney Dataset and open-source segmentation models (nnU-Net, Segmenter, YOLO with MedSAM and LiteMedSAM). Our open-source GitHub pipeline includes model implementations, measurement algorithms, and a Wi-Fi-based streaming solution, enhancing US training and diagnostics, especially in point-of-care settings.",
    "title": "Deep Learning-Based Semantic Segmentation for Real-Time Kidney Imaging and Measurements with Augmented Reality-Assisted Ultrasound",
    "id": "2506.23721v1",
    "date": "2025-06-30",
    "x": -2.636017322540283,
    "y": -0.28263092041015625,
    "z": -1.4718739986419678,
    "cluster_id": 4,
    "layouts": {
      "pacmap": {
        "x": -2.636017322540283,
        "y": -0.28263092041015625,
        "z": -1.4718739986419678
      },
      "starmap": {
        "x": 0.024278776720166206,
        "y": -0.035917095839977264,
        "z": -0.005974042695015669
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "id": "2506.23719v1",
    "title": "DABstep: Data Agent Benchmark for Multi-step Reasoning",
    "authors": "Alex Egg, Martin Iglesias Goyanes, Friso Kingma, Andreu Mora, Leandro von Werra, Thomas Wolf",
    "abstract": "We introduce DABstep, a novel benchmark for evaluating AI agents on realistic multi-step data analysis tasks. DABstep comprises over 450 real-world challenges derived from a financial analytics platform, requiring models to combine code-based data processing with contextual reasoning over heterogeneous documentation. Each task demands an iterative, multi-step problem-solving approach, testing capabilities in data manipulation, cross-referencing multiple sources, and precise result reporting. The benchmark provides a factoid-style answer format with automatic correctness checks for objective scoring at scale. We evaluate leading LLM-based agents, revealing a substantial performance gap: even the best agent achieves only 14.55% accuracy on the hardest tasks. We detail our benchmark's design, dataset composition, task formulation, evaluation protocol, report baseline results and analyze failure modes. DABstep is released with a public leaderboard and toolkit to accelerate research in autonomous data analysis.",
    "x": 2.5526063442230225,
    "y": 1.1907610893249512,
    "z": 0.742398202419281,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 2.5526063442230225,
        "y": 1.1907610893249512,
        "z": 0.742398202419281
      },
      "starmap": {
        "x": -0.024909628555178642,
        "y": 0.009911800734698772,
        "z": -0.001307606347836554
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23717v1",
    "date": "2025-06-30",
    "authors": "Xingting Yao, Qinghao Hu, Fei Zhou, Tielong Liu, Gang Li, Peisong Wang, Jian Cheng",
    "title": "Towards Efficient and Accurate Spiking Neural Networks via Adaptive Bit Allocation",
    "abstract": "Multi-bit spiking neural networks (SNNs) have recently become a heated research spot, pursuing energy-efficient and high-accurate AI. However, with more bits involved, the associated memory and computation demands escalate to the point where the performance improvements become disproportionate. Based on the insight that different layers demonstrate different importance and extra bits could be wasted and interfering, this paper presents an adaptive bit allocation strategy for direct-trained SNNs, achieving fine-grained layer-wise allocation of memory and computation resources. Thus, SNN's efficiency and accuracy can be improved. Specifically, we parametrize the temporal lengths and the bit widths of weights and spikes, and make them learnable and controllable through gradients. To address the challenges caused by changeable bit widths and temporal lengths, we propose the refined spiking neuron, which can handle different temporal lengths, enable the derivation of gradients for temporal lengths, and suit spike quantization better. In addition, we theoretically formulate the step-size mismatch problem of learnable bit widths, which may incur severe quantization errors to SNN, and accordingly propose the step-size renewal mechanism to alleviate this issue. Experiments on various datasets, including the static CIFAR and ImageNet and the dynamic CIFAR-DVS and DVS-GESTURE, demonstrate that our methods can reduce the overall memory and computation cost while achieving higher accuracy. Particularly, our SEWResNet-34 can achieve a 2.69\\% accuracy gain and 4.16$\\times$ lower bit budgets over the advanced baseline work on ImageNet. This work will be fully open-sourced.",
    "x": 0.3578437268733978,
    "y": -2.4077718257904053,
    "z": 0.9371533989906311,
    "cluster_id": 7,
    "layouts": {
      "pacmap": {
        "x": 0.3578437268733978,
        "y": -2.4077718257904053,
        "z": 0.9371533989906311
      },
      "starmap": {
        "x": 0.009096127934753895,
        "y": -0.013592180795967579,
        "z": -0.013422691263258457
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "Towards an Automated Multimodal Approach for Video Summarization: Building a Bridge Between Text, Audio and Facial Cue-Based Summarization",
    "date": "2025-06-30",
    "authors": "Md Moinul Islam, Sofoklis Kakouros, Janne Heikkil\u00e4, Mourad Oussalah",
    "abstract": "The increasing volume of video content in educational, professional, and social domains necessitates effective summarization techniques that go beyond traditional unimodal approaches. This paper proposes a behaviour-aware multimodal video summarization framework that integrates textual, audio, and visual cues to generate timestamp-aligned summaries. By extracting prosodic features, textual cues and visual indicators, the framework identifies semantically and emotionally important moments. A key contribution is the identification of bonus words, which are terms emphasized across multiple modalities and used to improve the semantic relevance and expressive clarity of the summaries. The approach is evaluated against pseudo-ground truth (pGT) summaries generated using LLM-based extractive method. Experimental results demonstrate significant improvements over traditional extractive method, such as the Edmundson method, in both text and video-based evaluation metrics. Text-based metrics show ROUGE-1 increasing from 0.4769 to 0.7929 and BERTScore from 0.9152 to 0.9536, while in video-based evaluation, our proposed framework improves F1-Score by almost 23%. The findings underscore the potential of multimodal integration in producing comprehensive and behaviourally informed video summaries.",
    "id": "2506.23714v1",
    "x": -1.9437817335128784,
    "y": 2.048773765563965,
    "z": 1.3854471445083618,
    "cluster_id": 3,
    "layouts": {
      "pacmap": {
        "x": -1.9437817335128784,
        "y": 2.048773765563965,
        "z": 1.3854471445083618
      },
      "starmap": {
        "x": 0.019174160435795784,
        "y": -0.03013698011636734,
        "z": -0.00952000543475151
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Haoyang Chen, Dongfang Sun, Caoyuan Ma, Shiqin Wang, Kewei Zhang, Zheng Wang, Zhixiang Wang",
    "id": "2506.23711v1",
    "abstract": "We propose Subjective Camera, a human-as-imaging-device paradigm that reconstructs real-world scenes from mental impressions through synergistic use of verbal descriptions and progressive rough sketches. This approach overcomes dual limitations of language ambiguity and sketch abstraction by treating the user's drawing sequence as priors, effectively translating subjective perceptual expectations into photorealistic images.   Existing approaches face three fundamental barriers: (1) user-specific subjective input biases, (2) huge modality gap between planar sketch and 3D priors in diffusion, and (3) sketch quality-sensitive performance degradation. Current solutions either demand resource-intensive model adaptation or impose impractical requirements on sketch precision.   Our framework addresses these challenges through concept-sequential generation. (1) We establish robust appearance priors through text-reward optimization, and then implement sequence-aware disentangled generation that processes concepts in sketching order; these steps accommodate user-specific subjective expectation in a train-free way. (2) We employ latent optimization that effectively bridges the modality gap between planar sketches and 3D priors in diffusion. (3) Our hierarchical reward-guided framework enables the use of rough sketches without demanding artistic expertise. Comprehensive evaluation across diverse datasets demonstrates that our approach achieves state-of-the-art performance in maintaining both semantic and spatial coherence.",
    "title": "Subjective Camera: Bridging Human Cognition and Visual Reconstruction through Sequence-Aware Sketch-Guided Diffusion",
    "date": "2025-06-30",
    "x": -1.1520428657531738,
    "y": 1.869603157043457,
    "z": -1.305679440498352,
    "cluster_id": 2,
    "layouts": {
      "pacmap": {
        "x": -1.1520428657531738,
        "y": 1.869603157043457,
        "z": -1.305679440498352
      },
      "starmap": {
        "x": 0.022650182247161865,
        "y": 0.0023550251498818398,
        "z": 0.07762238383293152
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "Attestable Audits: Verifiable AI Safety Benchmarks Using Trusted Execution Environments",
    "id": "2506.23706v1",
    "abstract": "Benchmarks are important measures to evaluate safety and compliance of AI models at scale. However, they typically do not offer verifiable results and lack confidentiality for model IP and benchmark datasets. We propose Attestable Audits, which run inside Trusted Execution Environments and enable users to verify interaction with a compliant AI model. Our work protects sensitive data even when model provider and auditor do not trust each other. This addresses verification challenges raised in recent AI governance frameworks. We build a prototype demonstrating feasibility on typical audit benchmarks against Llama-3.1.",
    "date": "2025-06-30",
    "authors": "Christoph Schnabl, Daniel Hugenroth, Bill Marino, Alastair R. Beresford",
    "x": 2.997028112411499,
    "y": -0.15554188191890717,
    "z": 1.2093101739883423,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 2.997028112411499,
        "y": -0.15554188191890717,
        "z": 1.2093101739883423
      },
      "starmap": {
        "x": -0.025119757279753685,
        "y": 0.009373404085636139,
        "z": -0.0008113944786600769
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "title": "Single Image Test-Time Adaptation via Multi-View Co-Training",
    "id": "2506.23705v1",
    "abstract": "Test-time adaptation enables a trained model to adjust to a new domain during inference, making it particularly valuable in clinical settings where such on-the-fly adaptation is required. However, existing techniques depend on large target domain datasets, which are often impractical and unavailable in medical scenarios that demand per-patient, real-time inference. Moreover, current methods commonly focus on two-dimensional images, failing to leverage the volumetric richness of medical imaging data. Bridging this gap, we propose a Patch-Based Multi-View Co-Training method for Single Image Test-Time adaptation. Our method enforces feature and prediction consistency through uncertainty-guided self-training, enabling effective volumetric segmentation in the target domain with only a single test-time image. Validated on three publicly available breast magnetic resonance imaging datasets for tumor segmentation, our method achieves performance close to the upper bound supervised benchmark while also outperforming all existing state-of-the-art methods, on average by a Dice Similarity Coefficient of 3.75%. We publicly share our accessible codebase, readily integrable with the popular nnUNet framework, at https://github.com/smriti-joshi/muvi.git.",
    "authors": "Smriti Joshi, Richard Osuala, Lidia Garrucho, Kaisar Kushibar, Dimitri Kessler, Oliver Diaz, Karim Lekadir",
    "x": -1.455864429473877,
    "y": -0.6363550424575806,
    "z": -0.9613463282585144,
    "cluster_id": 5,
    "layouts": {
      "pacmap": {
        "x": -1.455864429473877,
        "y": -0.6363550424575806,
        "z": -0.9613463282585144
      },
      "starmap": {
        "x": 0.014724483713507652,
        "y": -0.026050377637147903,
        "z": -0.015452253632247448
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "id": "2506.23703v1",
    "authors": "Lars Ullrich, Walter Zimmer, Ross Greer, Knut Graichen, Alois C. Knoll, Mohan Trivedi",
    "abstract": "While artificial intelligence (AI) is advancing rapidly and mastering increasingly complex problems with astonishing performance, the safety assurance of such systems is a major concern. Particularly in the context of safety-critical, real-world cyber-physical systems, AI promises to achieve a new level of autonomy but is hampered by a lack of safety assurance. While data-driven control takes up recent developments in AI to improve control systems, control theory in general could be leveraged to improve AI safety. Therefore, this article outlines a new perspective on AI safety based on an interdisciplinary interpretation of the underlying data-generation process and the respective abstraction by AI systems in a system theory-inspired and system analysis-driven manner. In this context, the new perspective, also referred to as data control, aims to stimulate AI engineering to take advantage of existing safety analysis and assurance in an interdisciplinary way to drive the paradigm of data control. Following a top-down approach, a generic foundation for safety analysis and assurance is outlined at an abstract level that can be refined for specific AI systems and applications and is prepared for future innovation.",
    "title": "A New Perspective On AI Safety Through Control Theory Methodologies",
    "x": 3.34409761428833,
    "y": -0.06721323728561401,
    "z": 0.956982433795929,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 3.34409761428833,
        "y": -0.06721323728561401,
        "z": 0.956982433795929
      },
      "starmap": {
        "x": -0.025581005960702896,
        "y": 0.009512840770184994,
        "z": -0.0004235696978867054
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23701v1",
    "date": "2025-06-30",
    "authors": "Lingtong Zhang, Mengdie Song, Xiaohan Hao, Huayu Mai, Bensheng Qiu",
    "abstract": "Magnetic Resonance Imaging (MRI) reconstruction is essential in medical diagnostics. As the latest generative models, diffusion models (DMs) have struggled to produce high-fidelity images due to their stochastic nature in image domains. Latent diffusion models (LDMs) yield both compact and detailed prior knowledge in latent domains, which could effectively guide the model towards more effective learning of the original data distribution. Inspired by this, we propose Multi-domain Diffusion Prior Guidance (MDPG) provided by pre-trained LDMs to enhance data consistency in MRI reconstruction tasks. Specifically, we first construct a Visual-Mamba-based backbone, which enables efficient encoding and reconstruction of under-sampled images. Then pre-trained LDMs are integrated to provide conditional priors in both latent and image domains. A novel Latent Guided Attention (LGA) is proposed for efficient fusion in multi-level latent domains. Simultaneously, to effectively utilize a prior in both the k-space and image domain, under-sampled images are fused with generated full-sampled images by the Dual-domain Fusion Branch (DFB) for self-adaption guidance. Lastly, to further enhance the data consistency, we propose a k-space regularization strategy based on the non-auto-calibration signal (NACS) set. Extensive experiments on two public MRI datasets fully demonstrate the effectiveness of the proposed methodology. The code is available at https://github.com/Zolento/MDPG.",
    "title": "MDPG: Multi-domain Diffusion Prior Guidance for MRI Reconstruction",
    "x": -2.5428807735443115,
    "y": 0.023196343332529068,
    "z": -0.8241331577301025,
    "cluster_id": 8,
    "layouts": {
      "pacmap": {
        "x": -2.5428807735443115,
        "y": 0.023196343332529068,
        "z": -0.8241331577301025
      },
      "starmap": {
        "x": 0.022070562466979027,
        "y": -0.014190031215548515,
        "z": 0.008414738811552525
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "MedSAM-CA: A CNN-Augmented ViT with Attention-Enhanced Multi-Scale Fusion for Medical Image Segmentation",
    "authors": "Peiting Tian, Xi Chen, Haixia Bi, Fan Li",
    "abstract": "Medical image segmentation plays a crucial role in clinical diagnosis and treatment planning, where accurate boundary delineation is essential for precise lesion localization, organ identification, and quantitative assessment. In recent years, deep learning-based methods have significantly advanced segmentation accuracy. However, two major challenges remain. First, the performance of these methods heavily relies on large-scale annotated datasets, which are often difficult to obtain in medical scenarios due to privacy concerns and high annotation costs. Second, clinically challenging scenarios, such as low contrast in certain imaging modalities and blurry lesion boundaries caused by malignancy, still pose obstacles to precise segmentation. To address these challenges, we propose MedSAM-CA, an architecture-level fine-tuning approach that mitigates reliance on extensive manual annotations by adapting the pretrained foundation model, Medical Segment Anything (MedSAM). MedSAM-CA introduces two key components: the Convolutional Attention-Enhanced Boundary Refinement Network (CBR-Net) and the Attention-Enhanced Feature Fusion Block (Atte-FFB). CBR-Net operates in parallel with the MedSAM encoder to recover boundary information potentially overlooked by long-range attention mechanisms, leveraging hierarchical convolutional processing. Atte-FFB, embedded in the MedSAM decoder, fuses multi-level fine-grained features from skip connections in CBR-Net with global representations upsampled within the decoder to enhance boundary delineation accuracy. Experiments on publicly available datasets covering dermoscopy, CT, and MRI imaging modalities validate the effectiveness of MedSAM-CA. On dermoscopy dataset, MedSAM-CA achieves 94.43% Dice with only 2% of full training data, reaching 97.25% of full-data training performance, demonstrating strong effectiveness in low-resource clinical settings.",
    "date": "2025-06-30",
    "id": "2506.23700v1",
    "x": -2.4580509662628174,
    "y": -0.2845017611980438,
    "z": -1.7705366611480713,
    "cluster_id": 4,
    "layouts": {
      "pacmap": {
        "x": -2.4580509662628174,
        "y": -0.2845017611980438,
        "z": -1.7705366611480713
      },
      "starmap": {
        "x": 0.025555847212672234,
        "y": -0.03828907012939453,
        "z": -0.005361546296626329
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "title": "Agent4S: The Transformation of Research Paradigms from the Perspective of Large Language Models",
    "abstract": "While AI for Science (AI4S) serves as an analytical tool in the current research paradigm, it doesn't solve its core inefficiency. We propose \"Agent for Science\" (Agent4S)-the use of LLM-driven agents to automate the entire research workflow-as the true Fifth Scientific Paradigm. This paper introduces a five-level classification for Agent4S, outlining a clear roadmap from simple task automation to fully autonomous, collaborative \"AI Scientists.\" This framework defines the next revolutionary step in scientific discovery.",
    "authors": "Boyuan Zheng, Zerui Fang, Zhe Xu, Rui Wang, Yiwen Chen, Cunshi Wang, Mengwei Qu, Lei Lei, Zhen Feng, Yan Liu, Yuyang Li, Mingzhou Tan, Jiaji Wu, Jianwei Shuai, Jia Li, Fangfu Ye",
    "id": "2506.23692v1",
    "x": 3.004653215408325,
    "y": -0.3094295263290405,
    "z": 0.14085644483566284,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 3.004653215408325,
        "y": -0.3094295263290405,
        "z": 0.14085644483566284
      },
      "starmap": {
        "x": -0.02259022928774357,
        "y": 0.006337984465062618,
        "z": -0.00262435432523489
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "SynMotion: Semantic-Visual Adaptation for Motion Customized Video Generation",
    "abstract": "Diffusion-based video motion customization facilitates the acquisition of human motion representations from a few video samples, while achieving arbitrary subjects transfer through precise textual conditioning. Existing approaches often rely on semantic-level alignment, expecting the model to learn new motion concepts and combine them with other entities (e.g., ''cats'' or ''dogs'') to produce visually appealing results. However, video data involve complex spatio-temporal patterns, and focusing solely on semantics cause the model to overlook the visual complexity of motion. Conversely, tuning only the visual representation leads to semantic confusion in representing the intended action. To address these limitations, we propose SynMotion, a new motion-customized video generation model that jointly leverages semantic guidance and visual adaptation. At the semantic level, we introduce the dual-embedding semantic comprehension mechanism which disentangles subject and motion representations, allowing the model to learn customized motion features while preserving its generative capabilities for diverse subjects. At the visual level, we integrate parameter-efficient motion adapters into a pre-trained video generation model to enhance motion fidelity and temporal coherence. Furthermore, we introduce a new embedding-specific training strategy which \\textbf{alternately optimizes} subject and motion embeddings, supported by the manually constructed Subject Prior Video (SPV) training dataset. This strategy promotes motion specificity while preserving generalization across diverse subjects. Lastly, we introduce MotionBench, a newly curated benchmark with diverse motion patterns. Experimental results across both T2V and I2V settings demonstrate that \\method outperforms existing baselines. Project page: https://lucaria-academy.github.io/SynMotion/",
    "date": "2025-06-30",
    "id": "2506.23690v1",
    "authors": "Shuai Tan, Biao Gong, Yujie Wei, Shiwei Zhang, Zhuoxin Liu, Dandan Zheng, Jingdong Chen, Yan Wang, Hao Ouyang, Kecheng Zheng, Yujun Shen",
    "x": -2.2996909618377686,
    "y": 1.7957385778427124,
    "z": 1.2748428583145142,
    "cluster_id": 2,
    "layouts": {
      "pacmap": {
        "x": -2.2996909618377686,
        "y": 1.7957385778427124,
        "z": 1.2748428583145142
      },
      "starmap": {
        "x": 0.024923982098698616,
        "y": -0.005555091891437769,
        "z": 0.051514267921447754
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "title": "Pok\u00e9AI: A Goal-Generating, Battle-Optimizing Multi-agent System for Pokemon Red",
    "authors": "Zihao Liu, Xinhang Sui, Yueran Song, Siwen Wang",
    "abstract": "We introduce Pok\\'eAI, the first text-based, multi-agent large language model (LLM) framework designed to autonomously play and progress through Pok\\'emon Red. Our system consists of three specialized agents-Planning, Execution, and Critique-each with its own memory bank, role, and skill set. The Planning Agent functions as the central brain, generating tasks to progress through the game. These tasks are then delegated to the Execution Agent, which carries them out within the game environment. Upon task completion, the Critique Agent evaluates the outcome to determine whether the objective was successfully achieved. Once verification is complete, control returns to the Planning Agent, forming a closed-loop decision-making system.   As a preliminary step, we developed a battle module within the Execution Agent. Our results show that the battle AI achieves an average win rate of 80.8% across 50 wild encounters, only 6% lower than the performance of an experienced human player. Furthermore, we find that a model's battle performance correlates strongly with its LLM Arena score on language-related tasks, indicating a meaningful link between linguistic ability and strategic reasoning. Finally, our analysis of gameplay logs reveals that each LLM exhibits a unique playstyle, suggesting that individual models develop distinct strategic behaviors.",
    "id": "2506.23689v1",
    "x": 2.5768260955810547,
    "y": 0.8347786664962769,
    "z": 1.7736337184906006,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 2.5768260955810547,
        "y": 0.8347786664962769,
        "z": 1.7736337184906006
      },
      "starmap": {
        "x": -0.024939754977822304,
        "y": 0.009707542136311531,
        "z": -0.0007049251580610871
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Modular exponentiation is crucial to number theory and cryptography, yet remains largely unexplored from a mechanistic interpretability standpoint. We train a 4-layer encoder-decoder Transformer model to perform this operation and investigate the emergence of numerical reasoning during training. Utilizing principled sampling strategies, PCA-based embedding analysis, and activation patching, we examine how number-theoretic properties are encoded within the model. We find that reciprocal operand training leads to strong performance gains, with sudden generalization across related moduli. These synchronized accuracy surges reflect grokking-like dynamics, suggesting the model internalizes shared arithmetic structure. We also find a subgraph consisting entirely of attention heads in the final layer sufficient to achieve full performance on the task of regular exponentiation. These results suggest that transformer models learn modular arithmetic through specialized computational circuits, paving the way for more interpretable and efficient neural approaches to modular exponentiation.",
    "id": "2506.23679v1",
    "date": "2025-06-30",
    "title": "Learning Modular Exponentiation with Transformers",
    "authors": "David Demitri Africa, Sara M. Kapoor, Theo Simon Sorg",
    "x": -0.9497992992401123,
    "y": -2.78017520904541,
    "z": 0.18496951460838318,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -0.9497992992401123,
        "y": -2.78017520904541,
        "z": 0.18496951460838318
      },
      "starmap": {
        "x": 0.0020356907043606043,
        "y": -0.010218772105872631,
        "z": -0.009505310095846653
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Rock Yuren Pang, K. J. Kevin Feng, Shangbin Feng, Chu Li, Weijia Shi, Yulia Tsvetkov, Jeffrey Heer, Katharina Reinecke",
    "date": "2025-06-30",
    "abstract": "The output quality of large language models (LLMs) can be improved via \"reasoning\": generating segments of chain-of-thought (CoT) content to further condition the model prior to producing user-facing output. While these chains contain valuable information, they are verbose and lack explicit organization, making them tedious to review. Moreover, they lack opportunities for user feedback, such as to remove unwanted considerations, add desired ones, or clarify unclear assumptions. We introduce Interactive Reasoning, an interaction design that visualizes chain-of-thought outputs as a hierarchy of topics and enables user review and modification. We implement interactive reasoning in Hippo, a prototype for AI-assisted decision making in the face of uncertain trade-offs. In a user study with 16 participants, we find that interactive reasoning in Hippo allows users to quickly identify and interrupt erroneous generations, efficiently steer the model towards customized responses, and better understand both model reasoning and model outputs. Our work contributes to a new paradigm that incorporates user oversight into LLM reasoning processes.",
    "id": "2506.23678v1",
    "title": "Interactive Reasoning: Visualizing and Controlling Chain-of-Thought Reasoning in Large Language Models",
    "x": 2.958911657333374,
    "y": 1.5291739702224731,
    "z": 0.7052394151687622,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 2.958911657333374,
        "y": 1.5291739702224731,
        "z": 0.7052394151687622
      },
      "starmap": {
        "x": -0.0211244635283947,
        "y": 0.01021038368344307,
        "z": -0.0014177741250023246
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "A Unified Framework for Stealthy Adversarial Generation via Latent Optimization and Transferability Enhancement",
    "authors": "Gaozheng Pei, Ke Ma, Dongpeng Zhang, Chengzhi Sun, Qianqian Xu, Qingming Huang",
    "id": "2506.23676v1",
    "date": "2025-06-30",
    "abstract": "Due to their powerful image generation capabilities, diffusion-based adversarial example generation methods through image editing are rapidly gaining popularity. However, due to reliance on the discriminative capability of the diffusion model, these diffusion-based methods often struggle to generalize beyond conventional image classification tasks, such as in Deepfake detection. Moreover, traditional strategies for enhancing adversarial example transferability are challenging to adapt to these methods. To address these challenges, we propose a unified framework that seamlessly incorporates traditional transferability enhancement strategies into diffusion model-based adversarial example generation via image editing, enabling their application across a wider range of downstream tasks. Our method won first place in the \"1st Adversarial Attacks on Deepfake Detectors: A Challenge in the Era of AI-Generated Media\" competition at ACM MM25, which validates the effectiveness of our approach.",
    "x": -3.2891440391540527,
    "y": 0.4949730336666107,
    "z": -0.04864763095974922,
    "cluster_id": 2,
    "layouts": {
      "pacmap": {
        "x": -3.2891440391540527,
        "y": 0.4949730336666107,
        "z": -0.04864763095974922
      },
      "starmap": {
        "x": 0.017133573070168495,
        "y": 0.000574711593799293,
        "z": 0.06500750035047531
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23675v1",
    "date": "2025-06-30",
    "abstract": "Vision Transformer have set new benchmarks in several tasks, but these models come with the lack of high computational costs which makes them impractical for resource limited hardware. Network pruning reduces the computational complexity by removing less important operations while maintaining performance. However, pruning a model on an unseen data domain, leads to a misevaluation of weight significance, resulting in suboptimal resource assignment. In this work, we find that task-sensitive layers initially fail to improve the feature representation on downstream tasks, leading to performance loss for early pruning decisions. To address this problem, we introduce Pruning by Block Benefit (P3B), a pruning method that utilizes the relative contribution on block level to globally assign parameter resources. P3B identifies low-impact components to reduce parameter allocation while preserving critical ones. Classical pruning mask optimization struggles to reactivate zero-mask-elements. In contrast, P3B sets a layerwise keep ratio based on global performance metrics, ensuring the reactivation of late-converging blocks. We show in extensive experiments that P3B is a state of the art pruning method with most noticeable gains in transfer learning tasks. Notably, P3B is able to conserve high performance, even in high sparsity regimes of 70% parameter reduction while only losing 0.64% in accuracy.",
    "authors": "Patrick Glandorf, Bodo Rosenhahn",
    "title": "Pruning by Block Benefit: Exploring the Properties of Vision Transformer Blocks during Domain Adaptation",
    "x": 1.1516319513320923,
    "y": -1.616517186164856,
    "z": -1.890323281288147,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": 1.1516319513320923,
        "y": -1.616517186164856,
        "z": -1.890323281288147
      },
      "starmap": {
        "x": 0.0054306467063724995,
        "y": -0.008560177870094776,
        "z": -0.012825078330934048
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Dongyue Wu, Zilin Guo, Jialong Zuo, Nong Sang, Changxin Gao",
    "id": "2506.23674v1",
    "title": "Partial Forward Blocking: A Novel Data Pruning Paradigm for Lossless Training Acceleration",
    "abstract": "The ever-growing size of training datasets enhances the generalization capability of modern machine learning models but also incurs exorbitant computational costs. Existing data pruning approaches aim to accelerate training by removing those less important samples. However, they often rely on gradients or proxy models, leading to prohibitive additional costs of gradient back-propagation and proxy model training. In this paper, we propose Partial Forward Blocking (PFB), a novel framework for lossless training acceleration. The efficiency of PFB stems from its unique adaptive pruning pipeline: sample importance is assessed based on features extracted from the shallow layers of the target model. Less important samples are then pruned, allowing only the retained ones to proceed with the subsequent forward pass and loss back-propagation. This mechanism significantly reduces the computational overhead of deep-layer forward passes and back-propagation for pruned samples, while also eliminating the need for auxiliary backward computations and proxy model training. Moreover, PFB introduces probability density as an indicator of sample importance. Combined with an adaptive distribution estimation module, our method dynamically prioritizes relatively rare samples, aligning with the constantly evolving training state. Extensive experiments demonstrate the significant superiority of PFB in performance and speed. On ImageNet, PFB achieves a 0.5% accuracy improvement and 33% training time reduction with 40% data pruned.",
    "date": "2025-06-30",
    "x": 0.7074689865112305,
    "y": -1.8247655630111694,
    "z": -1.3196473121643066,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": 0.7074689865112305,
        "y": -1.8247655630111694,
        "z": -1.3196473121643066
      },
      "starmap": {
        "x": 0.007410821970552206,
        "y": -0.010470503009855747,
        "z": -0.01146136224269867
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23673v1",
    "title": "HASD: Hierarchical Adaption for pathology Slide-level Domain-shift",
    "authors": "Jingsong Liu, Han Li, Chen Yang, Michael Deutges, Ario Sadafi, Xin You, Katharina Breininger, Nassir Navab, Peter J. Sch\u00fcffler",
    "abstract": "Domain shift is a critical problem for pathology AI as pathology data is heavily influenced by center-specific conditions. Current pathology domain adaptation methods focus on image patches rather than WSI, thus failing to capture global WSI features required in typical clinical scenarios. In this work, we address the challenges of slide-level domain shift by proposing a Hierarchical Adaptation framework for Slide-level Domain-shift (HASD). HASD achieves multi-scale feature consistency and computationally efficient slide-level domain adaptation through two key components: (1) a hierarchical adaptation framework that integrates a Domain-level Alignment Solver for feature alignment, a Slide-level Geometric Invariance Regularization to preserve the morphological structure, and a Patch-level Attention Consistency Regularization to maintain local critical diagnostic cues; and (2) a prototype selection mechanism that reduces computational overhead. We validate our method on two slide-level tasks across five datasets, achieving a 4.1\\% AUROC improvement in a Breast Cancer HER2 Grading cohort and a 3.9\\% C-index gain in a UCEC survival prediction cohort. Our method provides a practical and reliable slide-level domain adaption solution for pathology institutions, minimizing both computational and annotation costs.",
    "date": "2025-06-30",
    "x": -0.8319764733314514,
    "y": -0.9082277417182922,
    "z": -1.1795096397399902,
    "cluster_id": 4,
    "layouts": {
      "pacmap": {
        "x": -0.8319764733314514,
        "y": -0.9082277417182922,
        "z": -1.1795096397399902
      },
      "starmap": {
        "x": 0.017502322793006897,
        "y": -0.03201909363269806,
        "z": -0.01214008592069149
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Current speech language models exceed the size and latency constraints of many deployment environments. We build compact, expressive speech generation models through layer-aligned distillation, matching hidden states, attention maps, and softened logits to compress large multimodal transformers by 3x with minimal loss in performance. We introduce TinyWave, a family of 2B-parameter models for speech-to-speech and interleaved speech-text generation, trained on 50,000 hours of public audio. TinyWave supports (i) speech-only generation using phonetic or expressive tokens and (ii) mixed speech-text continuations. Evaluation on Libri-Light shows TinyWave within 1.4 normalized perplexity points of its teacher. Accuracy on spoken StoryCloze and SALMon reaches 93-97% of the teacher's performance, outperforming size-matched baselines. These models are optimized for deployment on commodity hardware, enabling applications in real-time conversational agents, assistive technologies, and low-resource environments. We release models, training code, and evaluation scripts to support reproducible research on compact, expressive speech generation.",
    "id": "2506.23670v1",
    "date": "2025-06-30",
    "title": "Efficient Interleaved Speech Modeling through Knowledge Distillation",
    "authors": "Mohammadmahdi Nouriborji, Morteza Rohanian",
    "x": -1.6604669094085693,
    "y": 0.537153422832489,
    "z": 0.8168284893035889,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": -1.6604669094085693,
        "y": 0.537153422832489,
        "z": 0.8168284893035889
      },
      "starmap": {
        "x": -0.01250398624688387,
        "y": 0.004159791860729456,
        "z": 0.01745671220123768
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "L0: Reinforcement Learning to Become General Agents",
    "authors": "Junjie Zhang, Jingyi Xi, Zhuoyang Song, Junyu Lu, Yuhua Ke, Ting Sun, Yukun Yang, Jiaxing Zhang, Songxin Zhang, Zejian Xie",
    "abstract": "Training large language models (LLMs) to act as autonomous agents for multi-turn, long-horizon tasks remains significant challenges in scalability and training efficiency. To address this, we introduce L-Zero (L0), a scalable, end-to-end training pipeline for general-purpose agents. Featuring a low-cost, extensible, and sandboxed concurrent agent worker pool, L0 lowers the barrier for applying reinforcement learning in complex environments. We also introduce NB-Agent, the agent scaffold within L0, which operates in a \"code-as-action\" fashion via a Read-Eval-Print-Loop (REPL). We evaluate L0 on factuality question-answering benchmarks. Our experiments demonstrate that a base model can develop robust problem-solving skills using solely Reinforcement Learning with Verifiable Rewards (RLVR). On the Qwen2.5-7B-Instruct model, our method boosts accuracy on SimpleQA from 30 % to 80 % and on HotpotQA from 22 % to 41 %. We have open-sourced the entire L0 system, including our L0 series models, the NB-Agent, a complete training pipeline, and the corresponding training recipes on (https://github.com/cmriat/l0).",
    "id": "2506.23667v1",
    "date": "2025-06-30",
    "x": 2.792696475982666,
    "y": 1.1286754608154297,
    "z": 1.950461506843567,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 2.792696475982666,
        "y": 1.1286754608154297,
        "z": 1.950461506843567
      },
      "starmap": {
        "x": -0.026076341047883034,
        "y": 0.01084577664732933,
        "z": 0.00031063880305737257
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23664v1",
    "authors": "Fangyijie Wang, Kevin Whelan, F\u00e9lix Balado, Gu\u00e9nol\u00e9 Silvestre, Kathleen M. Curran",
    "date": "2025-06-30",
    "abstract": "Medical image data is less accessible than in other domains due to privacy and regulatory constraints. In addition, labeling requires costly, time-intensive manual image annotation by clinical experts. To overcome these challenges, synthetic medical data generation offers a promising solution. Generative AI (GenAI), employing generative deep learning models, has proven effective at producing realistic synthetic images. This study proposes a novel mask-guided GenAI approach using diffusion models to generate synthetic fetal head ultrasound images paired with segmentation masks. These synthetic pairs augment real datasets for supervised fine-tuning of the Segment Anything Model (SAM). Our results show that the synthetic data captures real image features effectively, and this approach reaches state-of-the-art fetal head segmentation, especially when trained with a limited number of real image-mask pairs. In particular, the segmentation reaches Dice Scores of 94.66\\% and 94.38\\% using a handful of ultrasound images from the Spanish and African cohorts, respectively. Our code, models, and data are available on GitHub.",
    "title": "Diffusion Model-based Data Augmentation Method for Fetal Head Ultrasound Segmentation",
    "x": -1.9076443910598755,
    "y": -0.1939094513654709,
    "z": -0.9013040661811829,
    "cluster_id": 4,
    "layouts": {
      "pacmap": {
        "x": -1.9076443910598755,
        "y": -0.1939094513654709,
        "z": -0.9013040661811829
      },
      "starmap": {
        "x": 0.023077474907040596,
        "y": -0.032613225281238556,
        "z": -0.005332806147634983
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Mario Koddenbrock, Rudolf Hoffmann, David Brodmann, Erik Rodner",
    "title": "On the Domain Robustness of Contrastive Vision-Language Models",
    "date": "2025-06-30",
    "id": "2506.23663v1",
    "abstract": "In real-world vision-language applications, practitioners increasingly rely on large, pretrained foundation models rather than custom-built solutions, despite limited transparency regarding their training data and processes. While these models achieve impressive performance on general benchmarks, their effectiveness can decline notably under specialized domain shifts, such as unique imaging conditions or environmental variations. In this work, we introduce Deepbench, a framework designed to assess domain-specific robustness of vision-language models (VLMs). Deepbench leverages a large language model (LLM) to generate realistic, context-aware image corruptions tailored to specific deployment domains without requiring labeled data. We evaluate a range of contrastive vision-language architectures and architectural variants across six real-world domains and observe substantial variability in robustness, highlighting the need for targeted, domain-aware evaluation. Deepbench is released as open-source software to support further research into domain-aware robustness assessment.",
    "x": 1.181646704673767,
    "y": 0.7423845529556274,
    "z": -0.182707741856575,
    "cluster_id": 3,
    "layouts": {
      "pacmap": {
        "x": 1.181646704673767,
        "y": 0.7423845529556274,
        "z": -0.182707741856575
      },
      "starmap": {
        "x": 0.009662922471761703,
        "y": -0.03017268143594265,
        "z": -0.021897584199905396
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "Zero-Shot Contextual Embeddings via Offline Synthetic Corpus Generation",
    "abstract": "Context-aware embedding methods boost retrieval accuracy by conditioning on corpus statistics (e.g., term co-occurrence and topical patterns) extracted from neighboring documents. However, this context-aware approach requires access to the target corpus or requires domain-specific finetuning, posing practical barriers in privacy-sensitive or resource-constrained settings. We present ZEST, a zero-shot contextual adaptation framework that replaces real corpus access with a one-time offline synthesis of a compact proxy. Given only a handful exemplar documents representative of the general target domain, we use a multi-step hierarchical procedure to generate a synthetic context corpus of several hundred documents that aims to emulate key domain-specific distributions. At inference, the frozen context-aware encoder uses this proxy corpus -- without any finetuning or target corpus access -- to produce domain-adapted embeddings. Across the MTEB benchmark, ZEST's zero-shot synthetic context adaptation using only five example documents performs within 0.5% of models leveraging full target corpus access -- demonstrating remarkable efficacy without any retraining. ZEST thus provides a practical method for deploying high-performance, adaptable embeddings in constrained environments.",
    "date": "2025-06-30",
    "id": "2506.23662v1",
    "authors": "Philip Lippmann, Jie Yang",
    "x": 0.6787627339363098,
    "y": -0.5270344018936157,
    "z": -0.8175020217895508,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": 0.6787627339363098,
        "y": -0.5270344018936157,
        "z": -0.8175020217895508
      },
      "starmap": {
        "x": 0.00814870186150074,
        "y": -0.02330181561410427,
        "z": -0.0152884004637599
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23661v1",
    "title": "Robustness of Misinformation Classification Systems to Adversarial Examples Through BeamAttack",
    "abstract": "We extend BeamAttack, an adversarial attack algorithm designed to evaluate the robustness of text classification systems through word-level modifications guided by beam search. Our extensions include support for word deletions and the option to skip substitutions, enabling the discovery of minimal modifications that alter model predictions. We also integrate LIME to better prioritize word replacements. Evaluated across multiple datasets and victim models (BiLSTM, BERT, and adversarially trained RoBERTa) within the BODEGA framework, our approach achieves over a 99\\% attack success rate while preserving the semantic and lexical similarity of the original texts. Through both quantitative and qualitative analysis, we highlight BeamAttack's effectiveness and its limitations. Our implementation is available at https://github.com/LucK1Y/BeamAttack",
    "date": "2025-06-30",
    "authors": "Arnisa Fazla, Lucas Krauter, David Guzman Piedrahita, Andrianos Michail",
    "x": 2.350271224975586,
    "y": -1.2633349895477295,
    "z": 1.3964608907699585,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 2.350271224975586,
        "y": -1.2633349895477295,
        "z": 1.3964608907699585
      },
      "starmap": {
        "x": -0.021926671266555786,
        "y": 0.006796231959015131,
        "z": 0.0008186441264115274
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23657v2",
    "authors": "Connor Daly, Elettra Marconi, Marco Riva, Jinendra Ekanayake, Daniel S. Elson, Ferdinando Rodriguez y Baena",
    "title": "Towards Markerless Intraoperative Tracking of Deformable Spine Tissue",
    "date": "2025-06-30",
    "abstract": "Consumer-grade RGB-D imaging for intraoperative orthopedic tissue tracking is a promising method with high translational potential. Unlike bone-mounted tracking devices, markerless tracking can reduce operating time and complexity. However, its use has been limited to cadaveric studies. This paper introduces the first real-world clinical RGB-D dataset for spine surgery and develops SpineAlign, a system for capturing deformation between preoperative and intraoperative spine states. We also present an intraoperative segmentation network trained on this data and introduce CorrespondNet, a multi-task framework for predicting key regions for registration in both intraoperative and preoperative scenes.",
    "x": -1.2854171991348267,
    "y": -0.3270752727985382,
    "z": 1.435541033744812,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -1.2854171991348267,
        "y": -0.3270752727985382,
        "z": 1.435541033744812
      },
      "starmap": {
        "x": 0.022580526769161224,
        "y": -0.03915390744805336,
        "z": -0.024835947901010513
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Color Doppler echocardiography is a crucial tool for diagnosing mitral regurgitation (MR). Recent studies have explored intelligent methods for MR diagnosis to minimize user dependence and improve accuracy. However, these approaches often fail to align with clinical workflow and may lead to suboptimal accuracy and interpretability. In this study, we introduce an automated MR diagnosis model (MReg) developed on the 4-chamber cardiac color Doppler echocardiography video (A4C-CDV). It follows comprehensive feature mining strategies to detect MR and assess its severity, considering clinical realities. Our contribution is threefold. First, we formulate the MR diagnosis as a regression task to capture the continuity and ordinal relationships between categories. Second, we design a feature selection and amplification mechanism to imitate the sonographer's diagnostic logic for accurate MR grading. Third, inspired by the Mixture-of-Experts concept, we introduce a feature summary module to extract the category-level features, enhancing the representational capacity for more accurate grading. We trained and evaluated our proposed MReg on a large in-house A4C-CDV dataset comprising 1868 cases with three graded regurgitation labels. Compared to other weakly supervised video anomaly detection and supervised classification methods, MReg demonstrated superior performance in MR diagnosis. Our code is available at: https://github.com/cskdstz/MReg.",
    "id": "2506.23648v1",
    "authors": "Zhe Liu, Yuhao Huang, Lian Liu, Chengrui Zhang, Haotian Lin, Tong Han, Zhiyuan Zhu, Yanlin Chen, Yuerui Chen, Dong Ni, Zhongshan Gou, Xin Yang",
    "date": "2025-06-30",
    "title": "MReg: A Novel Regression Model with MoE-based Video Feature Mining for Mitral Regurgitation Diagnosis",
    "x": -2.761021375656128,
    "y": 2.008596658706665,
    "z": -1.7732771635055542,
    "cluster_id": 4,
    "layouts": {
      "pacmap": {
        "x": -2.761021375656128,
        "y": 2.008596658706665,
        "z": -1.7732771635055542
      },
      "starmap": {
        "x": 0.023266663774847984,
        "y": -0.032485853880643845,
        "z": -0.007598646450787783
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "QLPro: Automated Code Vulnerability Discovery via LLM and Static Code Analysis Integration",
    "abstract": "We introduce QLPro, a vulnerability detection framework that systematically integrates LLMs and static analysis tools to enable comprehensive vulnerability detection across entire open-source projects.We constructed a new dataset, JavaTest, comprising 10 open-source projects from GitHub with 62 confirmed vulnerabilities. CodeQL, a state-of-the-art static analysis tool, detected only 24 of these vulnerabilities while QLPro detected 41. Furthermore, QLPro discovered 6 previously unknown vulnerabilities, 2 of which have been confirmed as 0-days.",
    "date": "2025-06-30",
    "id": "2506.23644v1",
    "authors": "Junze Hu, Xiangyu Jin, Yizhe Zeng, Yuling Liu, Yunpeng Li, Dan Du, Kaiyu Xie, Hongsong Zhu",
    "x": 2.974243402481079,
    "y": 1.3952994346618652,
    "z": -1.1464923620224,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 2.974243402481079,
        "y": 1.3952994346618652,
        "z": -1.1464923620224
      },
      "starmap": {
        "x": -0.02318677119910717,
        "y": 0.007663146127015352,
        "z": -0.0012894527753815055
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "authors": "Peng Huang, Junhu Fu, Bowen Guo, Zeju Li, Yuanyuan Wang, Yi Guo",
    "abstract": "As the appearance of medical images is influenced by multiple underlying factors, generative models require rich attribute information beyond labels to produce realistic and diverse images. For instance, generating an image of skin lesion with specific patterns demands descriptions that go beyond diagnosis, such as shape, size, texture, and color. However, such detailed descriptions are not always accessible. To address this, we explore a framework, termed Visual Attribute Prompts (VAP)-Diffusion, to leverage external knowledge from pre-trained Multi-modal Large Language Models (MLLMs) to improve the quality and diversity of medical image generation. First, to derive descriptions from MLLMs without hallucination, we design a series of prompts following Chain-of-Thoughts for common medical imaging tasks, including dermatologic, colorectal, and chest X-ray images. Generated descriptions are utilized during training and stored across different categories. During testing, descriptions are randomly retrieved from the corresponding category for inference. Moreover, to make the generator robust to unseen combination of descriptions at the test time, we propose a Prototype Condition Mechanism that restricts test embeddings to be similar to those from training. Experiments on three common types of medical imaging across four datasets verify the effectiveness of VAP-Diffusion.",
    "id": "2506.23641v1",
    "title": "VAP-Diffusion: Enriching Descriptions with MLLMs for Enhanced Medical Image Generation",
    "x": -1.590211033821106,
    "y": 0.5386647582054138,
    "z": -1.6335408687591553,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -1.590211033821106,
        "y": 0.5386647582054138,
        "z": -1.6335408687591553
      },
      "starmap": {
        "x": 0.018790241330862045,
        "y": -0.02937394566833973,
        "z": -0.011859290301799774
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Recently, researchers have explored ML-based Traffic Engineering (TE), leveraging neural networks to solve TE problems traditionally addressed by optimization. However, existing ML-based TE schemes remain impractical: they either fail to handle topology changes or suffer from poor scalability due to excessive computational and memory overhead. To overcome these limitations, we propose Geminet, a lightweight and scalable ML-based TE framework that can handle changing topologies. Geminet is built upon two key insights: (i) a methodology that decouples neural networks from topology by learning an iterative gradient-descent-based adjustment process, as the update rule of gradient descent is topology-agnostic, relying only on a few gradient-related quantities; (ii) shifting optimization from path-level routing weights to edge-level dual variables, reducing memory consumption by leveraging the fact that edges are far fewer than paths. Evaluations on WAN and data center datasets show that Geminet significantly improves scalability. Its neural network size is only 0.04% to 7% of existing schemes, while handling topology variations as effectively as HARP, a state-of-the-art ML-based TE approach, without performance degradation. When trained on large-scale topologies, Geminet consumes under 10 GiB of memory, more than eight times less than the 80-plus GiB required by HARP, while achieving 5.45 times faster convergence speed, demonstrating its potential for large-scale deployment.",
    "id": "2506.23640v1",
    "title": "Geminet: Learning the Duality-based Iterative Process for Lightweight Traffic Engineering in Changing Topologies",
    "date": "2025-06-30",
    "authors": "Ximeng Liu, Shizhen Zhao, Xinbing Wang",
    "x": 0.4573012888431549,
    "y": -2.3572170734405518,
    "z": 0.760314404964447,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 0.4573012888431549,
        "y": -2.3572170734405518,
        "z": 0.760314404964447
      },
      "starmap": {
        "x": -0.01608569733798504,
        "y": 0.006661294959485531,
        "z": -0.0027192397974431515
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Wanpeng Zhang, Yicheng Feng, Hao Luo, Yijiang Li, Zihao Yue, Sipeng Zheng, Zongqing Lu",
    "id": "2506.23639v1",
    "date": "2025-06-30",
    "abstract": "Multimodal large language models (MLLMs) have made significant progress in vision-language understanding, yet effectively aligning different modalities remains a fundamental challenge. We present a framework that unifies multimodal understanding by applying byte-pair encoding to visual tokens. Unlike conventional approaches that rely on modality-specific encoders, our method directly incorporates structural information into visual tokens, mirroring successful tokenization strategies in text-only language models. We introduce a priority-guided encoding scheme that considers both frequency and spatial consistency, coupled with a multi-stage training procedure based on curriculum-driven data composition. These enhancements enable the transformer model to better capture cross-modal relationships and reason with visual information. Comprehensive experiments demonstrate improved performance across diverse vision-language tasks. By bridging the gap between visual and textual representations, our approach contributes to the advancement of more capable and efficient multimodal foundation models.",
    "title": "Unified Multimodal Understanding via Byte-Pair Visual Encoding",
    "x": -0.6553090214729309,
    "y": 1.0466673374176025,
    "z": 0.19766397774219513,
    "cluster_id": 3,
    "layouts": {
      "pacmap": {
        "x": -0.6553090214729309,
        "y": 1.0466673374176025,
        "z": 0.19766397774219513
      },
      "starmap": {
        "x": 0.01571958139538765,
        "y": -0.03537997230887413,
        "z": -0.02103271894156933
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "title": "Towards Building Private LLMs: Exploring Multi-Node Expert Parallelism on Apple Silicon for Mixture-of-Experts Large Language Model",
    "authors": "Mu-Chi Chen, Po-Hsuan Huang, Xiangrui Ke, Chia-Heng Tu, Chun Jason Xue, Shih-Hao Hung",
    "id": "2506.23635v1",
    "abstract": "Large Language Models (LLMs) have revolutionized Artificial Intelligence (AI) with significant advancements such as OpenAI's ChatGPT, Meta's Llama, and Databricks' DBRX. This paper addresses the cost and scalability challenges encountered when constructing private LLM systems for personal or small group services, as aimed by Apple Intelligence. A Mac Studio cluster with Apple's M2 Ultra chips is established as a cost-efficient solution to host and accelerate the pretrained DBRX model with the Mixture-of-Experts (MoE) architecture. Our performance analysis reveal that parallel execution of the model's experts across two to four machine nodes significantly reduces inference time. We find that computation time for the experts is comparable to the communication time for exchanging their outputs, emphasizing the importance of network latency over bandwidth. We also observe significant management overhead due to Apple software stack's memory management logic. Based on these findings, we develop optimization schemes to eliminate the memory management overhead. As a result, the Mac Studio cluster is 1.15 times more cost-efficient than the state-of-the-art AI supercomputer with NVIDIA H100 GPUs. In addition, we construct a performance model to estimate system performance under varying configurations, and the model provides valuable insights for designing private LLM systems.",
    "x": 2.521155834197998,
    "y": -0.36230722069740295,
    "z": 1.409204363822937,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 2.521155834197998,
        "y": -0.36230722069740295,
        "z": 1.409204363822937
      },
      "starmap": {
        "x": -0.023176327347755432,
        "y": 0.008132928982377052,
        "z": -0.0015892068622633815
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Youjeong Noh, Joon-Young Paik, Jingun Kwon, Eun-Sun Cho",
    "abstract": "Mixed Boolean-Arithmetic (MBA) obfuscation protects intellectual property by converting programs into forms that are more complex to analyze. However, MBA has been increasingly exploited by malware developers to evade detection and cause significant real-world problems. Traditional MBA deobfuscation methods often consider these expressions as part of a black box and overlook their internal semantic information. To bridge this gap, we propose a truth table, which is an automatically constructed semantic representation of an expression's behavior that does not rely on external resources. The truth table is a mathematical form that represents the output of expression for all possible combinations of input. We also propose a general and extensible guided MBA deobfuscation framework (gMBA) that modifies a Transformer-based neural encoder-decoder Seq2Seq architecture to incorporate this semantic guidance. Experimental results and in-depth analysis show that integrating expression semantics significantly improves performance and highlights the importance of internal semantic expressions in recovering obfuscated code to its original form.",
    "id": "2506.23634v1",
    "title": "gMBA: Expression Semantic Guided Mixed Boolean-Arithmetic Deobfuscation Using Transformer Architectures",
    "date": "2025-06-30",
    "x": -1.8496654033660889,
    "y": -2.131044864654541,
    "z": -0.9129177927970886,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": -1.8496654033660889,
        "y": -2.131044864654541,
        "z": -0.9129177927970886
      },
      "starmap": {
        "x": -0.012844083830714226,
        "y": 0.0021825479343533516,
        "z": -0.0007388152880594134
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "authors": "Lorenzo Olearo, Giorgio Longari, Alessandro Raganato, Rafael Pe\u00f1aloza, Simone Melzi",
    "id": "2506.23630v1",
    "title": "Blending Concepts with Text-to-Image Diffusion Models",
    "abstract": "Diffusion models have dramatically advanced text-to-image generation in recent years, translating abstract concepts into high-fidelity images with remarkable ease. In this work, we examine whether they can also blend distinct concepts, ranging from concrete objects to intangible ideas, into coherent new visual entities under a zero-shot framework. Specifically, concept blending merges the key attributes of multiple concepts (expressed as textual prompts) into a single, novel image that captures the essence of each concept. We investigate four blending methods, each exploiting different aspects of the diffusion pipeline (e.g., prompt scheduling, embedding interpolation, or layer-wise conditioning). Through systematic experimentation across diverse concept categories, such as merging concrete concepts, synthesizing compound words, transferring artistic styles, and blending architectural landmarks, we show that modern diffusion models indeed exhibit creative blending capabilities without further training or fine-tuning. Our extensive user study, involving 100 participants, reveals that no single approach dominates in all scenarios: each blending technique excels under certain conditions, with factors like prompt ordering, conceptual distance, and random seed affecting the outcome. These findings highlight the remarkable compositional potential of diffusion models while exposing their sensitivity to seemingly minor input variations.",
    "x": -1.8092228174209595,
    "y": 1.0518821477890015,
    "z": -1.2755506038665771,
    "cluster_id": 2,
    "layouts": {
      "pacmap": {
        "x": -1.8092228174209595,
        "y": 1.0518821477890015,
        "z": -1.2755506038665771
      },
      "starmap": {
        "x": 0.022163953632116318,
        "y": 8.905024878913537e-05,
        "z": 0.08004748076200485
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "abstract": "The integrity of Water Quality Data (WQD) is critical in environmental monitoring for scientific decision-making and ecological protection. However, water quality monitoring systems are often challenged by large amounts of missing data due to unavoidable problems such as sensor failures and communication delays, which further lead to water quality data becoming High-Dimensional and Sparse (HDS). Traditional data imputation methods are difficult to depict the potential dynamics and fail to capture the deep data features, resulting in unsatisfactory imputation performance. To effectively address the above issues, this paper proposes a Nonlinear Low-rank Representation model (NLR) with Convolutional Neural Networks (CNN) for imputing missing WQD, which utilizes CNNs to implement two ideas: a) fusing temporal features to model the temporal dependence of data between time slots, and b) Extracting nonlinear interactions and local patterns to mine higher-order relationships features and achieve deep fusion of multidimensional information. Experimental studies on three real water quality datasets demonstrate that the proposed model significantly outperforms existing state-of-the-art data imputation models in terms of estimation accuracy. It provides an effective approach for handling water quality monitoring data in complex dynamic environments.",
    "title": "A Nonlinear Low-rank Representation Model with Convolutional Neural Network for Imputing Water Quality Data",
    "authors": "Xin Liao, Bing Yang, Cai Yu",
    "id": "2506.23629v1",
    "x": 0.12619487941265106,
    "y": -1.3008801937103271,
    "z": 0.0844559594988823,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": 0.12619487941265106,
        "y": -1.3008801937103271,
        "z": 0.0844559594988823
      },
      "starmap": {
        "x": 0.005088504869490862,
        "y": -0.012138419784605503,
        "z": -0.012389671057462692
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Traditional Kubernetes networking struggles to meet the escalating demands of AI/ML and evolving Telco infrastructure. This paper introduces Kubernetes Network Drivers (KNDs), a transformative, modular, and declarative architecture designed to overcome current imperative provisioning and API limitations. KNDs integrate network resource management into Kubernetes' core by utilizing Dynamic Resource Allocation (DRA), Node Resource Interface (NRI) improvements, and upcoming OCI Runtime Specification changes. Our DraNet implementation demonstrates declarative attachment of network interfaces, including Remote Direct Memory Access (RDMA) devices, significantly boosting high-performance AI/ML workloads. This capability enables sophisticated cloud-native applications and lays crucial groundwork for future Telco solutions, fostering a \"galaxy\" of specialized KNDs for enhanced application delivery and reduced operational complexity.",
    "authors": "Antonio Ojea",
    "id": "2506.23628v1",
    "title": "The Kubernetes Network Driver Model: A Composable Architecture for High-Performance Networking",
    "date": "2025-06-30",
    "x": 1.8087360858917236,
    "y": -0.27790701389312744,
    "z": 0.6092670559883118,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 1.8087360858917236,
        "y": -0.27790701389312744,
        "z": 0.6092670559883118
      },
      "starmap": {
        "x": -0.022377800196409225,
        "y": 0.0078671770170331,
        "z": -0.0003785215667448938
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23627v1",
    "date": "2025-06-30",
    "title": "Brain Tumor Detection through Thermal Imaging and MobileNET",
    "authors": "Roham Maiti, Debasmita Bhoumik",
    "abstract": "Brain plays a crucial role in regulating body functions and cognitive processes, with brain tumors posing significant risks to human health. Precise and prompt detection is a key factor in proper treatment and better patient outcomes. Traditional methods for detecting brain tumors, that include biopsies, MRI, and CT scans often face challenges due to their high costs and the need for specialized medical expertise. Recent developments in machine learning (ML) and deep learning (DL) has exhibited strong capabilities in automating the identification and categorization of brain tumors from medical images, especially MRI scans. However, these classical ML models have limitations, such as high computational demands, the need for large datasets, and long training times, which hinder their accessibility and efficiency. Our research uses MobileNET model for efficient detection of these tumors. The novelty of this project lies in building an accurate tumor detection model which use less computing re-sources and runs in less time followed by efficient decision making through the use of image processing technique for accurate results. The suggested method attained an average accuracy of 98.5%.",
    "x": -2.1761863231658936,
    "y": -1.0445836782455444,
    "z": -1.423522710800171,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -2.1761863231658936,
        "y": -1.0445836782455444,
        "z": -1.423522710800171
      },
      "starmap": {
        "x": 0.014449873007833958,
        "y": -0.027713600546121597,
        "z": -0.008089574985206127
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "Self-correcting Reward Shaping via Language Models for Reinforcement Learning Agents in Games",
    "abstract": "Reinforcement Learning (RL) in games has gained significant momentum in recent years, enabling the creation of different agent behaviors that can transform a player's gaming experience. However, deploying RL agents in production environments presents two key challenges: (1) designing an effective reward function typically requires an RL expert, and (2) when a game's content or mechanics are modified, previously tuned reward weights may no longer be optimal. Towards the latter challenge, we propose an automated approach for iteratively fine-tuning an RL agent's reward function weights, based on a user-defined language based behavioral goal. A Language Model (LM) proposes updated weights at each iteration based on this target behavior and a summary of performance statistics from prior training rounds. This closed-loop process allows the LM to self-correct and refine its output over time, producing increasingly aligned behavior without the need for manual reward engineering. We evaluate our approach in a racing task and show that it consistently improves agent performance across iterations. The LM-guided agents show a significant increase in performance from $9\\%$ to $74\\%$ success rate in just one iteration. We compare our LM-guided tuning against a human expert's manual weight design in the racing task: by the final iteration, the LM-tuned agent achieved an $80\\%$ success rate, and completed laps in an average of $855$ time steps, a competitive performance against the expert-tuned agent's peak $94\\%$ success, and $850$ time steps.",
    "authors": "Ant\u00f3nio Afonso, Iolanda Leite, Alessandro Sestini, Florian Fuchs, Konrad Tollmar, Linus Gissl\u00e9n",
    "date": "2025-06-30",
    "id": "2506.23626v1",
    "x": 2.4889256954193115,
    "y": 0.6358652710914612,
    "z": 1.762235403060913,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 2.4889256954193115,
        "y": 0.6358652710914612,
        "z": 1.762235403060913
      },
      "starmap": {
        "x": -0.023991871625185013,
        "y": 0.00959455780684948,
        "z": 0.0003079099114984274
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Audio-Visual Segmentation (AVS) aims to segment sound-producing objects in video frames based on the associated audio signal. Prevailing AVS methods typically adopt an audio-centric Transformer architecture, where object queries are derived from audio features. However, audio-centric Transformers suffer from two limitations: perception ambiguity caused by the mixed nature of audio, and weakened dense prediction ability due to visual detail loss. To address these limitations, we propose a new Vision-Centric Transformer (VCT) framework that leverages vision-derived queries to iteratively fetch corresponding audio and visual information, enabling queries to better distinguish between different sounding objects from mixed audio and accurately delineate their contours. Additionally, we also introduce a Prototype Prompted Query Generation (PPQG) module within our VCT framework to generate vision-derived queries that are both semantically aware and visually rich through audio prototype prompting and pixel context grouping, facilitating audio-visual information aggregation. Extensive experiments demonstrate that our VCT framework achieves new state-of-the-art performances on three subsets of the AVSBench dataset. The code is available at https://github.com/spyflying/VCT_AVS.",
    "id": "2506.23623v1",
    "authors": "Shaofei Huang, Rui Ling, Tianrui Hui, Hongyu Li, Xu Zhou, Shifeng Zhang, Si Liu, Richang Hong, Meng Wang",
    "title": "Revisiting Audio-Visual Segmentation with Vision-Centric Transformer",
    "date": "2025-06-30",
    "x": -0.9412689208984375,
    "y": 1.4208037853240967,
    "z": 0.6409888863563538,
    "cluster_id": 3,
    "layouts": {
      "pacmap": {
        "x": -0.9412689208984375,
        "y": 1.4208037853240967,
        "z": 0.6409888863563538
      },
      "starmap": {
        "x": 0.01777142472565174,
        "y": -0.036734700202941895,
        "z": -0.016615284606814384
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "This paper investigates the impact of posterior drift on out-of-sample forecasting accuracy in overparametrized machine learning models. We document the loss in performance when the loadings of the data generating process change between the training and testing samples. This matters crucially in settings in which regime changes are likely to occur, for instance, in financial markets. Applied to equity premium forecasting, our results underline the sensitivity of a market timing strategy to sub-periods and to the bandwidth parameters that control the complexity of the model. For the average investor, we find that focusing on holding periods of 15 years can generate very heterogeneous returns, especially for small bandwidths. Large bandwidths yield much more consistent outcomes, but are far less appealing from a risk-adjusted return standpoint. All in all, our findings tend to recommend cautiousness when resorting to large linear models for stock market predictions.",
    "id": "2506.23619v1",
    "title": "Overparametrized models with posterior drift",
    "date": "2025-06-30",
    "authors": "Guillaume Coqueret, Martial Laguerre",
    "x": 0.994377613067627,
    "y": -1.2442580461502075,
    "z": -1.419338583946228,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": 0.994377613067627,
        "y": -1.2442580461502075,
        "z": -1.419338583946228
      },
      "starmap": {
        "x": 0.002782827243208885,
        "y": -0.015928007662296295,
        "z": -0.01329089980572462
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "TurboVSR: Fantastic Video Upscalers and Where to Find Them",
    "authors": "Zhongdao Wang, Guodongfang Zhao, Jingjing Ren, Bailan Feng, Shifeng Zhang, Wenbo Li",
    "date": "2025-06-30",
    "id": "2506.23618v1",
    "abstract": "Diffusion-based generative models have demonstrated exceptional promise in the video super-resolution (VSR) task, achieving a substantial advancement in detail generation relative to prior methods. However, these approaches face significant computational efficiency challenges. For instance, current techniques may require tens of minutes to super-resolve a mere 2-second, 1080p video. In this paper, we present TurboVSR, an ultra-efficient diffusion-based video super-resolution model. Our core design comprises three key aspects: (1) We employ an autoencoder with a high compression ratio of 32$\\times$32$\\times$8 to reduce the number of tokens. (2) Highly compressed latents pose substantial challenges for training. We introduce factorized conditioning to mitigate the learning complexity: we first learn to super-resolve the initial frame; subsequently, we condition the super-resolution of the remaining frames on the high-resolution initial frame and the low-resolution subsequent frames. (3) We convert the pre-trained diffusion model to a shortcut model to enable fewer sampling steps, further accelerating inference. As a result, TurboVSR performs on par with state-of-the-art VSR methods, while being 100+ times faster, taking only 7 seconds to process a 2-second long 1080p video. TurboVSR also supports image resolution by considering image as a one-frame video. Our efficient design makes SR beyond 1080p possible, results on 4K (3648$\\times$2048) image SR show surprising fine details.",
    "x": -2.6467039585113525,
    "y": 1.9856451749801636,
    "z": 1.3534815311431885,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -2.6467039585113525,
        "y": 1.9856451749801636,
        "z": 1.3534815311431885
      },
      "starmap": {
        "x": 0.02329983375966549,
        "y": -0.010814831592142582,
        "z": 0.031521037220954895
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "3D Gaussian Splatting (3DGS) is a powerful alternative to Neural Radiance Fields (NeRF), excelling in complex scene reconstruction and efficient rendering. However, it relies on high-quality point clouds from Structure-from-Motion (SfM), limiting its applicability. SfM also fails in texture-deficient or constrained-view scenarios, causing severe degradation in 3DGS reconstruction. To address this limitation, we propose AttentionGS, a novel framework that eliminates the dependency on high-quality initial point clouds by leveraging structural attention for direct 3D reconstruction from randomly initialization. In the early training stage, we introduce geometric attention to rapidly recover the global scene structure. As training progresses, we incorporate texture attention to refine fine-grained details and enhance rendering quality. Furthermore, we employ opacity-weighted gradients to guide Gaussian densification, leading to improved surface reconstruction. Extensive experiments on multiple benchmark datasets demonstrate that AttentionGS significantly outperforms state-of-the-art methods, particularly in scenarios where point cloud initialization is unreliable. Our approach paves the way for more robust and flexible 3D Gaussian Splatting in real-world applications.",
    "title": "AttentionGS: Towards Initialization-Free 3D Gaussian Splatting via Structural Attention",
    "authors": "Ziao Liu, Zhenjia Li, Yifeng Shi, Xiangang Li",
    "id": "2506.23611v1",
    "date": "2025-06-30",
    "x": -2.2619500160217285,
    "y": 2.44976544380188,
    "z": -0.46042153239250183,
    "cluster_id": 1,
    "layouts": {
      "pacmap": {
        "x": -2.2619500160217285,
        "y": 2.44976544380188,
        "z": -0.46042153239250183
      },
      "starmap": {
        "x": 0.05146212503314018,
        "y": 0.07367507368326187,
        "z": -0.028964431956410408
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "abstract": "Large language models (LLMs) make it possible to generate synthetic behavioural data at scale, offering an ethical and low-cost alternative to human experiments. Whether such data can faithfully capture psychological differences driven by personality traits, however, remains an open question. We evaluate the capacity of LLM agents, conditioned on Big-Five profiles, to reproduce personality-based variation in susceptibility to misinformation, focusing on news discernment, the ability to judge true headlines as true and false headlines as false. Leveraging published datasets in which human participants with known personality profiles rated headline accuracy, we create matching LLM agents and compare their responses to the original human patterns. Certain trait-misinformation associations, notably those involving Agreeableness and Conscientiousness, are reliably replicated, whereas others diverge, revealing systematic biases in how LLMs internalize and express personality. The results underscore both the promise and the limits of personality-aligned LLMs for behavioral simulation, and offer new insight into modeling cognitive diversity in artificial agents.",
    "authors": "Manuel Pratelli, Marinella Petrocchi",
    "title": "Evaluating the Simulation of Human Personality-Driven Susceptibility to Misinformation with LLMs",
    "id": "2506.23610v1",
    "x": 3.111090660095215,
    "y": 1.4504443407058716,
    "z": 0.4127686619758606,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 3.111090660095215,
        "y": 1.4504443407058716,
        "z": 0.4127686619758606
      },
      "starmap": {
        "x": -0.02464025840163231,
        "y": 0.008684830740094185,
        "z": -0.0006105541251599789
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23607v1",
    "date": "2025-06-30",
    "authors": "Shiqi Zhang, Sha Zhang, Jiajun Deng, Yedong Shen, Mingxiao MA, Yanyong Zhang",
    "title": "PGOV3D: Open-Vocabulary 3D Semantic Segmentation with Partial-to-Global Curriculum",
    "abstract": "Existing open-vocabulary 3D semantic segmentation methods typically supervise 3D segmentation models by merging text-aligned features (e.g., CLIP) extracted from multi-view images onto 3D points. However, such approaches treat multi-view images merely as intermediaries for transferring open-vocabulary information, overlooking their rich semantic content and cross-view correspondences, which limits model effectiveness. To address this, we propose PGOV3D, a novel framework that introduces a Partial-to-Global curriculum for improving open-vocabulary 3D semantic segmentation. The key innovation lies in a two-stage training strategy. In the first stage, we pre-train the model on partial scenes that provide dense semantic information but relatively simple geometry. These partial point clouds are derived from multi-view RGB-D inputs via pixel-wise depth projection. To enable open-vocabulary learning, we leverage a multi-modal large language model (MLLM) and a 2D segmentation foundation model to generate open-vocabulary labels for each viewpoint, offering rich and aligned supervision. An auxiliary inter-frame consistency module is introduced to enforce feature consistency across varying viewpoints and enhance spatial understanding. In the second stage, we fine-tune the model on complete scene-level point clouds, which are sparser and structurally more complex. We aggregate the partial vocabularies associated with each scene and generate pseudo labels using the pre-trained model, effectively bridging the semantic gap between dense partial observations and large-scale 3D environments. Extensive experiments on ScanNet, ScanNet200, and S3DIS benchmarks demonstrate that PGOV3D achieves competitive performance in open-vocabulary 3D semantic segmentation.",
    "x": -1.8696744441986084,
    "y": 1.6987773180007935,
    "z": -0.7991464734077454,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -1.8696744441986084,
        "y": 1.6987773180007935,
        "z": -0.7991464734077454
      },
      "starmap": {
        "x": 0.027727004140615463,
        "y": 0.011072436347603798,
        "z": -0.010796048678457737
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "SG-LDM: Semantic-Guided LiDAR Generation via Latent-Aligned Diffusion",
    "abstract": "Lidar point cloud synthesis based on generative models offers a promising solution to augment deep learning pipelines, particularly when real-world data is scarce or lacks diversity. By enabling flexible object manipulation, this synthesis approach can significantly enrich training datasets and enhance discriminative models. However, existing methods focus on unconditional lidar point cloud generation, overlooking their potential for real-world applications. In this paper, we propose SG-LDM, a Semantic-Guided Lidar Diffusion Model that employs latent alignment to enable robust semantic-to-lidar synthesis. By directly operating in the native lidar space and leveraging explicit semantic conditioning, SG-LDM achieves state-of-the-art performance in generating high-fidelity lidar point clouds guided by semantic labels. Moreover, we propose the first diffusion-based lidar translation framework based on SG-LDM, which enables cross-domain translation as a domain adaptation strategy to enhance downstream perception performance. Systematic experiments demonstrate that SG-LDM significantly outperforms existing lidar diffusion models and the proposed lidar translation framework further improves data augmentation performance in the downstream lidar segmentation task.",
    "id": "2506.23606v1",
    "date": "2025-06-30",
    "authors": "Zhengkang Xiang, Zizhao Li, Amir Khodabandeh, Kourosh Khoshelham",
    "x": -1.5525728464126587,
    "y": 0.7117869257926941,
    "z": -2.1214938163757324,
    "cluster_id": 4,
    "layouts": {
      "pacmap": {
        "x": -1.5525728464126587,
        "y": 0.7117869257926941,
        "z": -2.1214938163757324
      },
      "starmap": {
        "x": 0.020140785723924637,
        "y": -0.01852559484541416,
        "z": 0.001733913435600698
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "authors": "Suyash Maniyar, Vishvesh Trivedi, Ajoy Mondal, Anand Mishra, C. V. Jawahar",
    "id": "2506.23605v1",
    "title": "AI-Generated Lecture Slides for Improving Slide Element Detection and Retrieval",
    "abstract": "Lecture slide element detection and retrieval are key problems in slide understanding. Training effective models for these tasks often depends on extensive manual annotation. However, annotating large volumes of lecture slides for supervised training is labor intensive and requires domain expertise. To address this, we propose a large language model (LLM)-guided synthetic lecture slide generation pipeline, SynLecSlideGen, which produces high-quality, coherent and realistic slides. We also create an evaluation benchmark, namely RealSlide by manually annotating 1,050 real lecture slides. To assess the utility of our synthetic slides, we perform few-shot transfer learning on real data using models pre-trained on them. Experimental results show that few-shot transfer learning with pretraining on synthetic slides significantly improves performance compared to training only on real data. This demonstrates that synthetic data can effectively compensate for limited labeled lecture slides. The code and resources of our work are publicly available on our project website: https://synslidegen.github.io/.",
    "x": -0.6098058223724365,
    "y": -0.5084595084190369,
    "z": -0.6533883810043335,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -0.6098058223724365,
        "y": -0.5084595084190369,
        "z": -0.6533883810043335
      },
      "starmap": {
        "x": 0.018957622349262238,
        "y": -0.03215031698346138,
        "z": -0.014266406185925007
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23603v1",
    "abstract": "As Large Language Models (LLMs) are increasingly deployed in sensitive domains, traditional data privacy measures prove inadequate for protecting information that is implicit, contextual, or inferable - what we define as semantic privacy. This Systematization of Knowledge (SoK) introduces a lifecycle-centric framework to analyze how semantic privacy risks emerge across input processing, pretraining, fine-tuning, and alignment stages of LLMs. We categorize key attack vectors and assess how current defenses, such as differential privacy, embedding encryption, edge computing, and unlearning, address these threats. Our analysis reveals critical gaps in semantic-level protection, especially against contextual inference and latent representation leakage. We conclude by outlining open challenges, including quantifying semantic leakage, protecting multimodal inputs, balancing de-identification with generation quality, and ensuring transparency in privacy enforcement. This work aims to inform future research on designing robust, semantically aware privacy-preserving techniques for LLMs.",
    "authors": "Baihe Ma, Yanna Jiang, Xu Wang, Guangshen Yu, Qin Wang, Caijun Sun, Chen Li, Xuelei Qi, Ying He, Wei Ni, Ren Ping Liu",
    "date": "2025-06-30",
    "title": "SoK: Semantic Privacy in Large Language Models",
    "x": 0.5177108645439148,
    "y": 0.5190792083740234,
    "z": -0.4090133309364319,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 0.5177108645439148,
        "y": 0.5190792083740234,
        "z": -0.4090133309364319
      },
      "starmap": {
        "x": -0.01750350371003151,
        "y": 0.0030199780594557524,
        "z": -0.0009046335471794009
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "Semantic-guided Diverse Decoding for Large Language Model",
    "authors": "Weijie Shi, Yue Cui, Yaguang Wu, Jingzhi Fang, Shibo Zhang, Mengze Li, Sirui Han, Jia Zhu, Jiajie Xu, Xiaofang Zhou",
    "date": "2025-06-30",
    "id": "2506.23601v1",
    "abstract": "Diverse decoding of large language models is crucial for applications requiring multiple semantically distinct responses, yet existing methods primarily achieve lexical rather than semantic diversity. This limitation significantly constrains Best-of-N strategies, group-based reinforcement learning, and data synthesis. While temperature sampling and diverse beam search modify token distributions or apply n-gram penalties, they fail to ensure meaningful semantic differentiation. We introduce Semantic-guided Diverse Decoding (SemDiD), operating directly in embedding space that balances quality with diversity through three complementary mechanisms: orthogonal directional guidance, dynamic inter-group repulsion, and position-debiased probability assessment. SemDiD harmonizes these competing objectives using adaptive gain functions and constraint optimization, ensuring both quality thresholds and maximal semantic differentiation. Experiments show SemDiD consistently outperforms existing methods, improving Best-of-N coverage by 1.4-5.2% across diverse tasks and accelerating RLHF training convergence by 15% while increasing accuracy by up to 2.1%.",
    "x": -0.12989521026611328,
    "y": -0.10675220936536789,
    "z": -0.6373639702796936,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": -0.12989521026611328,
        "y": -0.10675220936536789,
        "z": -0.6373639702796936
      },
      "starmap": {
        "x": -0.011393259279429913,
        "y": -0.00014287418161984533,
        "z": -0.0008666038047522306
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "When Will It Fail?: Anomaly to Prompt for Forecasting Future Anomalies in Time Series",
    "id": "2506.23596v1",
    "authors": "Min-Yeong Park, Won-Jeong Lee, Seong Tae Kim, Gyeong-Moon Park",
    "date": "2025-06-30",
    "abstract": "Recently, forecasting future abnormal events has emerged as an important scenario to tackle real-world necessities. However, the solution of predicting specific future time points when anomalies will occur, known as Anomaly Prediction (AP), remains under-explored. Existing methods dealing with time series data fail in AP, focusing only on immediate anomalies or failing to provide precise predictions for future anomalies. To address the AP task, we propose a novel framework called Anomaly to Prompt (A2P), comprised of Anomaly-Aware Forecasting (AAF) and Synthetic Anomaly Prompting (SAP). To enable the forecasting model to forecast abnormal time points, we adopt a strategy to learn the relationships of anomalies. For the robust detection of anomalies, our proposed SAP introduces a learnable Anomaly Prompt Pool (APP) that simulates diverse anomaly patterns using signal adaptive prompt. Comprehensive experiments on multiple real-world datasets demonstrate the superiority of A2P over state-of-the-art methods, showcasing its ability to predict future anomalies. Our implementation code is available at https://github.com/KU-VGI/AP.",
    "x": -0.5325188040733337,
    "y": -0.3186270594596863,
    "z": -1.6714794635772705,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -0.5325188040733337,
        "y": -0.3186270594596863,
        "z": -1.6714794635772705
      },
      "starmap": {
        "x": 0.014788620173931122,
        "y": -0.01949509046971798,
        "z": -0.02485457807779312
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "title": "CAI: Caption-Sensitive Attention Intervention for Mitigating Object Hallucination in Large Vision-Language Models",
    "abstract": "Although Large Vision-Language Models (LVLMs) have demonstrated powerful capabilities in interpreting visual information, they frequently produce content that deviates from visual information, leading to object hallucination. To tackle this, recent works mostly depend on expensive manual annotations and training cost, or significantly increase inference time. In this work, we observe that LVLMs' attention to visual information is significantly stronger when answering caption queries compared to non-caption queries. Inspired by this phenomenon, we propose Caption-sensitive Attention Intervention (CAI), a training-free, plug-and-play hallucination mitigation method that leverages the attention activation pattern in response to caption queries to enhance LVLMs' visual perception capability. Extensive experimental results across four benchmarks covering both discriminative and generative tasks, demonstrate that CAI achieves state-of-the-art (SOTA) hallucination mitigating performance only with minimal additional inference cost.",
    "id": "2506.23590v1",
    "authors": "Qiming Li, Zekai Ye, Xiaocheng Feng, Weihong Zhong, Libo Qin, Ruihan Chen, Baohang Li, Kui Jiang, Yaowei Wang, Ting Liu, Bing Qin",
    "x": -1.1144359111785889,
    "y": 1.996772050857544,
    "z": -0.5116735100746155,
    "cluster_id": 3,
    "layouts": {
      "pacmap": {
        "x": -1.1144359111785889,
        "y": 1.996772050857544,
        "z": -0.5116735100746155
      },
      "starmap": {
        "x": 0.018491361290216446,
        "y": -0.042462024837732315,
        "z": -0.0259830541908741
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "authors": "Neta Shaul, Uriel Singer, Itai Gat, Yaron Lipman",
    "title": "Transition Matching: Scalable and Flexible Generative Modeling",
    "id": "2506.23589v1",
    "abstract": "Diffusion and flow matching models have significantly advanced media generation, yet their design space is well-explored, somewhat limiting further improvements. Concurrently, autoregressive (AR) models, particularly those generating continuous tokens, have emerged as a promising direction for unifying text and media generation. This paper introduces Transition Matching (TM), a novel discrete-time, continuous-state generative paradigm that unifies and advances both diffusion/flow models and continuous AR generation. TM decomposes complex generation tasks into simpler Markov transitions, allowing for expressive non-deterministic probability transition kernels and arbitrary non-continuous supervision processes, thereby unlocking new flexible design avenues. We explore these choices through three TM variants: (i) Difference Transition Matching (DTM), which generalizes flow matching to discrete-time by directly learning transition probabilities, yielding state-of-the-art image quality and text adherence as well as improved sampling efficiency. (ii) Autoregressive Transition Matching (ARTM) and (iii) Full History Transition Matching (FHTM) are partially and fully causal models, respectively, that generalize continuous AR methods. They achieve continuous causal AR generation quality comparable to non-causal approaches and potentially enable seamless integration with existing AR text generation techniques. Notably, FHTM is the first fully causal model to match or surpass the performance of flow-based methods on text-to-image task in continuous domains. We demonstrate these contributions through a rigorous large-scale comparison of TM variants and relevant baselines, maintaining a fixed architecture, training data, and hyperparameters.",
    "x": -2.4155216217041016,
    "y": 0.4162498116493225,
    "z": 0.7890337705612183,
    "cluster_id": 2,
    "layouts": {
      "pacmap": {
        "x": -2.4155216217041016,
        "y": 0.4162498116493225,
        "z": 0.7890337705612183
      },
      "starmap": {
        "x": 0.018933381885290146,
        "y": -0.0024028606712818146,
        "z": 0.06905779242515564
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Renjie Liang, Zhengkang Fan, Jinqian Pan, Chenkun Sun, Russell Terry, Jie Xu",
    "id": "2506.23584v1",
    "date": "2025-06-30",
    "title": "A Clinically-Grounded Two-Stage Framework for Renal CT Report Generation",
    "abstract": "Generating radiology reports from CT scans remains a complex task due to the nuanced nature of medical imaging and the variability in clinical documentation. In this study, we propose a two-stage framework for generating renal radiology reports from 2D CT slices. First, we extract structured abnormality features using a multi-task learning model trained to identify lesion attributes such as location, size, enhancement, and attenuation. These extracted features are subsequently combined with the corresponding CT image and fed into a fine-tuned vision-language model to generate natural language report sentences aligned with clinical findings. We conduct experiments on a curated dataset of renal CT studies with manually annotated sentence-slice-feature triplets and evaluate performance using both classification metrics and natural language generation metrics. Our results demonstrate that the proposed model outperforms random baselines across all abnormality types, and the generated reports capture key clinical content with reasonable textual accuracy. This exploratory work highlights the feasibility of modular, feature-informed report generation for renal imaging. Future efforts will focus on extending this pipeline to 3D CT volumes and further improving clinical fidelity in multimodal medical AI systems.",
    "x": -2.0826966762542725,
    "y": 0.018488116562366486,
    "z": -1.9768028259277344,
    "cluster_id": 4,
    "layouts": {
      "pacmap": {
        "x": -2.0826966762542725,
        "y": 0.018488116562366486,
        "z": -1.9768028259277344
      },
      "starmap": {
        "x": 0.022784702479839325,
        "y": -0.038179945200681686,
        "z": -0.00802755169570446
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "id": "2506.23583v1",
    "abstract": "Federated learning with secure aggregation enables private and collaborative learning from decentralised data without leaking sensitive client information. However, secure aggregation also complicates the detection of malicious client behaviour and the evaluation of individual client contributions to the learning. To address these challenges, QI (Pejo et al.) and FedGT (Xhemrishi et al.) were proposed for contribution evaluation (CE) and misbehaviour detection (MD), respectively. QI, however, lacks adequate MD accuracy due to its reliance on the random selection of clients in each training round, while FedGT lacks the CE ability. In this work, we combine the strengths of QI and FedGT to achieve both robust MD and accurate CE. Our experiments demonstrate superior performance compared to using either method independently.",
    "title": "Detect \\& Score: Privacy-Preserving Misbehaviour Detection and Contribution Evaluation in Federated Learning",
    "authors": "Marvin Xhemrishi, Alexandre Graell i Amat, Bal\u00e1zs Pej\u00f3",
    "x": 3.1413509845733643,
    "y": 0.5779744386672974,
    "z": 2.0055792331695557,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": 3.1413509845733643,
        "y": 0.5779744386672974,
        "z": 2.0055792331695557
      },
      "starmap": {
        "x": -0.013920005410909653,
        "y": 0.0025221568066626787,
        "z": -0.0003407051262911409
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23581v1",
    "title": "PBCAT: Patch-based composite adversarial training against physically realizable attacks on object detection",
    "date": "2025-06-30",
    "abstract": "Object detection plays a crucial role in many security-sensitive applications. However, several recent studies have shown that object detectors can be easily fooled by physically realizable attacks, \\eg, adversarial patches and recent adversarial textures, which pose realistic and urgent threats. Adversarial Training (AT) has been recognized as the most effective defense against adversarial attacks. While AT has been extensively studied in the $l_\\infty$ attack settings on classification models, AT against physically realizable attacks on object detectors has received limited exploration. Early attempts are only performed to defend against adversarial patches, leaving AT against a wider range of physically realizable attacks under-explored. In this work, we consider defending against various physically realizable attacks with a unified AT method. We propose PBCAT, a novel Patch-Based Composite Adversarial Training strategy. PBCAT optimizes the model by incorporating the combination of small-area gradient-guided adversarial patches and imperceptible global adversarial perturbations covering the entire image. With these designs, PBCAT has the potential to defend against not only adversarial patches but also unseen physically realizable attacks such as adversarial textures. Extensive experiments in multiple settings demonstrated that PBCAT significantly improved robustness against various physically realizable attacks over state-of-the-art defense methods. Notably, it improved the detection accuracy by 29.7\\% over previous defense methods under one recent adversarial texture attack.",
    "authors": "Xiao Li, Yiming Zhu, Yifan Huang, Wei Zhang, Yingzhe He, Jie Shi, Xiaolin Hu",
    "x": 2.1126599311828613,
    "y": -1.5348904132843018,
    "z": 1.9117469787597656,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": 2.1126599311828613,
        "y": -1.5348904132843018,
        "z": 1.9117469787597656
      },
      "starmap": {
        "x": -0.005182952154427767,
        "y": 0.0033231922425329685,
        "z": 0.02818824164569378
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Dataset distillation (DD) condenses large datasets into compact yet informative substitutes, preserving performance comparable to the original dataset while reducing storage, transmission costs, and computational consumption. However, previous DD methods mainly focus on distilling information from images, often overlooking the semantic information inherent in the data. The disregard for context hinders the model's generalization ability, particularly in tasks involving complex datasets, which may result in illogical outputs or the omission of critical objects. In this study, we integrate vision-language methods into DD by introducing text prototypes to distill language information and collaboratively synthesize data with image prototypes, thereby enhancing dataset distillation performance. Notably, the text prototypes utilized in this study are derived from descriptive text information generated by an open-source large language model. This framework demonstrates broad applicability across datasets without pre-existing text descriptions, expanding the potential of dataset distillation beyond traditional image-based approaches. Compared to other methods, the proposed approach generates logically coherent images containing target objects, achieving state-of-the-art validation performance and demonstrating robust generalization. Source code and generated data are available in https://github.com/zou-yawen/Dataset-Distillation-via-Vision-Language-Category-Prototype/",
    "title": "Dataset Distillation via Vision-Language Category Prototype",
    "authors": "Yawen Zou, Guang Li, Duo Su, Zi Wang, Jun Yu, Chao Zhang",
    "date": "2025-06-30",
    "id": "2506.23580v1",
    "x": -0.33180275559425354,
    "y": 0.919648289680481,
    "z": -0.8743148446083069,
    "cluster_id": 3,
    "layouts": {
      "pacmap": {
        "x": -0.33180275559425354,
        "y": 0.919648289680481,
        "z": -0.8743148446083069
      },
      "starmap": {
        "x": 0.014065198600292206,
        "y": -0.03268793970346451,
        "z": -0.021131662651896477
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "Reachability in symmetric VASS",
    "date": "2025-06-30",
    "abstract": "We investigate the reachability problem in symmetric vector addition systems with states (VASS), where transitions are invariant under a group of permutations of coordinates. One extremal case, the trivial groups, yields general VASS. In another extremal case, the symmetric groups, we show that the reachability problem can be solved in PSPACE, regardless of the dimension of input VASS (to be contrasted with Ackermannian complexity in general VASS). We also consider other groups, in particular alternating and cyclic ones. Furthermore, motivated by the open status of the reachability problem in data VASS, we estimate the gain in complexity when the group arises as a combination of the trivial and symmetric groups.",
    "authors": "\u0141ukasz Kami\u0144ski, S\u0142awomir Lasota",
    "id": "2506.23578v1",
    "x": -0.13795994222164154,
    "y": -1.5264240503311157,
    "z": 2.6445236206054688,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -0.13795994222164154,
        "y": -1.5264240503311157,
        "z": 2.6445236206054688
      },
      "starmap": {
        "x": -0.0009313864284195006,
        "y": -0.008090909570455551,
        "z": -0.007156033534556627
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "id": "2506.23577v1",
    "title": "StackCLIP: Clustering-Driven Stacked Prompt in Zero-Shot Industrial Anomaly Detection",
    "abstract": "Enhancing the alignment between text and image features in the CLIP model is a critical challenge in zero-shot industrial anomaly detection tasks. Recent studies predominantly utilize specific category prompts during pretraining, which can cause overfitting to the training categories and limit model generalization. To address this, we propose a method that transforms category names through multicategory name stacking to create stacked prompts, forming the basis of our StackCLIP model. Our approach introduces two key components. The Clustering-Driven Stacked Prompts (CSP) module constructs generic prompts by stacking semantically analogous categories, while utilizing multi-object textual feature fusion to amplify discriminative anomalies among similar objects. The Ensemble Feature Alignment (EFA) module trains knowledge-specific linear layers tailored for each stack cluster and adaptively integrates them based on the attributes of test categories. These modules work together to deliver superior training speed, stability, and convergence, significantly boosting anomaly segmentation performance. Additionally, our stacked prompt framework offers robust generalization across classification tasks. To further improve performance, we introduce the Regulating Prompt Learning (RPL) module, which leverages the generalization power of stacked prompts to refine prompt learning, elevating results in anomaly detection classification tasks. Extensive testing on seven industrial anomaly detection datasets demonstrates that our method achieves state-of-the-art performance in both zero-shot anomaly detection and segmentation tasks.",
    "authors": "Yanning Hou, Yanran Ruan, Junfa Li, Shanshan Wang, Jianfeng Qiu, Ke Xu",
    "x": -1.0749096870422363,
    "y": 0.2983722388744354,
    "z": -1.314984679222107,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -1.0749096870422363,
        "y": 0.2983722388744354,
        "z": -1.314984679222107
      },
      "starmap": {
        "x": 0.016991572454571724,
        "y": -0.021735278889536858,
        "z": -0.022044453769922256
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Recent advances in large language models (LLMs) have raised concerns about jailbreaking attacks, i.e., prompts that bypass safety mechanisms. This paper investigates the use of multi-agent LLM systems as a defence against such attacks. We evaluate three jailbreaking strategies, including the original AutoDefense attack and two from Deepleaps: BetterDan and JB. Reproducing the AutoDefense framework, we compare single-agent setups with two- and three-agent configurations. Our results show that multi-agent systems enhance resistance to jailbreaks, especially by reducing false negatives. However, its effectiveness varies by attack type, and it introduces trade-offs such as increased false positives and computational overhead. These findings point to the limitations of current automated defences and suggest directions for improving alignment robustness in future LLM systems.",
    "authors": "Maria Carolina Cornelia Wit, Jun Pang",
    "id": "2506.23576v1",
    "title": "Evaluating Multi-Agent Defences Against Jailbreaking Attacks on Large Language Models",
    "date": "2025-06-30",
    "x": 2.5082805156707764,
    "y": 0.2622717618942261,
    "z": 1.8007773160934448,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 2.5082805156707764,
        "y": 0.2622717618942261,
        "z": 1.8007773160934448
      },
      "starmap": {
        "x": -0.024823008105158806,
        "y": 0.009742310270667076,
        "z": 0.0012778916861861944
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Small object detection (SOD) in anti-UAV task is a challenging problem due to the small size of UAVs and complex backgrounds. Traditional frame-based cameras struggle to detect small objects in complex environments due to their low frame rates, limited dynamic range, and data redundancy. Event cameras, with microsecond temporal resolution and high dynamic range, provide a more effective solution for SOD. However, existing event-based object detection datasets are limited in scale, feature large targets size, and lack diverse backgrounds, making them unsuitable for SOD benchmarks. In this paper, we introduce a Event-based Small object detection (EVSOD) dataset (namely EV-UAV), the first large-scale, highly diverse benchmark for anti-UAV tasks. It includes 147 sequences with over 2.3 million event-level annotations, featuring extremely small targets (averaging 6.8 $\\times$ 5.4 pixels) and diverse scenarios such as urban clutter and extreme lighting conditions. Furthermore, based on the observation that small moving targets form continuous curves in spatiotemporal event point clouds, we propose Event based Sparse Segmentation Network (EV-SpSegNet), a novel baseline for event segmentation in point cloud space, along with a Spatiotemporal Correlation (STC) loss that leverages motion continuity to guide the network in retaining target events. Extensive experiments on the EV-UAV dataset demonstrate the superiority of our method and provide a benchmark for future research in EVSOD. The dataset and code are at https://github.com/ChenYichen9527/Ev-UAV.",
    "id": "2506.23575v1",
    "title": "Event-based Tiny Object Detection: A Benchmark Dataset and Baseline",
    "authors": "Nuo Chen, Chao Xiao, Yimian Dai, Shiman He, Miao Li, Wei An",
    "date": "2025-06-30",
    "x": -1.590771198272705,
    "y": -0.025916317477822304,
    "z": 1.1723500490188599,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -1.590771198272705,
        "y": -0.025916317477822304,
        "z": 1.1723500490188599
      },
      "starmap": {
        "x": 0.01983223855495453,
        "y": -0.020659899339079857,
        "z": -0.02015966735780239
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "abstract": "The deployment of robot assistants in large indoor spaces has seen significant growth, with escorting tasks becoming a key application. However, most current escorting robots primarily rely on navigation-focused strategies, assuming that the person being escorted will follow without issue. In crowded environments, this assumption often falls short, as individuals may struggle to keep pace, become obstructed, get distracted, or need to stop unexpectedly. As a result, conventional robotic systems are often unable to provide effective escorting services due to their limited understanding of human movement dynamics. To address these challenges, an effective escorting robot must continuously detect and interpret human actions during the escorting process and adjust its movement accordingly. However, there is currently no existing dataset designed specifically for human action detection in the context of escorting. Given that escorting often occurs in crowded environments, where other individuals may enter the robot's camera view, the robot also needs to identify the specific human it is escorting (the subject) before predicting their actions. Since no existing model performs both person re-identification and action prediction in real-time, we propose a novel neural network architecture that can accomplish both tasks. This enables the robot to adjust its speed dynamically based on the escortee's movements and seamlessly resume escorting after any disruption. In comparative evaluations against strong baselines, our system demonstrates superior efficiency and effectiveness, showcasing its potential to significantly improve robotic escorting services in complex, real-world scenarios.",
    "id": "2506.23573v1",
    "title": "Online Human Action Detection during Escorting",
    "authors": "Siddhartha Mondal, Avik Mitra, Chayan Sarkar",
    "x": -0.36867162585258484,
    "y": 2.383673667907715,
    "z": 0.645287275314331,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": -0.36867162585258484,
        "y": 2.383673667907715,
        "z": 0.645287275314331
      },
      "starmap": {
        "x": -0.016937393695116043,
        "y": 0.004255909472703934,
        "z": 0.00012285551929380745
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Luigi Sigillo, Renato Giamba, Danilo Comminiello",
    "date": "2025-06-30",
    "id": "2506.23566v1",
    "abstract": "The acquisition of high-resolution satellite imagery is often constrained by the spatial and temporal limitations of satellite sensors, as well as the high costs associated with frequent observations. These challenges hinder applications such as environmental monitoring, disaster response, and agricultural management, which require fine-grained and high-resolution data. In this paper, we propose MWT-Diff, an innovative framework for satellite image super-resolution (SR) that combines latent diffusion models with wavelet transforms to address these challenges. At the core of the framework is a novel metadata-, wavelet-, and time-aware encoder (MWT-Encoder), which generates embeddings that capture metadata attributes, multi-scale frequency information, and temporal relationships. The embedded feature representations steer the hierarchical diffusion dynamics, through which the model progressively reconstructs high-resolution satellite imagery from low-resolution inputs. This process preserves critical spatial characteristics including textural patterns, boundary discontinuities, and high-frequency spectral components essential for detailed remote sensing analysis. The comparative analysis of MWT-Diff across multiple datasets demonstrated favorable performance compared to recent approaches, as measured by standard perceptual quality metrics including FID and LPIPS.",
    "title": "Metadata, Wavelet, and Time Aware Diffusion Models for Satellite Image Super Resolution",
    "x": -3.5583152770996094,
    "y": 0.33662769198417664,
    "z": 0.7685649394989014,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -3.5583152770996094,
        "y": 0.33662769198417664,
        "z": 0.7685649394989014
      },
      "starmap": {
        "x": 0.02102750912308693,
        "y": -0.01097702793776989,
        "z": 0.029336828738451004
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Current multi-view 3D object detection methods typically transfer 2D features into 3D space using depth estimation or 3D position encoder, but in a fully data-driven and implicit manner, which limits the detection performance. Inspired by the success of radiance fields on 3D reconstruction, we assume they can be used to enhance the detector's ability of 3D geometry estimation. However, we observe a decline in detection performance, when we directly use them for 3D rendering as an auxiliary task. From our analysis, we find the performance drop is caused by the strong responses on the background when rendering the whole scene. To address this problem, we propose object-centric radiance fields, focusing on modeling foreground objects while discarding background noises. Specifically, we employ Object-centric Radiance Fields (OcRF) to enhance 3D voxel features via an auxiliary task of rendering foreground objects. We further use opacity - the side-product of rendering- to enhance the 2D foreground BEV features via Height-aware Opacity-based Attention (HOA), where attention maps at different height levels are generated separately via multiple networks in parallel. Extensive experiments on the nuScenes validation and test datasets demonstrate that our OcRFDet achieves superior performance, outperforming previous state-of-the-art methods with 57.2$\\%$ mAP and 64.8$\\%$ NDS on the nuScenes test benchmark. Code will be available at https://github.com/Mingqj/OcRFDet.",
    "date": "2025-06-30",
    "id": "2506.23565v1",
    "authors": "Mingqian Ji, Jian Yang, Shanshan Zhang",
    "title": "OcRFDet: Object-Centric Radiance Fields for Multi-View 3D Object Detection in Autonomous Driving",
    "x": -2.0481183528900146,
    "y": 2.389137029647827,
    "z": -0.4245680868625641,
    "cluster_id": 1,
    "layouts": {
      "pacmap": {
        "x": -2.0481183528900146,
        "y": 2.389137029647827,
        "z": -0.4245680868625641
      },
      "starmap": {
        "x": 0.04728786274790764,
        "y": 0.06491165608167648,
        "z": -0.02702277898788452
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for MLLMs Toward AGI",
    "abstract": "Reasoning plays a crucial role in advancing Multimodal Large Language Models (MLLMs) toward Artificial General Intelligence. However, existing MLLM benchmarks often fall short in precisely and comprehensively evaluating long-chain reasoning abilities from three key aspects: (1) lack of difficulty and diversity, (2) susceptibility to guessability and memorization, (3) inadequate assessment of intermediate reasoning steps. To fill this gap, we introduce MMReason, a new benchmark designed to precisely and comprehensively evaluate MLLM long-chain reasoning capability with diverse, open-ended, challenging questions. First, we curate challenging questions requiring multi-step reasoning from various fields (i.e., 6 disciplines) and multiple difficulty levels (i.e., from pre-university to university, and from foundational to competition tiers). Second, these questions are reformulated into an open-ended format and filtered using a multi-model voting technique to eliminate shortcut cases related to guessing and memorization, ensuring robust reasoning evaluations. Third, we annotate the questions with detailed step-by-step solutions, and design a reference-based ternary scoring mechanism to reliably assess intermediate reasoning steps. With MMReason, we benchmark popular leading MLLMs and provide an in-depth analysis of their reasoning capabilities. We hope MMReason will serve as a valuable resource for advancing MLLM reasoning research. Code will be available at https://github.com/HJYao00/MMReason.",
    "date": "2025-06-30",
    "authors": "Huanjin Yao, Jiaxing Huang, Yawen Qiu, Michael K. Chen, Wenzheng Liu, Wei Zhang, Wenjie Zeng, Xikun Zhang, Jingyi Zhang, Yuxin Song, Wenhao Wu, Dacheng Tao",
    "id": "2506.23563v1",
    "x": 3.1005237102508545,
    "y": 1.7759915590286255,
    "z": 0.33620351552963257,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 3.1005237102508545,
        "y": 1.7759915590286255,
        "z": 0.33620351552963257
      },
      "starmap": {
        "x": -0.02261638455092907,
        "y": 0.009970887564122677,
        "z": -0.003342588897794485
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "Tensor Train Quantum State Tomography using Compressed Sensing",
    "abstract": "Quantum state tomography (QST) is a fundamental technique for estimating the state of a quantum system from measured data and plays a crucial role in evaluating the performance of quantum devices. However, standard estimation methods become impractical due to the exponential growth of parameters in the state representation. In this work, we address this challenge by parameterizing the state using a low-rank block tensor train decomposition and demonstrate that our approach is both memory- and computationally efficient. This framework applies to a broad class of quantum states that can be well approximated by low-rank decompositions, including pure states, nearly pure states, and ground states of Hamiltonians.",
    "date": "2025-06-30",
    "id": "2506.23560v1",
    "authors": "Shakir Showkat Sofi, Charlotte Vermeylen, Lieven De Lathauwer",
    "x": -0.17553430795669556,
    "y": -1.0452250242233276,
    "z": 2.592007875442505,
    "cluster_id": 6,
    "layouts": {
      "pacmap": {
        "x": -0.17553430795669556,
        "y": -1.0452250242233276,
        "z": 2.592007875442505
      },
      "starmap": {
        "x": 0.0009587066597305238,
        "y": -0.008437654003500938,
        "z": 0.0005287674721330404
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "In current practical face authentication systems, most face recognition (FR) algorithms are based on cosine similarity with softmax classification. Despite its reliable classification performance, this method struggles with hard samples. A popular strategy to improve FR performance is incorporating angular or cosine margins. However, it does not take face quality or recognition hardness into account, simply increasing the margin value and thus causing an overly uniform training strategy. To address this problem, a novel loss function is proposed, named Loss function for Hard High-quality Face (LH2Face). Firstly, a similarity measure based on the von Mises-Fisher (vMF) distribution is stated, specifically focusing on the logarithm of the Probability Density Function (PDF), which represents the distance between a probability distribution and a vector. Then, an adaptive margin-based multi-classification method using softmax, called the Uncertainty-Aware Margin Function, is implemented in the article. Furthermore, proxy-based loss functions are used to apply extra constraints between the proxy and sample to optimize their representation space distribution. Finally, a renderer is constructed that optimizes FR through face reconstruction and vice versa. Our LH2Face is superior to similiar schemes on hard high-quality face datasets, achieving 49.39% accuracy on the IJB-B dataset, which surpasses the second-place method by 2.37%.",
    "date": "2025-06-30",
    "id": "2506.23555v1",
    "authors": "Fan Xie, Pan Cao",
    "title": "LH2Face: Loss function for Hard High-quality Face",
    "x": 1.1417770385742188,
    "y": -0.4835081696510315,
    "z": -1.9320439100265503,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": 1.1417770385742188,
        "y": -0.4835081696510315,
        "z": -1.9320439100265503
      },
      "starmap": {
        "x": 0.005544545128941536,
        "y": -0.006362434476613998,
        "z": 0.0005940984119661152
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "authors": "Mingi Kwon, Joonghyuk Shin, Jaeseok Jung, Jaesik Park, Youngjung Uh",
    "id": "2506.23552v1",
    "title": "JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching",
    "abstract": "The intrinsic link between facial motion and speech is often overlooked in generative modeling, where talking head synthesis and text-to-speech (TTS) are typically addressed as separate tasks. This paper introduces JAM-Flow, a unified framework to simultaneously synthesize and condition on both facial motion and speech. Our approach leverages flow matching and a novel Multi-Modal Diffusion Transformer (MM-DiT) architecture, integrating specialized Motion-DiT and Audio-DiT modules. These are coupled via selective joint attention layers and incorporate key architectural choices, such as temporally aligned positional embeddings and localized joint attention masking, to enable effective cross-modal interaction while preserving modality-specific strengths. Trained with an inpainting-style objective, JAM-Flow supports a wide array of conditioning inputs-including text, reference audio, and reference motion-facilitating tasks such as synchronized talking head generation from text, audio-driven animation, and much more, within a single, coherent model. JAM-Flow significantly advances multi-modal generative modeling by providing a practical solution for holistic audio-visual synthesis. project page: https://joonghyuk.com/jamflow-web",
    "x": -2.4033384323120117,
    "y": 1.0197848081588745,
    "z": 0.9883514642715454,
    "cluster_id": 2,
    "layouts": {
      "pacmap": {
        "x": -2.4033384323120117,
        "y": 1.0197848081588745,
        "z": 0.9883514642715454
      },
      "starmap": {
        "x": 0.020837489515542984,
        "y": -0.00471285916864872,
        "z": 0.059688832610845566
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "title": "A unified framework on the universal approximation of transformer-type architectures",
    "id": "2506.23551v1",
    "authors": "Jingpu Cheng, Qianxiao Li, Ting Lin, Zuowei Shen",
    "abstract": "We investigate the universal approximation property (UAP) of transformer-type architectures, providing a unified theoretical framework that extends prior results on residual networks to models incorporating attention mechanisms. Our work identifies token distinguishability as a fundamental requirement for UAP and introduces a general sufficient condition that applies to a broad class of architectures. Leveraging an analyticity assumption on the attention layer, we can significantly simplify the verification of this condition, providing a non-constructive approach in establishing UAP for such architectures. We demonstrate the applicability of our framework by proving UAP for transformers with various attention mechanisms, including kernel-based and sparse attention mechanisms. The corollaries of our results either generalize prior works or establish UAP for architectures not previously covered. Furthermore, our framework offers a principled foundation for designing novel transformer architectures with inherent UAP guarantees, including those with specific functional symmetries. We propose examples to illustrate these insights.",
    "x": -1.4697608947753906,
    "y": -2.973496675491333,
    "z": 0.1646774262189865,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -1.4697608947753906,
        "y": -2.973496675491333,
        "z": 0.1646774262189865
      },
      "starmap": {
        "x": 0.01591780222952366,
        "y": -0.017816731706261635,
        "z": -0.002268434502184391
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23550v1",
    "authors": "Ryui Kaneko, Shimpei Goto",
    "title": "Seeding neural network quantum states with tensor network states",
    "date": "2025-06-30",
    "abstract": "We find an efficient approach to approximately convert matrix product states (MPSs) into restricted Boltzmann machine wave functions consisting of a multinomial hidden unit through a canonical polyadic (CP) decomposition of the MPSs. This method allows us to generate well-behaved initial neural network quantum states for quantum many-body ground-state calculations in polynomial time of the number of variational parameters and systematically shorten the distance between the initial states and the ground states with increasing the rank of the CP decomposition. We demonstrate the efficiency of our method by taking the transverse-field Ising model as an example and discuss possible applications of our method to more general quantum many-body systems in which the ground-state wave functions possess complex nodal structures.",
    "x": -0.2645532786846161,
    "y": -1.1867543458938599,
    "z": 2.8135225772857666,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -0.2645532786846161,
        "y": -1.1867543458938599,
        "z": 2.8135225772857666
      },
      "starmap": {
        "x": 0.0005085606826469302,
        "y": -0.006761524826288223,
        "z": -0.0006379018886946142
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Effective coordination among artificial agents in dynamic and uncertain environments remains a significant challenge in multi-agent systems. Existing approaches, such as self-play and population-based methods, either generalize poorly to unseen partners or require extensive training. To overcome these limitations, we propose Coordination Transformers (CooT), a novel in-context coordination framework that uses recent interaction histories to adapt to unseen partners rapidly. Unlike previous approaches that primarily aim to increase the diversity of training partners, CooT explicitly focuses on adapting to new partner behaviors by predicting actions aligned with observed partner interactions. Trained on interaction trajectories collected from diverse pairs of agents with complementary behaviors, CooT quickly learns effective coordination strategies without explicit supervision or fine-tuning. Evaluations on the Overcooked benchmark demonstrate that CooT significantly outperforms baseline methods in coordination tasks involving previously unseen partners. Human evaluations further confirm CooT as the most effective collaborative partner, while extensive ablations highlight its robustness, flexibility, and sensitivity to context in multi-agent scenarios.",
    "id": "2506.23549v1",
    "title": "CooT: Learning to Coordinate In-Context with Coordination Transformers",
    "authors": "Huai-Chih Wang, Hsiang-Chun Chuang, Hsi-Chun Cheng, Dai-Jie Wu, Shao-Hua Sun",
    "date": "2025-06-30",
    "x": 1.8316148519515991,
    "y": 0.9496424794197083,
    "z": 2.515756845474243,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 1.8316148519515991,
        "y": 0.9496424794197083,
        "z": 2.515756845474243
      },
      "starmap": {
        "x": -0.024582967162132263,
        "y": 0.010021647438406944,
        "z": 7.368777005467564e-05
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Jiwon Kim, Soohyun Hwang, Dong-O Kim, Changsu Han, Min Kyu Park, Chang-Su Kim",
    "title": "Oneta: Multi-Style Image Enhancement Using Eigentransformation Functions",
    "date": "2025-06-30",
    "id": "2506.23547v1",
    "abstract": "The first algorithm, called Oneta, for a novel task of multi-style image enhancement is proposed in this work. Oneta uses two point operators sequentially: intensity enhancement with a transformation function (TF) and color correction with a color correction matrix (CCM). This two-step enhancement model, though simple, achieves a high performance upper bound. Also, we introduce eigentransformation function (eigenTF) to represent TF compactly. The Oneta network comprises Y-Net and C-Net to predict eigenTF and CCM parameters, respectively. To support $K$ styles, Oneta employs $K$ learnable tokens. During training, each style token is learned using image pairs from the corresponding dataset. In testing, Oneta selects one of the $K$ style tokens to enhance an image accordingly. Extensive experiments show that the single Oneta network can effectively undertake six enhancement tasks -- retouching, image signal processing, low-light image enhancement, dehazing, underwater image enhancement, and white balancing -- across 30 datasets.",
    "x": -3.525475025177002,
    "y": 0.6764295697212219,
    "z": -0.34220948815345764,
    "cluster_id": 1,
    "layouts": {
      "pacmap": {
        "x": -3.525475025177002,
        "y": 0.6764295697212219,
        "z": -0.34220948815345764
      },
      "starmap": {
        "x": 0.04247939586639404,
        "y": 0.047981780022382736,
        "z": 0.003490478964522481
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Fixed points of recurrent neural networks can be leveraged to store and generate information. These fixed points can be captured by the Boltzmann-Gibbs measure, which leads to neural Langevin dynamics that can be used for sampling and learning a real dataset. We call this type of generative model neural Langevin machine, which is interpretable due to its analytic form of distribution and is simple to train. Moreover, the learning process is derived as a local asymmetric plasticity rule, bearing biological relevance. Therefore, one can realize a continuous sampling of creative dynamics in a neural network, mimicking an imagination process in brain circuits. This neural Langevin machine may be another promising generative model, at least in its strength in circuit-based sampling and biologically plausible learning rule.",
    "id": "2506.23546v1",
    "title": "Neural Langevin Machine: a local asymmetric learning rule can be creative",
    "authors": "Zhendong Yu, Weizhong Huang, Haiping Huang",
    "date": "2025-06-30",
    "x": -0.2226821929216385,
    "y": -2.753051996231079,
    "z": 0.7657762169837952,
    "cluster_id": 7,
    "layouts": {
      "pacmap": {
        "x": -0.2226821929216385,
        "y": -2.753051996231079,
        "z": 0.7657762169837952
      },
      "starmap": {
        "x": 0.005928041413426399,
        "y": -0.011250298470258713,
        "z": -0.013115898706018925
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "authors": "Kento Imaizumi, Hideaki Iiduka",
    "id": "2506.23544v2",
    "abstract": "Momentum methods were originally introduced for their superiority to stochastic gradient descent (SGD) in deterministic settings with convex objective functions. However, despite their widespread application to deep neural networks -- a representative case of stochastic nonconvex optimization -- the theoretical justification for their effectiveness in such settings remains limited. Quasi-hyperbolic momentum (QHM) is an algorithm that generalizes various momentum methods and has been studied to better understand the class of momentum-based algorithms as a whole. In this paper, we provide both asymptotic and non-asymptotic convergence results for mini-batch QHM with an increasing batch size. We show that achieving asymptotic convergence requires either a decaying learning rate or an increasing batch size. Since a decaying learning rate adversely affects non-asymptotic convergence, we demonstrate that using mini-batch QHM with an increasing batch size -- without decaying the learning rate -- can be a more effective strategy. Our experiments show that even a finite increase in batch size can provide benefits for training neural networks.",
    "title": "Both Asymptotic and Non-Asymptotic Convergence of Quasi-Hyperbolic Momentum using Increasing Batch Size",
    "x": 0.7695498466491699,
    "y": -3.0717086791992188,
    "z": 0.7605557441711426,
    "cluster_id": 8,
    "layouts": {
      "pacmap": {
        "x": 0.7695498466491699,
        "y": -3.0717086791992188,
        "z": 0.7605557441711426
      },
      "starmap": {
        "x": 0.010286493226885796,
        "y": -0.007239061873406172,
        "z": -0.003944240510463715
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "id": "2506.23542v1",
    "title": "Consistent Time-of-Flight Depth Denoising via Graph-Informed Geometric Attention",
    "abstract": "Depth images captured by Time-of-Flight (ToF) sensors are prone to noise, requiring denoising for reliable downstream applications. Previous works either focus on single-frame processing, or perform multi-frame processing without considering depth variations at corresponding pixels across frames, leading to undesirable temporal inconsistency and spatial ambiguity. In this paper, we propose a novel ToF depth denoising network leveraging motion-invariant graph fusion to simultaneously enhance temporal stability and spatial sharpness. Specifically, despite depth shifts across frames, graph structures exhibit temporal self-similarity, enabling cross-frame geometric attention for graph fusion. Then, by incorporating an image smoothness prior on the fused graph and data fidelity term derived from ToF noise distribution, we formulate a maximum a posterior problem for ToF denoising. Finally, the solution is unrolled into iterative filters whose weights are adaptively learned from the graph-informed geometric attention, producing a high-performance yet interpretable network. Experimental results demonstrate that the proposed scheme achieves state-of-the-art performance in terms of accuracy and consistency on synthetic DVToF dataset and exhibits robust generalization on the real Kinectv2 dataset. Source code will be released at \\href{https://github.com/davidweidawang/GIGA-ToF}{https://github.com/davidweidawang/GIGA-ToF}.",
    "authors": "Weida Wang, Changyong He, Jin Zeng, Di Qiu",
    "x": -3.0259578227996826,
    "y": 0.18372468650341034,
    "z": 0.9130235314369202,
    "cluster_id": 1,
    "layouts": {
      "pacmap": {
        "x": -3.0259578227996826,
        "y": 0.18372468650341034,
        "z": 0.9130235314369202
      },
      "starmap": {
        "x": 0.03456592187285423,
        "y": 0.0330185666680336,
        "z": -0.013033105060458183
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Hui Li, Baoyou Chen, Liwei Zhang, Jiaye Li, Jingdong Wang, Siyu Zhu",
    "id": "2506.23543v1",
    "date": "2025-06-30",
    "title": "Pyramidal Patchification Flow for Visual Generation",
    "abstract": "Diffusion transformers (DiTs) adopt Patchify, mapping patch representations to token representations through linear projections, to adjust the number of tokens input to DiT blocks and thus the computation cost. Instead of a single patch size for all the timesteps, we introduce a Pyramidal Patchification Flow (PPFlow) approach: Large patch sizes are used for high noise timesteps and small patch sizes for low noise timesteps; Linear projections are learned for each patch size; and Unpatchify is accordingly modified. Unlike Pyramidal Flow, our approach operates over full latent representations other than pyramid representations, and adopts the normal denoising process without requiring the renoising trick. We demonstrate the effectiveness of our approach through two training manners. Training from scratch achieves a $1.6\\times$ ($2.0\\times$) inference speed over SiT-B/2 for 2-level (3-level) pyramid patchification with slightly lower training FLOPs and similar image generation performance. Training from pretrained normal DiTs achieves even better performance with small training time. The code and checkpoint are at https://github.com/fudan-generative-vision/PPFlow.",
    "x": -2.351888418197632,
    "y": -0.15098866820335388,
    "z": 0.5180385112762451,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -2.351888418197632,
        "y": -0.15098866820335388,
        "z": 0.5180385112762451
      },
      "starmap": {
        "x": 0.02194487489759922,
        "y": -0.0016084930393844843,
        "z": 0.004998975433409214
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Yuhao Huang, Yueyue Xu, Haoran Dou, Jiaxiao Deng, Xin Yang, Hongyu Zheng, Dong Ni",
    "id": "2506.23538v1",
    "date": "2025-06-30",
    "abstract": "Congenital uterine anomalies (CUAs) can lead to infertility, miscarriage, preterm birth, and an increased risk of pregnancy complications. Compared to traditional 2D ultrasound (US), 3D US can reconstruct the coronal plane, providing a clear visualization of the uterine morphology for assessing CUAs accurately. In this paper, we propose an intelligent system for simultaneous automated plane localization and CUA diagnosis. Our highlights are: 1) we develop a denoising diffusion model with local (plane) and global (volume/text) guidance, using an adaptive weighting strategy to optimize attention allocation to different conditions; 2) we introduce a reinforcement learning-based framework with unsupervised rewards to extract the key slice summary from redundant sequences, fully integrating information across multiple planes to reduce learning difficulty; 3) we provide text-driven uncertainty modeling for coarse prediction, and leverage it to adjust the classification probability for overall performance improvement. Extensive experiments on a large 3D uterine US dataset show the efficacy of our method, in terms of plane localization and CUA diagnosis. Code is available at https://github.com/yuhoo0302/CUA-US.",
    "title": "Uncertainty-aware Diffusion and Reinforcement Learning for Joint Plane Localization and Anomaly Diagnosis in 3D Ultrasound",
    "x": -2.132434129714966,
    "y": 2.213369607925415,
    "z": -1.4735602140426636,
    "cluster_id": 4,
    "layouts": {
      "pacmap": {
        "x": -2.132434129714966,
        "y": 2.213369607925415,
        "z": -1.4735602140426636
      },
      "starmap": {
        "x": 0.019269850105047226,
        "y": -0.01976042427122593,
        "z": -0.002709172433242202
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Existing learning-based methods effectively reconstruct HDR images from multi-exposure LDR inputs with extended dynamic range and improved detail, but they rely more on empirical design rather than theoretical foundation, which can impact their reliability. To address these limitations, we propose the cross-iterative Alignment and Fusion deep Unfolding Network (AFUNet), where HDR reconstruction is systematically decoupled into two interleaved subtasks -- alignment and fusion -- optimized through alternating refinement, achieving synergy between the two subtasks to enhance the overall performance. Our method formulates multi-exposure HDR reconstruction from a Maximum A Posteriori (MAP) estimation perspective, explicitly incorporating spatial correspondence priors across LDR images and naturally bridging the alignment and fusion subproblems through joint constraints. Building on the mathematical foundation, we reimagine traditional iterative optimization through unfolding -- transforming the conventional solution process into an end-to-end trainable AFUNet with carefully designed modules that work progressively. Specifically, each iteration of AFUNet incorporates an Alignment-Fusion Module (AFM) that alternates between a Spatial Alignment Module (SAM) for alignment and a Channel Fusion Module (CFM) for adaptive feature fusion, progressively bridging misaligned content and exposure discrepancies. Extensive qualitative and quantitative evaluations demonstrate AFUNet's superior performance, consistently surpassing state-of-the-art methods. Our code is available at: https://github.com/eezkni/AFUNet",
    "id": "2506.23537v1",
    "authors": "Xinyue Li, Zhangkai Ni, Wenhan Yang",
    "date": "2025-06-30",
    "title": "AFUNet: Cross-Iterative Alignment-Fusion Synergy for HDR Reconstruction via Deep Unfolding Paradigm",
    "x": -3.1922388076782227,
    "y": -0.05136573314666748,
    "z": 0.5906679630279541,
    "cluster_id": 1,
    "layouts": {
      "pacmap": {
        "x": -3.1922388076782227,
        "y": -0.05136573314666748,
        "z": 0.5906679630279541
      },
      "starmap": {
        "x": 0.040652185678482056,
        "y": 0.04240886867046356,
        "z": -0.012154119089245796
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "We introduce GVIT, a classification framework that abandons conventional pixel or patch grid input representations in favor of a compact set of learnable 2D Gaussians. Each image is encoded as a few hundred Gaussians whose positions, scales, orientations, colors, and opacities are optimized jointly with a ViT classifier trained on top of these representations. We reuse the classifier gradients as constructive guidance, steering the Gaussians toward class-salient regions while a differentiable renderer optimizes an image reconstruction loss. We demonstrate that by 2D Gaussian input representations coupled with our GVIT guidance, using a relatively standard ViT architecture, closely matches the performance of a traditional patch-based ViT, reaching a 76.9% top-1 accuracy on Imagenet-1k using a ViT-B architecture.",
    "authors": "Jefferson Hernandez, Ruozhen He, Guha Balakrishnan, Alexander C. Berg, Vicente Ordonez",
    "title": "GViT: Representing Images as Gaussians for Visual Recognition",
    "id": "2506.23532v1",
    "date": "2025-06-30",
    "x": -2.9388222694396973,
    "y": 0.07762856781482697,
    "z": -0.1472609043121338,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -2.9388222694396973,
        "y": 0.07762856781482697,
        "z": -0.1472609043121338
      },
      "starmap": {
        "x": 0.02972458116710186,
        "y": 0.021325230598449707,
        "z": 0.003811295609921217
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23529v1",
    "authors": "Jisu Han, Jihee Park, Dongyoon Han, Wonjun Hwang",
    "date": "2025-06-30",
    "abstract": "Training on test-time data enables deep learning models to adapt to dynamic environmental changes, enhancing their practical applicability. Online adaptation from source to target domains is promising but it remains highly reliant on the performance of source pretrained model. In this paper, we investigate whether test-time adaptation (TTA) methods can continuously improve models trained via self-supervised learning (SSL) without relying on source pretraining. We introduce a self-supervised TTA protocol after observing that existing TTA approaches struggle when directly applied to self-supervised models with low accuracy on the source domain. Furthermore, we propose a collaborative learning framework that integrates SSL and TTA models, leveraging contrastive learning and knowledge distillation for stepwise representation refinement. We validate our method on diverse self-supervised models, including DINO, MoCo, and iBOT, across TTA benchmarks. Extensive experiments validate the effectiveness of our approach in SSL, showing that it achieves competitive performance even without source pretraining.",
    "title": "When Test-Time Adaptation Meets Self-Supervised Models",
    "x": -1.0303964614868164,
    "y": -2.229722261428833,
    "z": -0.9916090369224548,
    "cluster_id": 5,
    "layouts": {
      "pacmap": {
        "x": -1.0303964614868164,
        "y": -2.229722261428833,
        "z": -0.9916090369224548
      },
      "starmap": {
        "x": 0.01002814806997776,
        "y": -0.026277603581547737,
        "z": -0.023011017590761185
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "title": "On Recipe Memorization and Creativity in Large Language Models: Is Your Model a Creative Cook, a Bad Cook, or Merely a Plagiator?",
    "id": "2506.23527v1",
    "abstract": "This work-in-progress investigates the memorization, creativity, and nonsense found in cooking recipes generated from Large Language Models (LLMs). Precisely, we aim (i) to analyze memorization, creativity, and non-sense in LLMs using a small, high-quality set of human judgments and (ii) to evaluate potential approaches to automate such a human annotation in order to scale our study to hundreds of recipes. To achieve (i), we conduct a detailed human annotation on 20 preselected recipes generated by LLM (Mixtral), extracting each recipe's ingredients and step-by-step actions to assess which elements are memorized--i.e., directly traceable to online sources possibly seen during training--and which arise from genuine creative synthesis or outright nonsense. We find that Mixtral consistently reuses ingredients that can be found in online documents, potentially seen during model training, suggesting strong reliance on memorized content. To achieve aim (ii) and scale our analysis beyond small sample sizes and single LLM validation, we design an ``LLM-as-judge'' pipeline that automates recipe generation, nonsense detection, parsing ingredients and recipe steps, and their annotation. For instance, comparing its output against human annotations, the best ingredient extractor and annotator is Llama 3.1+Gemma 2 9B, achieving up to 78% accuracy on ingredient matching. This automated framework enables large-scale quantification of memorization, creativity, and nonsense in generated recipes, providing rigorous evidence of the models' creative capacities.",
    "authors": "Jan Kvapil, Martin Fajcik",
    "x": 2.676025390625,
    "y": 1.3765136003494263,
    "z": 1.368382453918457,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 2.676025390625,
        "y": 1.3765136003494263,
        "z": 1.368382453918457
      },
      "starmap": {
        "x": -0.023102324455976486,
        "y": 0.008317066356539726,
        "z": 0.0013750846264883876
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "abstract": "In the field of education, understanding students' opinions through their comments is crucial, especially in the Vietnamese language, where resources remain limited. Existing educational datasets often lack domain relevance and student slang. To address these gaps, we introduce NEU-ESC, a new Vietnamese dataset for Educational Sentiment Classification and Topic Classification, curated from university forums, which offers more samples, richer class diversity, longer texts, and broader vocabulary. In addition, we explore multitask learning using encoder-only language models (BERT), in which we showed that it achieves performance up to 83.7% and 79.8% accuracy for sentiment and topic classification tasks. We also benchmark our dataset and model with other datasets and models, including Large Language Models, and discuss these benchmarks. The dataset is publicly available at: https://huggingface.co/datasets/hung20gg/NEU-ESC.",
    "title": "NEU-ESC: A Comprehensive Vietnamese dataset for Educational Sentiment analysis and topic Classification toward multitask learning",
    "authors": "Phan Quoc Hung Mai, Quang Hung Nguyen, Phuong Giang Duong, Hong Hanh Nguyen, Nguyen Tuan Long",
    "id": "2506.23524v1",
    "x": 1.4196261167526245,
    "y": 0.8832892775535583,
    "z": -1.3013068437576294,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 1.4196261167526245,
        "y": 0.8832892775535583,
        "z": -1.3013068437576294
      },
      "starmap": {
        "x": -0.019030479714274406,
        "y": 0.003981271293014288,
        "z": -0.0011993858497589827
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "abstract": "Traditional vision-based autonomous driving systems often face difficulties in navigating complex environments when relying solely on single-image inputs. To overcome this limitation, incorporating temporal data such as past image frames or steering sequences, has proven effective in enhancing robustness and adaptability in challenging scenarios. While previous high-performance methods exist, they often rely on resource-intensive fusion networks, making them impractical for training and unsuitable for federated learning. To address these challenges, we propose lightweight temporal transformer decomposition, a method that processes sequential image frames and temporal steering data by breaking down large attention maps into smaller matrices. This approach reduces model complexity, enabling efficient weight updates for convergence and real-time predictions while leveraging temporal information to enhance autonomous driving performance. Intensive experiments on three datasets demonstrate that our method outperforms recent approaches by a clear margin while achieving real-time performance. Additionally, real robot experiments further confirm the effectiveness of our method.",
    "id": "2506.23523v1",
    "title": "Lightweight Temporal Transformer Decomposition for Federated Autonomous Driving",
    "authors": "Tuong Do, Binh X. Nguyen, Quang D. Tran, Erman Tjiputra, Te-Chuan Chiu, Anh Nguyen",
    "x": -0.718745231628418,
    "y": -0.27109044790267944,
    "z": 0.6596670746803284,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": -0.718745231628418,
        "y": -0.27109044790267944,
        "z": 0.6596670746803284
      },
      "starmap": {
        "x": -0.011677844449877739,
        "y": 0.004329633433371782,
        "z": -0.0010721714934334159
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "With the increasing interest in robotic synthesis in the context of organic chemistry, the automated extraction of chemical procedures from literature is critical. However, this task remains challenging due to the inherent ambiguity of chemical language and the high cost of human annotation required for developing reliable computer-aided extraction protocols. Here, we present ChemActor, a fully fine-tuned large language model (LLM), as a chemical executor to convert between unstructured experimental procedures and structured action sequences. We propose a sequential LLM-generated data framework to address the challenges of insufficient and low-quality annotated data. This framework integrates a data selection module that selects data based on distribution divergence, with a general-purpose LLM, to generate machine-executable actions from a single molecule input. Additionally, we introduce a novel multi-round LLMs circle review metric, which reflects the model's advanced understanding of chemical experimental procedures. Extensive experiments on reaction-to-description (R2D) and description-to-action (D2A) tasks demonstrate that ChemActor, augmented by LLM-generated data, achieves state-of-the-art performance, outperforming the baseline model by 10%. The code is available at: https://github.com/Zhanghahah/ChemActor.",
    "authors": "Yu Zhang, Ruijie Yu, Jidong Tian, Feng Zhu, Jiapeng Liu, Xiaokang Yang, Yaohui Jin, Yanyan Xu",
    "title": "ChemActor: Enhancing Automated Extraction of Chemical Synthesis Actions with LLM-Generated Data",
    "id": "2506.23520v2",
    "date": "2025-06-30",
    "x": 2.0057873725891113,
    "y": 1.0417120456695557,
    "z": 1.197209358215332,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 2.0057873725891113,
        "y": 1.0417120456695557,
        "z": 1.197209358215332
      },
      "starmap": {
        "x": -0.020286351442337036,
        "y": 0.005162817891687155,
        "z": -0.0004917256883345544
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "id": "2506.23519v1",
    "authors": "Qi Qin, Runmin Cong, Gen Zhan, Yiting Liao, Sam Kwong",
    "title": "From Sight to Insight: Unleashing Eye-Tracking in Weakly Supervised Video Salient Object Detection",
    "abstract": "The eye-tracking video saliency prediction (VSP) task and video salient object detection (VSOD) task both focus on the most attractive objects in video and show the result in the form of predictive heatmaps and pixel-level saliency masks, respectively. In practical applications, eye tracker annotations are more readily obtainable and align closely with the authentic visual patterns of human eyes. Therefore, this paper aims to introduce fixation information to assist the detection of video salient objects under weak supervision. On the one hand, we ponder how to better explore and utilize the information provided by fixation, and then propose a Position and Semantic Embedding (PSE) module to provide location and semantic guidance during the feature learning process. On the other hand, we achieve spatiotemporal feature modeling under weak supervision from the aspects of feature selection and feature contrast. A Semantics and Locality Query (SLQ) Competitor with semantic and locality constraints is designed to effectively select the most matching and accurate object query for spatiotemporal modeling. In addition, an Intra-Inter Mixed Contrastive (IIMC) model improves the spatiotemporal modeling capabilities under weak supervision by forming an intra-video and inter-video contrastive learning paradigm. Experimental results on five popular VSOD benchmarks indicate that our model outperforms other competitors on various evaluation metrics.",
    "x": -2.344172477722168,
    "y": 1.8926235437393188,
    "z": 1.0523368120193481,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -2.344172477722168,
        "y": 1.8926235437393188,
        "z": 1.0523368120193481
      },
      "starmap": {
        "x": 0.026875928044319153,
        "y": -0.006294707767665386,
        "z": 0.020996710285544395
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "abstract": "Generating high-quality novel views of a scene from a single image requires maintaining structural coherence across different views, referred to as view consistency. While diffusion models have driven advancements in novel view synthesis, they still struggle to preserve spatial continuity across views. Diffusion models have been combined with 3D models to address the issue, but such approaches lack efficiency due to their complex multi-step pipelines. This paper proposes a novel view-consistent image generation method which utilizes diffusion models without additional modules. Our key idea is to enhance diffusion models with a training-free method that enables adaptive attention manipulation and noise reinitialization by leveraging view-guided warping to ensure view consistency. Through our comprehensive metric framework suitable for novel-view datasets, we show that our method improves view consistency across various diffusion models, demonstrating its broader applicability.",
    "authors": "Jiwoo Park, Tae Eun Choi, Youngjun Jun, Seong Jae Hwang",
    "id": "2506.23518v1",
    "title": "WAVE: Warp-Based View Guidance for Consistent Novel View Synthesis Using a Single Image",
    "x": -2.595029830932617,
    "y": 1.504791021347046,
    "z": -0.5513482689857483,
    "cluster_id": 2,
    "layouts": {
      "pacmap": {
        "x": -2.595029830932617,
        "y": 1.504791021347046,
        "z": -0.5513482689857483
      },
      "starmap": {
        "x": 0.02667098306119442,
        "y": 0.004243642557412386,
        "z": 0.06403766572475433
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "As the use of AI tools by students has become more prevalent, instructors have started using AI detection tools like GPTZero and QuillBot to detect AI written text. However, the reliability of these detectors remains uncertain. In our study, we focused mostly on the success rate of GPTZero, the most-used AI detector, in identifying AI-generated texts based on different lengths of randomly submitted essays: short (40-100 word count), medium (100-350 word count), and long (350-800 word count). We gathered a data set consisting of twenty-eight AI-generated papers and fifty human-written papers. With this randomized essay data, papers were individually plugged into GPTZero and measured for percentage of AI generation and confidence. A vast majority of the AI-generated papers were detected accurately (ranging from 91-100% AI believed generation), while the human generated essays fluctuated; there were a handful of false positives. These findings suggest that although GPTZero is effective at detecting purely AI-generated content, its reliability in distinguishing human-authored texts is limited. Educators should therefore exercise caution when relying solely on AI detection tools.",
    "authors": "Selin Dik, Osman Erdem, Mehmet Dik",
    "title": "Assessing GPTZero's Accuracy in Identifying AI vs. Human-Written Essays",
    "date": "2025-06-30",
    "id": "2506.23517v1",
    "x": 3.217794418334961,
    "y": -0.6740509867668152,
    "z": 0.8901575803756714,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 3.217794418334961,
        "y": -0.6740509867668152,
        "z": 0.8901575803756714
      },
      "starmap": {
        "x": -0.02354496717453003,
        "y": 0.007931869477033615,
        "z": 0.0009116611327044666
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization",
    "abstract": "Federated learning (FL) often suffers from performance degradation due to key challenges such as data heterogeneity and communication constraints. To address these limitations, we present a novel FL framework called FedWSQ, which integrates weight standardization (WS) and the proposed distribution-aware non-uniform quantization (DANUQ). WS enhances FL performance by filtering out biased components in local updates during training, thereby improving the robustness of the model against data heterogeneity and unstable client participation. In addition, DANUQ minimizes quantization errors by leveraging the statistical properties of local model updates. As a result, FedWSQ significantly reduces communication overhead while maintaining superior model accuracy. Extensive experiments on FL benchmark datasets demonstrate that FedWSQ consistently outperforms existing FL methods across various challenging FL settings, including extreme data heterogeneity and ultra-low-bit communication scenarios.",
    "id": "2506.23516v1",
    "authors": "Seung-Wook Kim, Seongyeol Kim, Jiah Kim, Seowon Ji, Se-Ho Lee",
    "date": "2025-06-30",
    "x": -0.17573626339435577,
    "y": -2.024061918258667,
    "z": 0.6972757577896118,
    "cluster_id": 6,
    "layouts": {
      "pacmap": {
        "x": -0.17573626339435577,
        "y": -2.024061918258667,
        "z": 0.6972757577896118
      },
      "starmap": {
        "x": 0.013764351606369019,
        "y": -0.016869978979229927,
        "z": -0.015629447996616364
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Sai Krishna Ghanta, Ramviyas Parasuraman",
    "abstract": "Relative localization is a crucial capability for multi-robot systems operating in GPS-denied environments. Existing approaches for multi-robot relative localization often depend on costly or short-range sensors like cameras and LiDARs. Consequently, these approaches face challenges such as high computational overhead (e.g., map merging) and difficulties in disjoint environments. To address this limitation, this paper introduces MGPRL, a novel distributed framework for multi-robot relative localization using convex-hull of multiple Wi-Fi access points (AP). To accomplish this, we employ co-regionalized multi-output Gaussian Processes for efficient Radio Signal Strength Indicator (RSSI) field prediction and perform uncertainty-aware multi-AP localization, which is further coupled with weighted convex hull-based alignment for robust relative pose estimation. Each robot predicts the RSSI field of the environment by an online scan of APs in its environment, which are utilized for position estimation of multiple APs. To perform relative localization, each robot aligns the convex hull of its predicted AP locations with that of the neighbor robots. This approach is well-suited for devices with limited computational resources and operates solely on widely available Wi-Fi RSSI measurements without necessitating any dedicated pre-calibration or offline fingerprinting. We rigorously evaluate the performance of the proposed MGPRL in ROS simulations and demonstrate it with real-world experiments, comparing it against multiple state-of-the-art approaches. The results showcase that MGPRL outperforms existing methods in terms of localization accuracy and computational efficiency. Finally, we open source MGPRL as a ROS package https://github.com/herolab-uga/MGPRL.",
    "id": "2506.23514v1",
    "date": "2025-06-30",
    "title": "MGPRL: Distributed Multi-Gaussian Processes for Wi-Fi-based Multi-Robot Relative Localization in Large Indoor Environments",
    "x": -1.3892757892608643,
    "y": 1.1622405052185059,
    "z": -0.9006442427635193,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": -1.3892757892608643,
        "y": 1.1622405052185059,
        "z": -0.9006442427635193
      },
      "starmap": {
        "x": -0.009461859241127968,
        "y": 0.0009820442646741867,
        "z": 0.0003901249438058585
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Zixun Fang, Kai Zhu, Zhiheng Liu, Yu Liu, Wei Zhai, Yang Cao, Zheng-Jun Zha",
    "title": "ViewPoint: Panoramic Video Generation with Pretrained Diffusion Models",
    "id": "2506.23513v1",
    "abstract": "Panoramic video generation aims to synthesize 360-degree immersive videos, holding significant importance in the fields of VR, world models, and spatial intelligence. Existing works fail to synthesize high-quality panoramic videos due to the inherent modality gap between panoramic data and perspective data, which constitutes the majority of the training data for modern diffusion models. In this paper, we propose a novel framework utilizing pretrained perspective video models for generating panoramic videos. Specifically, we design a novel panorama representation named ViewPoint map, which possesses global spatial continuity and fine-grained visual details simultaneously. With our proposed Pano-Perspective attention mechanism, the model benefits from pretrained perspective priors and captures the panoramic spatial correlations of the ViewPoint map effectively. Extensive experiments demonstrate that our method can synthesize highly dynamic and spatially consistent panoramic videos, achieving state-of-the-art performance and surpassing previous methods.",
    "date": "2025-06-30",
    "x": -2.1598856449127197,
    "y": 2.0381577014923096,
    "z": 1.6054595708847046,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -2.1598856449127197,
        "y": 2.0381577014923096,
        "z": 1.6054595708847046
      },
      "starmap": {
        "x": 0.009599527344107628,
        "y": -0.006957588251680136,
        "z": 0.0258816946297884
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23508v1",
    "date": "2025-06-30",
    "authors": "Zhihao Zhang, Qiaole Dong, Qi Zhang, Jun Zhao, Enyu Zhou, Zhiheng Xi, Senjie Jin, Xiaoran Fan, Yuhao Zhou, Yanwei Fu, Tao Ji, Tao Gui, Xuanjing Huang",
    "abstract": "Post-training algorithms such as Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning (RFT) are widely used to adapt multimodal large language models to downstream tasks. While effective at task adaptation, their impact on prior knowledge remains unclear. In this paper, we introduce jigsaw puzzles as a novel task absent from existing pretraining corpora and systematically study the behavior of SFT and RFT on an open-source multimodal model, Qwen2.5-VL. Our experiments reveal a sharp trade-off: SFT enables rapid task acquisition but leads to catastrophic forgetting, whereas RFT learns more slowly on novel tasks but maintains prior knowledge. We analyze this phenomenon through the lens of learning dynamics, showing that RFT reinforces correct samples that are naturally aligned with the base model's probability landscape, mitigating interference with prior knowledge. Moreover, supervised training on correct RFT-simulated rollouts allows SFT to preserve knowledge while rapidly learning new tasks. These findings suggest that data distribution, rather than algorithmic differences, plays a central role in forgetting, and highlight RFT's potential for stable continual learning in multimodal large language models.",
    "title": "Reinforcement Fine-Tuning Enables MLLMs Learning Novel Tasks Stably",
    "x": -1.0115615129470825,
    "y": -2.3027806282043457,
    "z": -0.7260631322860718,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": -1.0115615129470825,
        "y": -2.3027806282043457,
        "z": -0.7260631322860718
      },
      "starmap": {
        "x": -0.019561003893613815,
        "y": 0.004595167934894562,
        "z": -0.002733552595600486
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "title": "Artificial Intelligence-assisted Pixel-level Lung (APL) Scoring for Fast and Accurate Quantification in Ultra-short Echo-time MRI",
    "authors": "Bowen Xin, Rohan Hickey, Tamara Blake, Jin Jin, Claire E Wainwright, Thomas Benkert, Alto Stemmer, Peter Sly, David Coman, Jason Dowling",
    "id": "2506.23506v1",
    "abstract": "Lung magnetic resonance imaging (MRI) with ultrashort echo-time (UTE) represents a recent breakthrough in lung structure imaging, providing image resolution and quality comparable to computed tomography (CT). Due to the absence of ionising radiation, MRI is often preferred over CT in paediatric diseases such as cystic fibrosis (CF), one of the most common genetic disorders in Caucasians. To assess structural lung damage in CF imaging, CT scoring systems provide valuable quantitative insights for disease diagnosis and progression. However, few quantitative scoring systems are available in structural lung MRI (e.g., UTE-MRI). To provide fast and accurate quantification in lung MRI, we investigated the feasibility of novel Artificial intelligence-assisted Pixel-level Lung (APL) scoring for CF. APL scoring consists of 5 stages, including 1) image loading, 2) AI lung segmentation, 3) lung-bounded slice sampling, 4) pixel-level annotation, and 5) quantification and reporting. The results shows that our APL scoring took 8.2 minutes per subject, which was more than twice as fast as the previous grid-level scoring. Additionally, our pixel-level scoring was statistically more accurate (p=0.021), while strongly correlating with grid-level scoring (R=0.973, p=5.85e-9). This tool has great potential to streamline the workflow of UTE lung MRI in clinical settings, and be extended to other structural lung MRI sequences (e.g., BLADE MRI), and for other lung diseases (e.g., bronchopulmonary dysplasia).",
    "x": -2.668752431869507,
    "y": -0.9103211164474487,
    "z": -1.9915684461593628,
    "cluster_id": 4,
    "layouts": {
      "pacmap": {
        "x": -2.668752431869507,
        "y": -0.9103211164474487,
        "z": -1.9915684461593628
      },
      "starmap": {
        "x": 0.024793732911348343,
        "y": -0.042283881455659866,
        "z": -0.009156090207397938
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "title": "Improve Underwater Object Detection through YOLOv12 Architecture and Physics-informed Augmentation",
    "abstract": "Underwater object detection is crucial for autonomous navigation, environmental monitoring, and marine exploration, but it is severely hampered by light attenuation, turbidity, and occlusion. Current methods balance accuracy and computational efficiency, but they have trouble deploying in real-time under low visibility conditions. Through the integration of physics-informed augmentation techniques with the YOLOv12 architecture, this study advances underwater detection. With Residual ELAN blocks to preserve structural features in turbid waters and Area Attention to maintain large receptive fields for occluded objects while reducing computational complexity. Underwater optical properties are addressed by domain-specific augmentations such as turbulence adaptive blurring, biologically grounded occlusion simulation, and spectral HSV transformations for color distortion. Extensive tests on four difficult datasets show state-of-the-art performance, with Brackish data registering 98.30% mAP at 142 FPS. YOLOv12 improves occlusion robustness by 18.9%, small-object recall by 22.4%, and detection precision by up to 7.94% compared to previous models. The crucial role of augmentation strategy is validated by ablation studies. This work offers a precise and effective solution for conservation and underwater robotics applications.",
    "id": "2506.23505v1",
    "authors": "Tinh Nguyen",
    "x": -1.1435214281082153,
    "y": -0.36002597212791443,
    "z": -0.2571961581707001,
    "cluster_id": 1,
    "layouts": {
      "pacmap": {
        "x": -1.1435214281082153,
        "y": -0.36002597212791443,
        "z": -0.2571961581707001
      },
      "starmap": {
        "x": 0.04352814704179764,
        "y": 0.05432920530438423,
        "z": -0.019518328830599785
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "authors": "Bosubabu Sambana, Kotamsetty Geethika Devi, Bandi Rajeswara Reddy, Galeti Mohammad Hussain, Gownivalla Siddartha",
    "id": "2506.23504v1",
    "title": "Hybrid Approach for Electricity Price Forecasting using AlexNet and LSTM",
    "abstract": "The recent development of advanced machine learning methods for hybrid models has greatly addressed the need for the correct prediction of electrical prices. This method combines AlexNet and LSTM algorithms, which are used to introduce a new model with higher accuracy in price forecasting. Despite RNN and ANN being effective, they often fail to deal with forex time sequence data. The traditional methods do not accurately forecast the prices. These traditional methods only focus on demand and price which leads to insufficient analysis of data. To address this issue, using the hybrid approach, which focuses on external variables that also effect the predicted prices. Nevertheless, due to AlexNet's excellent feature extraction and LSTM's learning sequential patterns, the prediction accuracy is vastly increased. The model is built on the past data, which has been supplied with the most significant elements like demand, temperature, sunlight, and rain. For example, the model applies methods, such as minimum-maximum scaling and a time window, to predict the electricity prices of the future. The results show that this hybrid model is good than the standalone ones in terms of accuracy. Although we got our accuracy rating of 97.08, it shows higher accompaniments than remaining models RNN and ANN with accuracies of 96.64 and 96.63 respectively.",
    "x": 0.4740983545780182,
    "y": -1.2689541578292847,
    "z": -0.8557320237159729,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": 0.4740983545780182,
        "y": -1.2689541578292847,
        "z": -0.8557320237159729
      },
      "starmap": {
        "x": 0.014073344878852367,
        "y": -0.018017789348959923,
        "z": -0.018100937828421593
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Cognitive Behavioral Therapy (CBT) is a proven approach for addressing the irrational thought patterns associated with mental health disorders, but its effectiveness relies on accurately identifying cognitive pathways to provide targeted treatment. In today's digital age, individuals often express negative emotions on social media, where they may reveal cognitive distortions, and in severe cases, exhibit suicidal tendencies. However, there is a significant gap in methodologies designed to analyze these cognitive pathways, which could be critical for psychotherapists aiming to deliver timely and effective interventions in online environments. Cognitive Behavioral Therapy (CBT) framework leveraging acceptance, commitment and data augmentation to categorize and address both textual and visual content as positive or negative. Specifically, the system employs BERT, RoBERTa for Sentiment Analysis and T5, PEGASUS for Text Summarization, mT5 for Text Translation in Multiple Languages focusing on detecting negative emotions and cognitive distortions within social media data. While existing models are primarily designed to identify negative thoughts, the proposed system goes beyond this by predicting additional negative side effects and other potential mental health disorders likes Phobias, Eating Disorders. This enhancement allows for a more comprehensive understanding and intervention strategy, offering psychotherapists a powerful tool for early detection and treatment of various psychological issues.",
    "date": "2025-06-30",
    "id": "2506.23503v1",
    "title": "Data Augmentation for Cognitive Behavioral Therapy: Leveraging ERNIE Language Models using Artificial Intelligence",
    "authors": "Bosubabu Sambana, Kondreddygari Archana, Suram Indhra Sena Reddy, Shaik Meethaigar Jameer Basha, Shaik Karishma",
    "x": 3.207791566848755,
    "y": 0.12899386882781982,
    "z": -1.1228753328323364,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 3.207791566848755,
        "y": 0.12899386882781982,
        "z": -1.1228753328323364
      },
      "starmap": {
        "x": -0.019220056012272835,
        "y": 0.003300231648609042,
        "z": -0.0015232979785650969
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Mengxiao Tian, Xinxiao Wu, Shuo Yang",
    "title": "LLM-enhanced Action-aware Multi-modal Prompt Tuning for Image-Text Matching",
    "abstract": "Driven by large-scale contrastive vision-language pre-trained models such as CLIP, recent advancements in the image-text matching task have achieved remarkable success in representation learning. Due to image-level visual-language alignment, CLIP falls short in understanding fine-grained details such as object attributes and spatial relationships between objects. Recent efforts have attempted to compel CLIP to acquire structured visual representations by introducing prompt learning to achieve object-level alignment. While achieving promising results, they still lack the capability to perceive actions, which are crucial for describing the states or relationships between objects. Therefore, we propose to endow CLIP with fine-grained action-level understanding by introducing an LLM-enhanced action-aware multi-modal prompt-tuning method, incorporating the action-related external knowledge generated by large language models (LLMs). Specifically, we design an action triplet prompt and an action state prompt to exploit compositional semantic knowledge and state-related causal knowledge implicitly stored in LLMs. Subsequently, we propose an adaptive interaction module to aggregate attentive visual features conditioned on action-aware prompted knowledge for establishing discriminative and action-aware visual representations, which further improves the performance. Comprehensive experimental results on two benchmark datasets demonstrate the effectiveness of our method.",
    "date": "2025-06-30",
    "id": "2506.23502v1",
    "x": -0.8797833919525146,
    "y": 0.9724050164222717,
    "z": -0.2586265504360199,
    "cluster_id": 3,
    "layouts": {
      "pacmap": {
        "x": -0.8797833919525146,
        "y": 0.9724050164222717,
        "z": -0.2586265504360199
      },
      "starmap": {
        "x": 0.016714617609977722,
        "y": -0.03857855498790741,
        "z": -0.023579023778438568
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "abstract": "Recent advances in deep learning have significantly improved predictive accuracy. However, modern neural networks remain systematically overconfident, posing risks for deployment in safety-critical scenarios. Current post-hoc calibration methods face a fundamental dilemma: global approaches like Temperature Scaling apply uniform adjustments across all samples, introducing high bias despite computational efficiency, while more expressive methods that operate on full logit distributions suffer from high variance due to noisy high-dimensional inputs and insufficient validation data. To address these challenges, we propose Sample Margin-Aware Recalibration of Temperature (SMART), a lightweight, data-efficient recalibration method that precisely scales logits based on the margin between the top two logits -- termed the logit gap. Specifically, the logit gap serves as a denoised, scalar signal directly tied to decision boundary uncertainty, providing a robust indicator that avoids the noise inherent in high-dimensional logit spaces while preserving model prediction invariance. Meanwhile, SMART employs a novel soft-binned Expected Calibration Error (SoftECE) objective that balances model bias and variance through adaptive binning, enabling stable parameter updates even with extremely limited calibration data. Extensive evaluations across diverse datasets and architectures demonstrate that SMART achieves state-of-the-art calibration performance even with substantially fewer parameters compared to existing parametric methods, offering a principled, robust, and highly efficient solution for practical uncertainty quantification in neural network predictions. The source code is available at: https://anonymous.4open.science/r/SMART-8B11.",
    "id": "2506.23492v1",
    "authors": "Haolan Guo, Linwei Tao, Haoyang Luo, Minjing Dong, Chang Xu",
    "title": "Sample Margin-Aware Recalibration of Temperature Scaling",
    "x": 1.131514310836792,
    "y": -1.1886192560195923,
    "z": -0.7237304449081421,
    "cluster_id": 6,
    "layouts": {
      "pacmap": {
        "x": 1.131514310836792,
        "y": -1.1886192560195923,
        "z": -0.7237304449081421
      },
      "starmap": {
        "x": 0.01235067006200552,
        "y": -0.0025757148396223783,
        "z": -0.018239179626107216
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "id": "2506.23491v2",
    "title": "ZonUI-3B: A Lightweight Vision-Language Model for Cross-Resolution GUI Grounding",
    "authors": "ZongHan Hsieh, Tzer-Jen Wei, ShengJing Yang",
    "abstract": "This paper introduces ZonUI-3B, a lightweight Vision-Language Model (VLM) specifically designed for Graphical User Interface grounding tasks, achieving performance competitive with significantly larger models. Unlike large-scale VLMs (>7B parameters) that are computationally intensive and impractical for consumer-grade hardware, ZonUI-3B delivers strong grounding accuracy while being fully trainable on a single GPU (RTX 4090). The model incorporates several key innovations: (i) combine cross-platform, multi-resolution dataset of 24K examples from diverse sources including mobile, desktop, and web GUI screenshots to effectively address data scarcity in high-resolution desktop environments; (ii) a two-stage fine-tuning strategy, where initial cross-platform training establishes robust GUI understanding, followed by specialized fine-tuning on high-resolution data to significantly enhance model adaptability; and (iii) data curation and redundancy reduction strategies, demonstrating that randomly sampling a smaller subset with reduced redundancy achieves performance comparable to larger datasets, emphasizing data diversity over sheer volume. Empirical evaluation on standard GUI grounding benchmarks-including ScreenSpot, ScreenSpot-v2, and the challenging ScreenSpot-Pro, highlights ZonUI-3B's exceptional accuracy, achieving 84.9% on ScreenSpot and 86.4% on ScreenSpot-v2, surpassing prior models under 4B parameters. Ablation studies validate the critical role of balanced sampling and two-stage fine-tuning in enhancing robustness, particularly in high-resolution desktop scenarios. The ZonUI-3B is available at: https://github.com/Han1018/ZonUI-3B",
    "x": -3.8134260177612305,
    "y": 0.11132824420928955,
    "z": 0.26011204719543457,
    "cluster_id": 3,
    "layouts": {
      "pacmap": {
        "x": -3.8134260177612305,
        "y": 0.11132824420928955,
        "z": 0.26011204719543457
      },
      "starmap": {
        "x": 0.012233537621796131,
        "y": -0.013752421364188194,
        "z": 0.009907611645758152
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Echocardiography is routine for cardiac examination. However, 2D ultrasound (US) struggles with accurate metric calculation and direct observation of 3D cardiac structures. Moreover, 3D US is limited by low resolution, small field of view and scarce availability in practice. Constructing the cardiac anatomical twin from 2D images is promising to provide precise treatment planning and clinical quantification. However, it remains challenging due to the rare paired data, complex structures, and US noises. In this study, we introduce a novel generative framework UltraTwin, to obtain cardiac anatomical twin from sparse multi-view 2D US. Our contribution is three-fold. First, pioneered the construction of a real-world and high-quality dataset containing strictly paired multi-view 2D US and CT, and pseudo-paired data. Second, we propose a coarse-to-fine scheme to achieve hierarchical reconstruction optimization. Last, we introduce an implicit autoencoder for topology-aware constraints. Extensive experiments show that UltraTwin reconstructs high-quality anatomical twins versus strong competitors. We believe it advances anatomical twin modeling for potential applications in personalized cardiac care.",
    "title": "UltraTwin: Towards Cardiac Anatomical Twin Generation from Multi-view 2D Ultrasound",
    "id": "2506.23490v1",
    "authors": "Junxuan Yu, Yaofei Duan, Yuhao Huang, Yu Wang, Rongbo Ling, Weihao Luo, Ang Zhang, Jingxian Xu, Qiongying Ni, Yongsong Zhou, Binghan Li, Haoran Dou, Liping Liu, Yanfen Chu, Feng Geng, Zhe Sheng, Zhifeng Ding, Dingxin Zhang, Rui Huang, Yuhang Zhang, Xiaowei Xu, Tao Tan, Dong Ni, Zhongshan Gou, Xin Yang",
    "date": "2025-06-30",
    "x": -2.642003297805786,
    "y": 1.657482624053955,
    "z": -1.363563060760498,
    "cluster_id": 4,
    "layouts": {
      "pacmap": {
        "x": -2.642003297805786,
        "y": 1.657482624053955,
        "z": -1.363563060760498
      },
      "starmap": {
        "x": 0.026441186666488647,
        "y": -0.030590767040848732,
        "z": -0.003021987620741129
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "abstract": "We propose a novel test for assessing partial effects in Frechet regression on Bures Wasserstein manifolds. Our approach employs a sample splitting strategy: the first subsample is used to fit the Frechet regression model, yielding estimates of the covariance matrices and their associated optimal transport maps, while the second subsample is used to construct the test statistic. We prove that this statistic converges in distribution to a weighted mixture of chi squared components, where the weights correspond to the eigenvalues of an integral operator defined by an appropriate RKHS kernel. We establish that our procedure achieves the nominal asymptotic size and demonstrate that its worst-case power converges uniformly to one. Through extensive simulations and a real data application, we illustrate the test's finite-sample accuracy and practical utility.",
    "title": "Test of partial effects for Frechet regression on Bures-Wasserstein manifolds",
    "authors": "Haoshu Xu, Hongzhe Li",
    "id": "2506.23487v1",
    "x": 0.7722606658935547,
    "y": -2.815670967102051,
    "z": -0.27430295944213867,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": 0.7722606658935547,
        "y": -2.815670967102051,
        "z": -0.27430295944213867
      },
      "starmap": {
        "x": 0.011109629645943642,
        "y": -0.013607950881123543,
        "z": -0.011664439924061298
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Haocheng Yu, Yaxiong Wu, Hao Wang, Wei Guo, Yong Liu, Yawen Li, Yuyang Ye, Junping Du, Enhong Chen",
    "abstract": "Interactive recommendation is a typical information-seeking task that allows users to interactively express their needs through natural language and obtain personalized recommendations. Large language model-powered (LLM-powered) agents have become a new paradigm in interactive recommendations, effectively capturing users' real-time needs and enhancing personalized experiences. However, due to limited planning and generalization capabilities, existing formulations of LLM-powered interactive recommender agents struggle to effectively address diverse and complex user intents, such as intuitive, unrefined, or occasionally ambiguous requests. To tackle this challenge, we propose a novel thought-augmented interactive recommender agent system (TAIRA) that addresses complex user intents through distilled thought patterns. Specifically, TAIRA is designed as an LLM-powered multi-agent system featuring a manager agent that orchestrates recommendation tasks by decomposing user needs and planning subtasks, with its planning capacity strengthened through Thought Pattern Distillation (TPD), a thought-augmentation method that extracts high-level thoughts from the agent's and human experts' experiences. Moreover, we designed a set of user simulation schemes to generate personalized queries of different difficulties and evaluate the recommendations based on specific datasets. Through comprehensive experiments conducted across multiple datasets, TAIRA exhibits significantly enhanced performance compared to existing methods. Notably, TAIRA shows a greater advantage on more challenging tasks while generalizing effectively on novel tasks, further validating its superiority in managing complex user intents within interactive recommendation systems. The code is publicly available at:https://github.com/Alcein/TAIRA.",
    "date": "2025-06-30",
    "id": "2506.23485v1",
    "title": "Thought-Augmented Planning for LLM-Powered Interactive Recommender Agent",
    "x": 2.0642945766448975,
    "y": 0.9034169912338257,
    "z": 1.8747916221618652,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 2.0642945766448975,
        "y": 0.9034169912338257,
        "z": 1.8747916221618652
      },
      "starmap": {
        "x": -0.023659037426114082,
        "y": 0.010699711740016937,
        "z": 0.0027842698618769646
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "AI-generated content (AIGC) enables efficient visual creation but raises copyright and authenticity risks. As a common technique for integrity verification and source tracing, digital image watermarking is regarded as a potential solution to above issues. Among these, watermarking methods capable of preserving the generation quality are receiving increased attention. However, the proliferation and high performance of generative image editing applications have elevated the risks of malicious tampering, creating new demands. 1) The tamper robustness of current lossless visual quality watermarks remains constrained by the modification-sensitive diffusion inversion process, necessitating enhanced robustness. 2) The improved tampering quality and rapid iteration cycles render passive tampering detection methods inadequate, making proactive tampering localization capability a desired feature for watermarks. To address these requirements, this paper proposes a Tamper-Aware Generative image WaterMarking method named TAG-WM. The proposed method comprises four key modules: a dual-mark joint sampling (DMJS) algorithm for embedding copyright and localization watermarks into the latent space while preserving generative quality, the watermark latent reconstruction (WLR) utilizing reversed DMJS, a dense variation region detector (DVRD) leveraging diffusion inversion sensitivity to identify tampered areas via statistical deviation analysis, and the tamper-aware decoding (TAD) guided by localization results. The experimental results indicate that TAG-WM achieves SOTA tampering robustness and tampering localization capability with distortions while maintaining lossless generation quality and a considerable capacity of 256 bits.",
    "title": "TAG-WM: Tamper-Aware Generative Image Watermarking via Diffusion Inversion Sensitivity",
    "date": "2025-06-30",
    "authors": "Yuzhuo Chen, Zehua Ma, Han Fang, Weiming Zhang, Nenghai Yu",
    "id": "2506.23484v1",
    "x": -3.4504151344299316,
    "y": 1.0820871591567993,
    "z": -0.8541868925094604,
    "cluster_id": 2,
    "layouts": {
      "pacmap": {
        "x": -3.4504151344299316,
        "y": 1.0820871591567993,
        "z": -0.8541868925094604
      },
      "starmap": {
        "x": 0.023061560466885567,
        "y": -0.0001637625537114218,
        "z": 0.08121512830257416
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "MTADiffusion: Mask Text Alignment Diffusion Model for Object Inpainting",
    "authors": "Jun Huang, Ting Liu, Yihang Wu, Xiaochao Qu, Luoqi Liu, Xiaolin Hu",
    "date": "2025-06-30",
    "id": "2506.23482v1",
    "abstract": "Advancements in generative models have enabled image inpainting models to generate content within specific regions of an image based on provided prompts and masks. However, existing inpainting methods often suffer from problems such as semantic misalignment, structural distortion, and style inconsistency. In this work, we present MTADiffusion, a Mask-Text Alignment diffusion model designed for object inpainting. To enhance the semantic capabilities of the inpainting model, we introduce MTAPipeline, an automatic solution for annotating masks with detailed descriptions. Based on the MTAPipeline, we construct a new MTADataset comprising 5 million images and 25 million mask-text pairs. Furthermore, we propose a multi-task training strategy that integrates both inpainting and edge prediction tasks to improve structural stability. To promote style consistency, we present a novel inpainting style-consistency loss using a pre-trained VGG network and the Gram matrix. Comprehensive evaluations on BrushBench and EditBench demonstrate that MTADiffusion achieves state-of-the-art performance compared to other methods.",
    "x": -3.0598559379577637,
    "y": 0.7552711963653564,
    "z": -0.439674437046051,
    "cluster_id": 2,
    "layouts": {
      "pacmap": {
        "x": -3.0598559379577637,
        "y": 0.7552711963653564,
        "z": -0.439674437046051
      },
      "starmap": {
        "x": 0.02586193196475506,
        "y": -0.0011929370230063796,
        "z": 0.07574371993541718
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Objectives: The rapid advancement of Multimodal Large Language Models (MLLMs) has significantly enhanced their reasoning capabilities, enabling a wide range of intelligent applications. However, these advancements also raise critical concerns regarding privacy and ethics. MLLMs are now capable of inferring the geographic location of images -- such as those shared on social media or captured from street views -- based solely on visual content, thereby posing serious risks of privacy invasion, including doxxing, surveillance, and other security threats.   Methods: This study provides a comprehensive analysis of existing geolocation techniques based on MLLMs. It systematically reviews relevant litera-ture and evaluates the performance of state-of-the-art visual reasoning models on geolocation tasks, particularly in identifying the origins of street view imagery.   Results: Empirical evaluation reveals that the most advanced visual large models can successfully localize the origin of street-level imagery with up to $49\\%$ accuracy within a 1-kilometer radius. This performance underscores the models' powerful capacity to extract and utilize fine-grained geographic cues from visual data.   Conclusions: Building on these findings, the study identifies key visual elements that contribute to suc-cessful geolocation, such as text, architectural styles, and environmental features. Furthermore, it discusses the potential privacy implications associated with MLLM-enabled geolocation and discuss several technical and policy-based coun-termeasures to mitigate associated risks. Our code and dataset are available at https://github.com/zxyl1003/MLLM-Geolocation-Evaluation.",
    "title": "Evaluation of Geolocation Capabilities of Multimodal Large Language Models and Analysis of Associated Privacy Risks",
    "date": "2025-06-30",
    "authors": "Xian Zhang, Xiang Cheng",
    "id": "2506.23481v1",
    "x": 0.48865270614624023,
    "y": 1.4641239643096924,
    "z": 0.3657921254634857,
    "cluster_id": 3,
    "layouts": {
      "pacmap": {
        "x": 0.48865270614624023,
        "y": 1.4641239643096924,
        "z": 0.3657921254634857
      },
      "starmap": {
        "x": 0.015003075823187828,
        "y": -0.0344722718000412,
        "z": -0.022693771868944168
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "Instant GaussianImage: A Generalizable and Self-Adaptive Image Representation via 2D Gaussian Splatting",
    "authors": "Zhaojie Zeng, Yuesong Wang, Chao Yang, Tao Guan, Lili Ju",
    "date": "2025-06-30",
    "id": "2506.23479v1",
    "abstract": "Implicit Neural Representation (INR) has demonstrated remarkable advances in the field of image representation but demands substantial GPU resources. GaussianImage recently pioneered the use of Gaussian Splatting to mitigate this cost, however, the slow training process limits its practicality, and the fixed number of Gaussians per image limits its adaptability to varying information entropy. To address these issues, we propose in this paper a generalizable and self-adaptive image representation framework based on 2D Gaussian Splatting. Our method employs a network to quickly generate a coarse Gaussian representation, followed by minimal fine-tuning steps, achieving comparable rendering quality of GaussianImage while significantly reducing training time. Moreover, our approach dynamically adjusts the number of Gaussian points based on image complexity to further enhance flexibility and efficiency in practice. Experiments on DIV2K and Kodak datasets show that our method matches or exceeds GaussianImage's rendering performance with far fewer iterations and shorter training times. Specifically, our method reduces the training time by up to one order of magnitude while achieving superior rendering performance with the same number of Gaussians.",
    "x": -2.799851179122925,
    "y": 1.8946473598480225,
    "z": 0.22710269689559937,
    "cluster_id": 1,
    "layouts": {
      "pacmap": {
        "x": -2.799851179122925,
        "y": 1.8946473598480225,
        "z": 0.22710269689559937
      },
      "starmap": {
        "x": 0.04382985085248947,
        "y": 0.05770459771156311,
        "z": -0.01898994855582714
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Chamfer Distance (CD) is a widely adopted metric in 3D point cloud learning due to its simplicity and efficiency. However, it suffers from a fundamental limitation: it relies solely on Euclidean distances, which often fail to capture the intrinsic geometry of 3D shapes. To address this limitation, we propose GeoCD, a topology-aware and fully differentiable approximation of geodesic distance designed to serve as a metric for 3D point cloud learning. Our experiments show that GeoCD consistently improves reconstruction quality over standard CD across various architectures and datasets. We demonstrate this by fine-tuning several models, initially trained with standard CD, using GeoCD. Remarkably, fine-tuning for a single epoch with GeoCD yields significant gains across multiple evaluation metrics.",
    "title": "GeoCD: A Differential Local Approximation for Geodesic Chamfer Distance",
    "authors": "Pedro Alonso, Tianrui Li, Chongshou Li",
    "id": "2506.23478v1",
    "date": "2025-06-30",
    "x": -2.2115750312805176,
    "y": 3.0242013931274414,
    "z": -0.5156064033508301,
    "cluster_id": 1,
    "layouts": {
      "pacmap": {
        "x": -2.2115750312805176,
        "y": 3.0242013931274414,
        "z": -0.5156064033508301
      },
      "starmap": {
        "x": 0.05217340216040611,
        "y": 0.07801457494497299,
        "z": -0.029876738786697388
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23471v1",
    "authors": "Thanh-Tung Phan-Nguyen, Khoi-Nguyen Nguyen-Ngoc, Tam V. Nguyen, Minh-Triet Tran, Trung-Nghia Le",
    "date": "2025-06-30",
    "title": "KiseKloset: Comprehensive System For Outfit Retrieval, Recommendation, And Try-On",
    "abstract": "The global fashion e-commerce industry has become integral to people's daily lives, leveraging technological advancements to offer personalized shopping experiences, primarily through recommendation systems that enhance customer engagement through personalized suggestions. To improve customers' experience in online shopping, we propose a novel comprehensive KiseKloset system for outfit retrieval, recommendation, and try-on. We explore two approaches for outfit retrieval: similar item retrieval and text feedback-guided item retrieval. Notably, we introduce a novel transformer architecture designed to recommend complementary items from diverse categories. Furthermore, we enhance the overall performance of the search pipeline by integrating approximate algorithms to optimize the search process. Additionally, addressing the crucial needs of online shoppers, we employ a lightweight yet efficient virtual try-on framework capable of real-time operation, memory efficiency, and maintaining realistic outputs compared to its predecessors. This virtual try-on module empowers users to visualize specific garments on themselves, enhancing the customers' experience and reducing costs associated with damaged items for retailers. We deployed our end-to-end system for online users to test and provide feedback, enabling us to measure their satisfaction levels. The results of our user study revealed that 84% of participants found our comprehensive system highly useful, significantly improving their online shopping experience.",
    "x": 1.222665548324585,
    "y": 1.5379780530929565,
    "z": 1.0151677131652832,
    "cluster_id": 2,
    "layouts": {
      "pacmap": {
        "x": 1.222665548324585,
        "y": 1.5379780530929565,
        "z": 1.0151677131652832
      },
      "starmap": {
        "x": -0.0019313786178827286,
        "y": 0.00851389765739441,
        "z": 0.04070368409156799
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Ngoc-Do Tran, Minh-Tuan Huynh, Tam V. Nguyen, Minh-Triet Tran, Trung-Nghia Le",
    "title": "Interactive Interface For Semantic Segmentation Dataset Synthesis",
    "date": "2025-06-30",
    "id": "2506.23470v1",
    "abstract": "The rapid advancement of AI and computer vision has significantly increased the demand for high-quality annotated datasets, particularly for semantic segmentation. However, creating such datasets is resource-intensive, requiring substantial time, labor, and financial investment, and often raises privacy concerns due to the use of real-world data. To mitigate these challenges, we present SynthLab, consisting of a modular platform for visual data synthesis and a user-friendly interface. The modular architecture of SynthLab enables easy maintenance, scalability with centralized updates, and seamless integration of new features. Each module handles distinct aspects of computer vision tasks, enhancing flexibility and adaptability. Meanwhile, its interactive, user-friendly interface allows users to quickly customize their data pipelines through drag-and-drop actions. Extensive user studies involving a diverse range of users across different ages, professions, and expertise levels, have demonstrated flexible usage, and high accessibility of SynthLab, enabling users without deep technical expertise to harness AI for real-world applications.",
    "x": 1.4325453042984009,
    "y": 0.99758380651474,
    "z": 0.4460267722606659,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": 1.4325453042984009,
        "y": 0.99758380651474,
        "z": 0.4460267722606659
      },
      "starmap": {
        "x": -0.013402003794908524,
        "y": 0.00835529062896967,
        "z": 0.018008042126893997
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Graph anomaly detection is critical in domains such as healthcare and economics, where identifying deviations can prevent substantial losses. Existing unsupervised approaches strive to learn a single model capable of detecting both attribute and structural anomalies. However, they confront the tug-of-war problem between two distinct types of anomalies, resulting in suboptimal performance. This work presents TripleAD, a mutual distillation-based triple-channel graph anomaly detection framework. It includes three estimation modules to identify the attribute, structural, and mixed anomalies while mitigating the interference between different types of anomalies. In the first channel, we design a multiscale attribute estimation module to capture extensive node interactions and ameliorate the over-smoothing issue. To better identify structural anomalies, we introduce a link-enhanced structure estimation module in the second channel that facilitates information flow to topologically isolated nodes. The third channel is powered by an attribute-mixed curvature, a new indicator that encapsulates both attribute and structural information for discriminating mixed anomalies. Moreover, a mutual distillation strategy is introduced to encourage communication and collaboration between the three channels. Extensive experiments demonstrate the effectiveness of the proposed TripleAD model against strong baselines.",
    "authors": "Chunjing Xiao, Jiahui Lu, Xovee Xu, Fan Zhou, Tianshu Xie, Wei Lu, Lifeng Xu",
    "title": "Reconciling Attribute and Structural Anomalies for Improved Graph Anomaly Detection",
    "id": "2506.23469v1",
    "date": "2025-06-30",
    "x": -0.47217288613319397,
    "y": -0.5800300240516663,
    "z": -2.2519750595092773,
    "cluster_id": 6,
    "layouts": {
      "pacmap": {
        "x": -0.47217288613319397,
        "y": -0.5800300240516663,
        "z": -2.2519750595092773
      },
      "starmap": {
        "x": 0.015566329471766949,
        "y": -0.017449721693992615,
        "z": -0.027779320254921913
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23468v1",
    "title": "NavMorph: A Self-Evolving World Model for Vision-and-Language Navigation in Continuous Environments",
    "date": "2025-06-30",
    "authors": "Xuan Yao, Junyu Gao, Changsheng Xu",
    "abstract": "Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires agents to execute sequential navigation actions in complex environments guided by natural language instructions. Current approaches often struggle with generalizing to novel environments and adapting to ongoing changes during navigation. Inspired by human cognition, we present NavMorph, a self-evolving world model framework that enhances environmental understanding and decision-making in VLN-CE tasks. NavMorph employs compact latent representations to model environmental dynamics, equipping agents with foresight for adaptive planning and policy refinement. By integrating a novel Contextual Evolution Memory, NavMorph leverages scene-contextual information to support effective navigation while maintaining online adaptability. Extensive experiments demonstrate that our method achieves notable performance improvements on popular VLN-CE benchmarks. Code is available at \\href{https://github.com/Feliciaxyao/NavMorph}{this https URL}.",
    "x": 1.7316977977752686,
    "y": 0.7445942759513855,
    "z": 2.078622341156006,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 1.7316977977752686,
        "y": 0.7445942759513855,
        "z": 2.078622341156006
      },
      "starmap": {
        "x": -0.021614044904708862,
        "y": 0.008291562087833881,
        "z": 0.0002747630060184747
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Contrastive Language-Image Pre-training (CLIP) models have demonstrated superior performance across various visual tasks including medical image classification. However, fairness concerns, including demographic biases, have received limited attention for CLIP models. This oversight leads to critical issues, particularly those related to race and gender, resulting in disparities in diagnostic outcomes and reduced reliability for underrepresented groups. To address these challenges, we introduce AdFair-CLIP, a novel framework employing adversarial feature intervention to suppress sensitive attributes, thereby mitigating spurious correlations and improving prediction fairness. We conduct comprehensive experiments on chest X-ray (CXR) datasets, and show that AdFair-CLIP significantly enhances both fairness and diagnostic accuracy, while maintaining robust generalization in zero-shot and few-shot scenarios. These results establish new benchmarks for fairness-aware learning in CLIP-based medical diagnostic models, particularly for CXR analysis.",
    "authors": "Chenlang Yi, Zizhan Xiong, Qi Qi, Xiyuan Wei, Girish Bathla, Ching-Long Lin, Bobak Jack Mortazavi, Tianbao Yang",
    "id": "2506.23467v1",
    "date": "2025-06-30",
    "title": "AdFair-CLIP: Adversarial Fair Contrastive Language-Image Pre-training for Chest X-rays",
    "x": -0.7092911601066589,
    "y": 0.9402996897697449,
    "z": -1.4746124744415283,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -0.7092911601066589,
        "y": 0.9402996897697449,
        "z": -1.4746124744415283
      },
      "starmap": {
        "x": 0.0038605614099651575,
        "y": -0.017719319090247154,
        "z": -0.016697874292731285
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "authors": "Qiqing Liu, Guoquan Wei, Zekun Zhou, Yiyang Wen, Liu Shi, Qiegen Liu",
    "abstract": "Low-dose computed tomography (LDCT) reduces radiation exposure but suffers from image artifacts and loss of detail due to quantum and electronic noise, potentially impacting diagnostic accuracy. Transformer combined with diffusion models has been a promising approach for image generation. Nevertheless, existing methods exhibit limitations in preserving finegrained image details. To address this issue, frequency domain-directed diffusion transformer (FD-DiT) is proposed for LDCT reconstruction. FD-DiT centers on a diffusion strategy that progressively introduces noise until the distribution statistically aligns with that of LDCT data, followed by denoising processing. Furthermore, we employ a frequency decoupling technique to concentrate noise primarily in high-frequency domain, thereby facilitating effective capture of essential anatomical structures and fine details. A hybrid denoising network is then utilized to optimize the overall data reconstruction process. To enhance the capability in recognizing high-frequency noise, we incorporate sliding sparse local attention to leverage the sparsity and locality of shallow-layer information, propagating them via skip connections for improving feature representation. Finally, we propose a learnable dynamic fusion strategy for optimal component integration. Experimental results demonstrate that at identical dose levels, LDCT images reconstructed by FD-DiT exhibit superior noise and artifact suppression compared to state-of-the-art methods.",
    "title": "FD-DiT: Frequency Domain-Directed Diffusion Transformer for Low-Dose CT Reconstruction",
    "id": "2506.23466v1",
    "x": -3.5096688270568848,
    "y": -0.008767891675233841,
    "z": 0.6939359307289124,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -3.5096688270568848,
        "y": -0.008767891675233841,
        "z": 0.6939359307289124
      },
      "starmap": {
        "x": 0.02874911203980446,
        "y": 0.007516121491789818,
        "z": 0.014498280361294746
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Nazanin Mahjourian, Vinh Nguyen",
    "date": "2025-06-30",
    "id": "2506.23465v1",
    "abstract": "The success of machine learning models in industrial applications is heavily dependent on the quality of the datasets used to train the models. However, large-scale datasets, specially those constructed from crowd-sourcing and web-scraping, often suffer from label noise, inconsistencies, and errors. This problem is particularly pronounced in manufacturing domains, where obtaining high-quality labels is costly and time-consuming. This paper introduces Vision-Language Sanitization and Refinement (VLSR), which is a vision-language-based framework for label sanitization and refinement in multi-label manufacturing image datasets. This method embeds both images and their associated textual labels into a shared semantic space leveraging the CLIP vision-language model. Then two key tasks are addressed in this process by computing the cosine similarity between embeddings. First, label sanitization is performed to identify irrelevant, misspelled, or semantically weak labels, and surface the most semantically aligned label for each image by comparing image-label pairs using cosine similarity between image and label embeddings. Second, the method applies density-based clustering on text embeddings, followed by iterative cluster merging, to group semantically similar labels into unified label groups. The Factorynet dataset, which includes noisy labels from both human annotations and web-scraped sources, is employed to evaluate the effectiveness of the proposed framework. Experimental results demonstrate that the VLSR framework successfully identifies problematic labels and improves label consistency. This method enables a significant reduction in label vocabulary through clustering, which ultimately enhances the dataset's quality for training robust machine learning models in industrial applications with minimal human intervention.",
    "title": "Sanitizing Manufacturing Dataset Labels Using Vision-Language Models",
    "x": -1.3856337070465088,
    "y": -2.6772167682647705,
    "z": -0.28870540857315063,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -1.3856337070465088,
        "y": -2.6772167682647705,
        "z": -0.28870540857315063
      },
      "starmap": {
        "x": 0.013424120843410492,
        "y": -0.014563838951289654,
        "z": -0.015388892963528633
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Sahil Tripathi, Md Tabrez Nafis, Imran Hussain, Jiechao Gao",
    "id": "2506.23464v1",
    "title": "The Confidence Paradox: Can LLM Know When It's Wrong",
    "abstract": "Document Visual Question Answering (DocVQA) systems are increasingly deployed in real world applications, yet they remain ethically opaque-often producing overconfident answers to ambiguous questions or failing to communicate uncertainty in a trustworthy manner. This misalignment between model confidence and actual knowledge poses significant risks, particularly in domains requiring ethical accountability. Existing approaches such as LayoutLMv3, UDOP, and DONUT have advanced SOTA performance by focusing on architectural sophistication and accuracy; however, they fall short in ethical responsiveness.   To address these limitations, we introduce HonestVQA, a self-supervised honesty calibration framework for ethically aligned DocVQA. Our model-agnostic method quantifies uncertainty to identify knowledge gaps, aligns model confidence with actual correctness using weighted loss functions, and enforces ethical response behavior via contrastive learning. We further introduce two principled evaluation metrics--Honesty Score (H-Score) and Ethical Confidence Index (ECI)--to benchmark alignment between confidence, accuracy, and ethical communication. Empirically, HonestVQA improves DocVQA accuracy by up to 4.3% and F1 by 4.3% across SpDocVQA, InfographicsVQA, and SROIE datasets. It reduces overconfidence, lowering H-Score and ECI by 0.072 and 0.078, respectively. In cross domain evaluation, it achieves up to 78.9% accuracy and 76.1% F1-score, demonstrating strong generalization. Ablation shows a 3.8% drop in accuracy without alignment or contrastive loss.",
    "date": "2025-06-30",
    "x": 1.4396077394485474,
    "y": -2.226977586746216,
    "z": -1.2172480821609497,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": 1.4396077394485474,
        "y": -2.226977586746216,
        "z": -1.2172480821609497
      },
      "starmap": {
        "x": 0.010113051161170006,
        "y": -0.01392933540046215,
        "z": -0.0182478167116642
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Large language models (LLMs) for table-based reasoning often struggle with large tables due to input length limits. We propose ATF (Adaptive Table Filtering Framework), a modular and question-aware filtering pipeline that prunes uninformative columns and rows using LLM-generated column descriptions, clustering, and sparse-dense alignment scores. ATF integrates seamlessly with existing models (e.g., TAPAS, TAPEX) without retraining. Experiments show that ATF reduces table cells by ~70\\%, boosting performance on out-of-domain TableQA tasks while causing slight performance drops on Table Fact Verification, where full-table context is more critical. These results highlight ATF's ability to adaptively balance informativeness and minimalism across tasks.",
    "id": "2506.23463v1",
    "date": "2025-06-30",
    "title": "What to Keep and What to Drop: Adaptive Table Filtering Framework",
    "authors": "Jang Won June",
    "x": -1.524577260017395,
    "y": -3.3779754638671875,
    "z": -0.7055651545524597,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": -1.524577260017395,
        "y": -3.3779754638671875,
        "z": -0.7055651545524597
      },
      "starmap": {
        "x": -0.020658619701862335,
        "y": 0.007042881567031145,
        "z": -0.0029337559826672077
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "title": "Can We Predict the Unpredictable? Leveraging DisasterNet-LLM for Multimodal Disaster Classification",
    "abstract": "Effective disaster management requires timely and accurate insights, yet traditional methods struggle to integrate multimodal data such as images, weather records, and textual reports. To address this, we propose DisasterNet-LLM, a specialized Large Language Model (LLM) designed for comprehensive disaster analysis. By leveraging advanced pretraining, cross-modal attention mechanisms, and adaptive transformers, DisasterNet-LLM excels in disaster classification. Experimental results demonstrate its superiority over state-of-the-art models, achieving higher accuracy of 89.5%, an F1 score of 88.0%, AUC of 0.92%, and BERTScore of 0.88% in multimodal disaster classification tasks.",
    "authors": "Manaswi Kulahara, Gautam Siddharth Kashyap, Nipun Joshi, Arpita Soni",
    "id": "2506.23462v1",
    "x": 2.279081344604492,
    "y": 0.5867558717727661,
    "z": 1.1237412691116333,
    "cluster_id": 3,
    "layouts": {
      "pacmap": {
        "x": 2.279081344604492,
        "y": 0.5867558717727661,
        "z": 1.1237412691116333
      },
      "starmap": {
        "x": 0.012594196014106274,
        "y": -0.03012973628938198,
        "z": -0.014955243095755577
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "abstract": "In this work, we focus on a novel and practical task, i.e., Time-vAriant iMage inPainting (TAMP). The aim of TAMP is to restore a damaged target image by leveraging the complementary information from a reference image, where both images captured the same scene but with a significant time gap in between, i.e., time-variant images. Different from conventional reference-guided image inpainting, the reference image under TAMP setup presents significant content distinction to the target image and potentially also suffers from damages. Such an application frequently happens in our daily lives to restore a damaged image by referring to another reference image, where there is no guarantee of the reference image's source and quality. In particular, our study finds that even state-of-the-art (SOTA) reference-guided image inpainting methods fail to achieve plausible results due to the chaotic image complementation. To address such an ill-posed problem, we propose a novel Interactive Distribution Transition Estimation (InDiTE) module which interactively complements the time-variant images with adaptive semantics thus facilitate the restoration of damaged regions. To further boost the performance, we propose our TAMP solution, namely Interactive Distribution Transition Estimation-driven Diffusion (InDiTE-Diff), which integrates InDiTE with SOTA diffusion model and conducts latent cross-reference during sampling. Moreover, considering the lack of benchmarks for TAMP task, we newly assembled a dataset, i.e., TAMP-Street, based on existing image and mask datasets. We conduct experiments on the TAMP-Street datasets under two different time-variant image inpainting settings, which show our method consistently outperform SOTA reference-guided image inpainting methods for solving TAMP.",
    "title": "Time-variant Image Inpainting via Interactive Distribution Transition Estimation",
    "authors": "Yun Xing, Qing Guo, Xiaoguang Li, Yihao Huang, Xiaofeng Cao, Di Lin, Ivor Tsang, Lei Ma",
    "id": "2506.23461v1",
    "x": -3.4166667461395264,
    "y": 0.42651742696762085,
    "z": -0.23682869970798492,
    "cluster_id": 2,
    "layouts": {
      "pacmap": {
        "x": -3.4166667461395264,
        "y": 0.42651742696762085,
        "z": -0.23682869970798492
      },
      "starmap": {
        "x": 0.024193642660975456,
        "y": -0.00145830144174397,
        "z": 0.07515627145767212
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Dewen Zeng, Xinrong Hu, Yu-Jen Chen, Yawen Wu, Xiaowei Xu, Yiyu Shi",
    "title": "Contrastive Learning with Diffusion Features for Weakly Supervised Medical Image Segmentation",
    "id": "2506.23460v1",
    "abstract": "Weakly supervised semantic segmentation (WSSS) methods using class labels often rely on class activation maps (CAMs) to localize objects. However, traditional CAM-based methods struggle with partial activations and imprecise object boundaries due to optimization discrepancies between classification and segmentation. Recently, the conditional diffusion model (CDM) has been used as an alternative for generating segmentation masks in WSSS, leveraging its strong image generation capabilities tailored to specific class distributions. By modifying or perturbing the condition during diffusion sampling, the related objects can be highlighted in the generated images. Yet, the saliency maps generated by CDMs are prone to noise from background alterations during reverse diffusion. To alleviate the problem, we introduce Contrastive Learning with Diffusion Features (CLDF), a novel method that uses contrastive learning to train a pixel decoder to map the diffusion features from a frozen CDM to a low-dimensional embedding space for segmentation. Specifically, we integrate gradient maps generated from CDM external classifier with CAMs to identify foreground and background pixels with fewer false positives/negatives for contrastive learning, enabling robust pixel embedding learning. Experimental results on four segmentation tasks from two public medical datasets demonstrate that our method significantly outperforms existing baselines.",
    "date": "2025-06-30",
    "x": -2.3396012783050537,
    "y": 0.6072390675544739,
    "z": -0.779508650302887,
    "cluster_id": 4,
    "layouts": {
      "pacmap": {
        "x": -2.3396012783050537,
        "y": 0.6072390675544739,
        "z": -0.779508650302887
      },
      "starmap": {
        "x": 0.025164837017655373,
        "y": -0.02345719002187252,
        "z": 0.00025485281366854906
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Xiaoxiao Yang, Chao Feng, Jiancheng Chen",
    "date": "2025-06-30",
    "abstract": "Portable and wearable consumer-grade electroencephalography (EEG) devices, like Muse headbands, offer unprecedented mobility for daily brain-computer interface (BCI) applications, including cognitive load detection. However, the exacerbated non-stationarity in portable EEG signals constrains data fidelity and decoding accuracy, creating a fundamental trade-off between portability and performance. To mitigate such limitation, we propose MuseCogNet (Muse-based Cognitive Network), a unified joint learning framework integrating self-supervised and supervised training paradigms. In particular, we introduce an EEG-grounded self-supervised reconstruction loss based on average pooling to capture robust neurophysiological patterns, while cross-entropy loss refines task-specific cognitive discriminants. This joint learning framework resembles the bottom-up and top-down attention in humans, enabling MuseCogNet to significantly outperform state-of-the-art methods on a publicly available Muse dataset and establish an implementable pathway for neurocognitive monitoring in ecological settings.",
    "title": "Neuro-Informed Joint Learning Enhances Cognitive Workload Decoding in Portable BCIs",
    "id": "2506.23458v2",
    "x": -1.5886813402175903,
    "y": -2.736311435699463,
    "z": -0.6922376155853271,
    "cluster_id": 7,
    "layouts": {
      "pacmap": {
        "x": -1.5886813402175903,
        "y": -2.736311435699463,
        "z": -0.6922376155853271
      },
      "starmap": {
        "x": 0.006310933269560337,
        "y": -0.01692509837448597,
        "z": -0.01086839847266674
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "William Gay, William He, Nicholas Kocurek, Ryan O'Donnell",
    "abstract": "Certain tasks in high-dimensional statistics become easier when the underlying distribution satisfies a local-to-global property called approximate tensorization of entropy (ATE). For example, the Glauber dynamics Markov chain of an ATE distribution mixes fast and can produce approximate samples in a small amount of time, since such a distribution satisfies a modified log-Sobolev inequality. Moreover, identity-testing for an ATE distribution requires few samples if the tester is given coordinate conditional access to the unknown distribution, as shown by Blanca, Chen, \\v{S}tefankovi\\v{c}, and Vigoda (COLT 2023).   A natural class of distributions that do not satisfy ATE consists of mixtures of (few) distributions that do satisfy ATE. We study the complexity of identity-testing and sampling for these distributions. Our main results are the following:   1. We show fast mixing of Glauber dynamics from a data-based initialization, with optimal sample complexity, for mixtures of distributions satisfying modified log-Sobolev inequalities. This extends work of Huang, Koehler, Lee, Mohanty, Rajaraman, Vuong, and Wu (STOC 2025, COLT 2025) for mixtures of distributions satisfying Poincar\\'e inequalities.   2. Answering an open question posed by Blanca et al., we give efficient identity-testers for mixtures of ATE distributions in the coordinate-conditional sampling access model. We also give some simplifications and improvements to the original algorithm of Blanca et al.",
    "title": "Sampling and Identity-Testing Without Approximate Tensorization of Entropy",
    "date": "2025-06-30",
    "id": "2506.23456v1",
    "x": 1.0746852159500122,
    "y": -3.4929983615875244,
    "z": -1.0222545862197876,
    "cluster_id": 6,
    "layouts": {
      "pacmap": {
        "x": 1.0746852159500122,
        "y": -3.4929983615875244,
        "z": -1.0222545862197876
      },
      "starmap": {
        "x": 0.009441455826163292,
        "y": -0.012232238426804543,
        "z": -0.01171815674751997
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "Minimax Optimal Two-Stage Algorithm For Moment Estimation Under Covariate Shift",
    "abstract": "Covariate shift occurs when the distribution of input features differs between the training and testing phases. In covariate shift, estimating an unknown function's moment is a classical problem that remains under-explored, despite its common occurrence in real-world scenarios. In this paper, we investigate the minimax lower bound of the problem when the source and target distributions are known. To achieve the minimax optimal bound (up to a logarithmic factor), we propose a two-stage algorithm. Specifically, it first trains an optimal estimator for the function under the source distribution, and then uses a likelihood ratio reweighting procedure to calibrate the moment estimator. In practice, the source and target distributions are typically unknown, and estimating the likelihood ratio may be unstable. To solve this problem, we propose a truncated version of the estimator that ensures double robustness and provide the corresponding upper bound. Extensive numerical studies on synthetic examples confirm our theoretical findings and further illustrate the effectiveness of our proposed method.",
    "date": "2025-06-30",
    "id": "2506.23453v1",
    "authors": "Zhen Zhang, Xin Liu, Shaoli Wang, Jiaye Teng",
    "x": 0.9232105612754822,
    "y": -3.152886152267456,
    "z": -1.1476244926452637,
    "cluster_id": 6,
    "layouts": {
      "pacmap": {
        "x": 0.9232105612754822,
        "y": -3.152886152267456,
        "z": -1.1476244926452637
      },
      "starmap": {
        "x": 0.008508101105690002,
        "y": -0.015311548486351967,
        "z": -0.018062962219119072
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "Enhancing Insider Threat Detection Using User-Based Sequencing and Transformer Encoders",
    "abstract": "Insider threat detection presents unique challenges due to the authorized status of malicious actors and the subtlety of anomalous behaviors. Existing machine learning methods often treat user activity as isolated events, thereby failing to leverage sequential dependencies in user behavior. In this study, we propose a User-Based Sequencing (UBS) methodology, transforming the CERT insider threat dataset into structured temporal sequences suitable for deep sequential modeling. We deploy a Transformer Encoder architecture to model benign user activity and employ its reconstruction errors as anomaly scores. These scores are subsequently evaluated using three unsupervised outlier detection algorithms: One-Class SVM (OCSVM), Local Outlier Factor (LOF), and Isolation Forest (iForest). Across four rigorously designed test sets, including combinations of multiple CERT dataset releases, our UBS-Transformer pipeline consistently achieves state-of-the-art performance - notably 96.61% accuracy, 99.43% recall, 96.38% F1-score, 95.00% AUROC, and exceptionally low false negative (0.0057) and false positive (0.0571) rates. Comparative analyses demonstrate that our approach substantially outperforms tabular and conventional autoencoder baselines, underscoring the efficacy of sequential user modeling and advanced anomaly detection in the insider threat domain.",
    "date": "2025-06-30",
    "authors": "Mohamed Elbasheer, Adewale Akinfaderin",
    "id": "2506.23446v1",
    "x": -0.1538277268409729,
    "y": 0.8384298086166382,
    "z": -2.1063101291656494,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -0.1538277268409729,
        "y": 0.8384298086166382,
        "z": -2.1063101291656494
      },
      "starmap": {
        "x": 0.014997759833931923,
        "y": 0.01274524163454771,
        "z": -0.0146512845531106
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-30",
    "title": "PathDiff: Histopathology Image Synthesis with Unpaired Text and Mask Conditions",
    "authors": "Mahesh Bhosale, Abdul Wasi, Yuanhao Zhai, Yunjie Tian, Samuel Border, Nan Xi, Pinaki Sarder, Junsong Yuan, David Doermann, Xuan Gong",
    "id": "2506.23440v1",
    "abstract": "Diffusion-based generative models have shown promise in synthesizing histopathology images to address data scarcity caused by privacy constraints. Diagnostic text reports provide high-level semantic descriptions, and masks offer fine-grained spatial structures essential for representing distinct morphological regions. However, public datasets lack paired text and mask data for the same histopathological images, limiting their joint use in image generation. This constraint restricts the ability to fully exploit the benefits of combining both modalities for enhanced control over semantics and spatial details. To overcome this, we propose PathDiff, a diffusion framework that effectively learns from unpaired mask-text data by integrating both modalities into a unified conditioning space. PathDiff allows precise control over structural and contextual features, generating high-quality, semantically accurate images. PathDiff also improves image fidelity, text-image alignment, and faithfulness, enhancing data augmentation for downstream tasks like nuclei segmentation and classification. Extensive experiments demonstrate its superiority over existing methods.",
    "x": -2.436196804046631,
    "y": 0.5047113299369812,
    "z": -0.46936115622520447,
    "cluster_id": 4,
    "layouts": {
      "pacmap": {
        "x": -2.436196804046631,
        "y": 0.5047113299369812,
        "z": -0.46936115622520447
      },
      "starmap": {
        "x": 0.02331739105284214,
        "y": -0.024459704756736755,
        "z": 0.007078103255480528
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "From Large-scale Audio Tagging to Real-Time Explainable Emergency Vehicle Sirens Detection",
    "id": "2506.23437v1",
    "date": "2025-06-30",
    "authors": "Stefano Giacomelli, Marco Giordano, Claudia Rinaldi, Fabio Graziosi",
    "abstract": "Accurate recognition of Emergency Vehicle (EV) sirens is critical for the integration of intelligent transportation systems, smart city monitoring systems, and autonomous driving technologies. Modern automatic solutions are limited by the lack of large scale, curated datasets and by the computational demands of state of the art sound event detection models. This work introduces E2PANNs (Efficient Emergency Pre trained Audio Neural Networks), a lightweight Convolutional Neural Network architecture derived from the PANNs framework, specifically optimized for binary EV siren detection. Leveraging our dedicated subset of AudioSet (AudioSet EV) we fine-tune and evaluate E2PANNs across multiple reference datasets and test its viability on embedded hardware. The experimental campaign includes ablation studies, cross-domain benchmarking, and real-time inference deployment on edge device. Interpretability analyses exploiting Guided Backpropagation and ScoreCAM algorithms provide insights into the model internal representations and validate its ability to capture distinct spectrotemporal patterns associated with different types of EV sirens. Real time performance is assessed through frame wise and event based detection metrics, as well as a detailed analysis of false positive activations. Results demonstrate that E2PANNs establish a new state of the art in this research domain, with high computational efficiency, and suitability for edge-based audio monitoring and safety-critical applications.",
    "x": -0.4125049412250519,
    "y": -0.7395442724227905,
    "z": 0.7434201836585999,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -0.4125049412250519,
        "y": -0.7395442724227905,
        "z": 0.7434201836585999
      },
      "starmap": {
        "x": 0.01667061261832714,
        "y": -0.018453506752848625,
        "z": -0.014823698438704014
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Tianran Liu, Shengwen Zhao, Nicholas Rhinehart",
    "date": "2025-06-30",
    "abstract": "LiDAR-based world models offer more structured and geometry-aware representations than their image-based counterparts. However, existing LiDAR world models are narrowly trained; each model excels only in the domain for which it was built. Can we develop LiDAR world models that exhibit strong transferability across multiple domains? We conduct the first systematic domain transfer study across three demanding scenarios: (i) outdoor to indoor generalization, (ii) sparse-beam \\& dense-beam adaptation, and (iii) non-semantic to semantic transfer. Given different amounts of fine-tuning data, our experiments show that a single pre-trained model can achieve up to 11% absolute improvement (83\\% relative) over training from scratch and outperforms training from scratch in 30/36 of our comparisons. This transferability of dynamic learning significantly reduces the reliance on manually annotated data for semantic occupancy forecasting: our method exceed the previous semantic occupancy forecasting models with only 5% of the labeled training data required by prior models. We also observed inefficiencies of current LiDAR world models, mainly through their under-compression of LiDAR data and inefficient training objectives. To address this, we propose a latent conditional flow matching (CFM)-based frameworks that achieves state-of-the-art reconstruction accuracy using only half the training data and a compression ratio 6 times higher than that of prior methods. Our model achieves SOTA performance on future-trajectory-conditioned semantic occupancy forecasting while being 23x more computationally efficient (a 28x FPS speedup); and achieves SOTA performance on semantic occupancy forecasting while being 2x more computationally efficient (a 1.1x FPS speedup).",
    "title": "Towards foundational LiDAR world models with efficient latent flow matching",
    "id": "2506.23434v1",
    "x": -0.5068269371986389,
    "y": -0.49154895544052124,
    "z": -1.0836095809936523,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -0.5068269371986389,
        "y": -0.49154895544052124,
        "z": -1.0836095809936523
      },
      "starmap": {
        "x": 0.013114923611283302,
        "y": -0.01901499554514885,
        "z": -0.008123830892145634
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Zixian Huang, Chenxu Niu, Yu Gu, Gengyang Xiao, Xinwei Huang, Gong Cheng",
    "title": "Pipelined Decoder for Efficient Context-Aware Text Generation",
    "id": "2506.23431v2",
    "abstract": "As the basis of generative AI, an autoregressive model requires the generation of a new token depending on all the previously generated tokens, which brings high quality but also restricts the model to generate tokens one by one, forming a bottleneck limiting the generation speed. In this paper, we propose a new decoder architecture that efficiently generates text in parallel for context-aware generation tasks. Our proposed pipelined decoder initiates the generation of multiple subsequences simultaneously, and, at each time-step, it generates a new token for each subsequence to realize parallelism. Experiments on multiple text generation tasks, including question answering, text summarization, and keyphrase generation, show that our pipelined decoder significantly improves the generation speed without a significant loss of generation quality or additional memory consumption.",
    "date": "2025-06-29",
    "x": -2.2415144443511963,
    "y": 0.6916278004646301,
    "z": 0.24546051025390625,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": -2.2415144443511963,
        "y": 0.6916278004646301,
        "z": 0.24546051025390625
      },
      "starmap": {
        "x": -0.014456276781857014,
        "y": 0.004947144538164139,
        "z": 0.011422093957662582
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-29",
    "title": "DPOT: A DeepParticle method for Computation of Optimal Transport with convergence guarantee",
    "authors": "Yingyuan Li, Aokun Wang, Zhongjian Wang",
    "abstract": "In this work, we propose a novel machine learning approach to compute the optimal transport map between two continuous distributions from their unpaired samples, based on the DeepParticle methods. The proposed method leads to a min-min optimization during training and does not impose any restriction on the network structure. Theoretically we establish a weak convergence guarantee and a quantitative error bound between the learned map and the optimal transport map. Our numerical experiments validate the theoretical results and the effectiveness of the new approach, particularly on real-world tasks.",
    "id": "2506.23429v1",
    "x": 0.7961645126342773,
    "y": -2.9117562770843506,
    "z": -0.4688762426376343,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": 0.7961645126342773,
        "y": -2.9117562770843506,
        "z": -0.4688762426376343
      },
      "starmap": {
        "x": 0.00851704366505146,
        "y": -0.01508408598601818,
        "z": -0.01347363367676735
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Menna Taha, Aya Ahmed, Mohammed Karmoose, Yasser Gadallah",
    "abstract": "Autonomous vehicles (AVs) use object detection models to recognize their surroundings and make driving decisions accordingly. Conventional object detection approaches classify objects into known classes, which limits the AV's ability to detect and appropriately respond to Out-of-Distribution (OOD) objects. This problem is a significant safety concern since the AV may fail to detect objects or misclassify them, which can potentially lead to hazardous situations such as accidents. Consequently, we propose a novel object detection approach that shifts the emphasis from conventional class-based classification to object harmfulness determination. Instead of object detection by their specific class, our method identifies them as either 'harmful' or 'harmless' based on whether they pose a danger to the AV. This is done based on the object position relative to the AV and its trajectory. With this metric, our model can effectively detect previously unseen objects to enable the AV to make safer real-time decisions. Our results demonstrate that the proposed model effectively detects OOD objects, evaluates their harmfulness, and classifies them accordingly, thus enhancing the AV decision-making effectiveness in dynamic environments.",
    "id": "2506.23426v1",
    "date": "2025-06-29",
    "title": "Detecting What Matters: A Novel Approach for Out-of-Distribution 3D Object Detection in Autonomous Vehicles",
    "x": -1.3188893795013428,
    "y": 1.475106954574585,
    "z": -0.6070226430892944,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -1.3188893795013428,
        "y": 1.475106954574585,
        "z": -0.6070226430892944
      },
      "starmap": {
        "x": 0.02848455309867859,
        "y": 0.019549710676074028,
        "z": -0.021136561408638954
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Real-world time series often exhibit a non-stationary nature, degrading the performance of pre-trained forecasting models. Test-Time Adaptation (TTA) addresses this by adjusting models during inference, but existing methods typically update the full model, increasing memory and compute costs. We propose PETSA, a parameter-efficient method that adapts forecasters at test time by only updating small calibration modules on the input and output. PETSA uses low-rank adapters and dynamic gating to adjust representations without retraining. To maintain accuracy despite limited adaptation capacity, we introduce a specialized loss combining three components: (1) a robust term, (2) a frequency-domain term to preserve periodicity, and (3) a patch-wise structural term for structural alignment. PETSA improves the adaptability of various forecasting backbones while requiring fewer parameters than baselines. Experimental results on benchmark datasets show that PETSA achieves competitive or better performance across all horizons. Our code is available at: https://github.com/BorealisAI/PETSA",
    "title": "Accurate Parameter-Efficient Test-Time Adaptation for Time Series Forecasting",
    "id": "2506.23424v1",
    "authors": "Heitor R. Medeiros, Hossein Sharifi-Noghabi, Gabriel L. Oliveira, Saghar Irandoust",
    "date": "2025-06-29",
    "x": -0.578776478767395,
    "y": -0.8272016644477844,
    "z": -1.103142499923706,
    "cluster_id": 5,
    "layouts": {
      "pacmap": {
        "x": -0.578776478767395,
        "y": -0.8272016644477844,
        "z": -1.103142499923706
      },
      "starmap": {
        "x": 0.015262068249285221,
        "y": -0.026575947180390358,
        "z": -0.01979728229343891
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-29",
    "abstract": "Past work has studied the effects of fine-tuning on large language models' (LLMs) overall performance on certain tasks. However, a quantitative and systematic method for analyzing its effect on individual outputs is still lacking. Here, we propose a new method for measuring the contribution that fine-tuning makes to individual LLM responses, assuming access to the original pre-trained model. Our method tracks the model's intermediate hidden states, providing a more fine-grained insight into the effects of fine-tuning than a simple comparison of final outputs from pre-trained and fine-tuned models. We introduce and theoretically analyze an exact decomposition of any fine-tuned LLM into a pre-training component and a fine-tuning component. Empirically, we find that model behavior and performance can be steered by up- or down-scaling the fine-tuning component during the forward pass. Motivated by this finding and our theoretical analysis, we define the Tuning Contribution (TuCo) as the ratio of the magnitudes of the fine-tuning component to the pre-training component. We observe that three prominent adversarial attacks on LLMs circumvent safety measures in a way that reduces TuCo, and that TuCo is consistently lower on prompts where these attacks succeed compared to those where they do not. This suggests that attenuating the effect of fine-tuning on model outputs plays a role in the success of such attacks. In summary, TuCo enables the quantitative study of how fine-tuning influences model behavior and safety, and vice versa.",
    "authors": "Felipe Nuti, Tim Franzmeyer, Jo\u00e3o Henriques",
    "title": "TuCo: Measuring the Contribution of Fine-Tuning to Individual Responses of LLMs",
    "id": "2506.23423v1",
    "x": 1.0130339860916138,
    "y": -0.5565268397331238,
    "z": 0.722888708114624,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 1.0130339860916138,
        "y": -0.5565268397331238,
        "z": 0.722888708114624
      },
      "starmap": {
        "x": -0.014772639609873295,
        "y": 0.0032713771797716618,
        "z": -0.0005411537131294608
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Benchmark data sets are a cornerstone of machine learning development and applications, ensuring new methods are robust, reliable and competitive. The relative rarity of benchmark sets in computational science, due to the uniqueness of the problems and the pace of change in the associated domains, makes evaluating new innovations difficult for computational scientists. In this paper a new tool is developed and tested to potentially turn any of the increasing numbers of scientific data sets made openly available into a benchmark accessible to the community. BenchMake uses non-negative matrix factorisation to deterministically identify and isolate challenging edge cases on the convex hull (the smallest convex set that contains all existing data instances) and partitions a required fraction of matched data instances into a testing set that maximises divergence and statistical significance, across tabular, graph, image, signal and textual modalities. BenchMake splits are compared to establish splits and random splits using ten publicly available benchmark sets from different areas of science, with different sizes, shapes, distributions.",
    "title": "BenchMake: Turn any scientific data set into a reproducible benchmark",
    "date": "2025-06-29",
    "authors": "Amanda S Barnard",
    "id": "2506.23419v1",
    "x": 1.8822110891342163,
    "y": -1.4954553842544556,
    "z": -1.0328701734542847,
    "cluster_id": 6,
    "layouts": {
      "pacmap": {
        "x": 1.8822110891342163,
        "y": -1.4954553842544556,
        "z": -1.0328701734542847
      },
      "starmap": {
        "x": 0.009016038849949837,
        "y": -0.009628663770854473,
        "z": -0.02223089151084423
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23418v1",
    "title": "Why Settle for Mid: A Probabilistic Viewpoint to Spatial Relationship Alignment in Text-to-image Models",
    "authors": "Parham Rezaei, Arash Marioriyad, Mahdieh Soleymani Baghshah, Mohammad Hossein Rohban",
    "date": "2025-06-29",
    "abstract": "Despite the ability of text-to-image models to generate high-quality, realistic, and diverse images, they face challenges in compositional generation, often struggling to accurately represent details specified in the input prompt. A prevalent issue in compositional generation is the misalignment of spatial relationships, as models often fail to faithfully generate images that reflect the spatial configurations specified between objects in the input prompts. To address this challenge, we propose a novel probabilistic framework for modeling the relative spatial positioning of objects in a scene, leveraging the concept of Probability of Superiority (PoS). Building on this insight, we make two key contributions. First, we introduce a novel evaluation metric, PoS-based Evaluation (PSE), designed to assess the alignment of 2D and 3D spatial relationships between text and image, with improved adherence to human judgment. Second, we propose PoS-based Generation (PSG), an inference-time method that improves the alignment of 2D and 3D spatial relationships in T2I models without requiring fine-tuning. PSG employs a Part-of-Speech PoS-based reward function that can be utilized in two distinct ways: (1) as a gradient-based guidance mechanism applied to the cross-attention maps during the denoising steps, or (2) as a search-based strategy that evaluates a set of initial noise vectors to select the best one. Extensive experiments demonstrate that the PSE metric exhibits stronger alignment with human judgment compared to traditional center-based metrics, providing a more nuanced and reliable measure of complex spatial relationship accuracy in text-image alignment. Furthermore, PSG significantly enhances the ability of text-to-image models to generate images with specified spatial configurations, outperforming state-of-the-art methods across multiple evaluation metrics and benchmarks.",
    "x": -2.751690149307251,
    "y": 0.8244869709014893,
    "z": -0.21987693011760712,
    "cluster_id": 2,
    "layouts": {
      "pacmap": {
        "x": -2.751690149307251,
        "y": 0.8244869709014893,
        "z": -0.21987693011760712
      },
      "starmap": {
        "x": 0.02391776628792286,
        "y": -0.00029952474869787693,
        "z": 0.07099408656358719
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-29",
    "title": "A High-Throughput Platform to Bench Test Smartphone-Based Heart Rate Measurements Derived From Video",
    "authors": "Ming-Zher Poh, Jonathan Wang, Jonathan Hsu, Lawrence Cai, Eric Teasley, James A. Taylor, Jameson K. Rogers, Anupam Pathak, Shwetak Patel",
    "abstract": "Smartphone-based heart rate (HR) monitoring apps using finger-over-camera photoplethysmography (PPG) face significant challenges in performance evaluation and device compatibility due to device variability and fragmentation. Manual testing is impractical, and standardized methods are lacking. This paper presents a novel, high-throughput bench-testing platform to address this critical need. We designed a system comprising a test rig capable of holding 12 smartphones for parallel testing, a method for generating synthetic PPG test videos with controllable HR and signal quality, and a host machine for coordinating video playback and data logging. The system achieved a mean absolute percentage error (MAPE) of 0.11% +/- 0.001% between input and measured HR, and a correlation coefficient of 0.92 +/- 0.008 between input and measured PPG signals using a clinically-validated smartphone-based HR app. Bench-testing results of 20 different smartphone models correctly classified all the devices as meeting the ANSI/CTA accuracy standards for HR monitors (MAPE <10%) when compared to a prospective clinical study with 80 participants, demonstrating high positive predictive value. This platform offers a scalable solution for pre-deployment testing of smartphone HR apps to improve app performance, ensure device compatibility, and advance the field of mobile health.",
    "id": "2506.23414v1",
    "x": 1.9115952253341675,
    "y": -1.8107178211212158,
    "z": -0.7776299715042114,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": 1.9115952253341675,
        "y": -1.8107178211212158,
        "z": -0.7776299715042114
      },
      "starmap": {
        "x": 0.012522827833890915,
        "y": -0.014383641071617603,
        "z": 0.017724918201565742
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "Datasets for Fairness in Language Models: An In-Depth Survey",
    "abstract": "Fairness benchmarks play a central role in shaping how we evaluate language models, yet surprisingly little attention has been given to examining the datasets that these benchmarks rely on. This survey addresses that gap by presenting a broad and careful review of the most widely used fairness datasets in current language model research, characterizing them along several key dimensions including their origin, scope, content, and intended use to help researchers better appreciate the assumptions and limitations embedded in these resources. To support more meaningful comparisons and analyses, we introduce a unified evaluation framework that reveals consistent patterns of demographic disparities across datasets and scoring methods. Applying this framework to twenty four common benchmarks, we highlight the often overlooked biases that can influence conclusions about model fairness and offer practical guidance for selecting, combining, and interpreting these datasets. We also point to opportunities for creating new fairness benchmarks that reflect more diverse social contexts and encourage more thoughtful use of these tools going forward. All code, data, and detailed results are publicly available at https://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasets to promote transparency and reproducibility across the research community.",
    "id": "2506.23411v1",
    "authors": "Jiale Zhang, Zichong Wang, Avash Palikhe, Zhipeng Yin, Wenbin Zhang",
    "date": "2025-06-29",
    "x": 2.2000677585601807,
    "y": 0.780150830745697,
    "z": -1.23101806640625,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 2.2000677585601807,
        "y": 0.780150830745697,
        "z": -1.23101806640625
      },
      "starmap": {
        "x": -0.018906081095337868,
        "y": 0.003138935659080744,
        "z": -0.0035226561594754457
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23408v1",
    "title": "Do LLMs Dream of Discrete Algorithms?",
    "date": "2025-06-29",
    "abstract": "Large Language Models (LLMs) have rapidly transformed the landscape of artificial intelligence, enabling natural language interfaces and dynamic orchestration of software components. However, their reliance on probabilistic inference limits their effectiveness in domains requiring strict logical reasoning, discrete decision-making, and robust interpretability. This paper investigates these limitations and proposes a neurosymbolic approach that augments LLMs with logic-based reasoning modules, particularly leveraging Prolog predicates and composable toolsets. By integrating first-order logic and explicit rule systems, our framework enables LLMs to decompose complex queries into verifiable sub-tasks, orchestrate reliable solutions, and mitigate common failure modes such as hallucination and incorrect step decomposition. We demonstrate the practical benefits of this hybrid architecture through experiments on the DABStep benchmark, showing improved precision, coverage, and system documentation in multi-step reasoning tasks. Our results indicate that combining LLMs with modular logic reasoning restores engineering rigor, enhances system reliability, and offers a scalable path toward trustworthy, interpretable AI agents across complex domains.",
    "authors": "Claudionor Coelho Jr, Yanen Li, Philip Tee",
    "x": 3.019092082977295,
    "y": 1.4860683679580688,
    "z": 0.48145654797554016,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 3.019092082977295,
        "y": 1.4860683679580688,
        "z": 0.48145654797554016
      },
      "starmap": {
        "x": -0.022998390719294548,
        "y": 0.009720232337713242,
        "z": -0.0026820339262485504
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "The opacity of many supervised learning algorithms remains a key challenge, hindering scientific discovery and limiting broader deployment -- particularly in high-stakes domains. This paper develops model- and distribution-agnostic significance tests to assess the influence of input features in any regression or classification algorithm. Our method evaluates a feature's incremental contribution to model performance by masking its values across samples. Under the null hypothesis, the distribution of performance differences across a test set has a non-positive median. We construct a uniformly most powerful, randomized sign test for this median, yielding exact p-values for assessing feature significance and confidence intervals with exact coverage for estimating population-level feature importance. The approach requires minimal assumptions, avoids model retraining or auxiliary models, and remains computationally efficient even for large-scale, high-dimensional settings. Experiments on synthetic tasks validate its statistical and computational advantages, and applications to real-world data illustrate its practical utility.",
    "date": "2025-06-29",
    "authors": "Kay Giesecke, Enguerrand Horel, Chartsiri Jirachotkulthorn",
    "id": "2506.23396v1",
    "title": "AICO: Feature Significance Tests for Supervised Learning",
    "x": 0.9499024152755737,
    "y": -2.6637611389160156,
    "z": -1.377469778060913,
    "cluster_id": 6,
    "layouts": {
      "pacmap": {
        "x": 0.9499024152755737,
        "y": -2.6637611389160156,
        "z": -1.377469778060913
      },
      "starmap": {
        "x": 0.00965732242912054,
        "y": -0.014147229492664337,
        "z": -0.018448354676365852
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23394v1",
    "title": "Teaching a Language Model to Speak the Language of Tools",
    "date": "2025-06-29",
    "abstract": "External tool integration through function-calling is essential for practical language model applications, yet most multilingual models lack reliable tool-use capabilities in non-English languages. Even state-of-the-art multilingual models struggle with determining when to use tools and generating the structured outputs required for function calls, often exhibiting language confusion when prompted in lower-resource languages. This work presents a methodology for adapting existing language models to enable robust tool use in any target language, using Bulgarian as a case study. The approach involves continued training of the BgGPT model series (2.6B, 9B, 27B parameters) on a novel bilingual dataset of 10,035 function-calling examples designed to support standardized protocols like MCP (Model Context Protocol). The research introduces TUCAN (Tool-Using Capable Assistant Navigator), which achieves up to 28.75% improvement in function-calling accuracy over base models while preserving core language understanding, as verified on established Bulgarian benchmarks. Beyond accuracy gains, TUCAN models demonstrate production-ready response formatting with clean, parsable function calls, contrasting with the verbose and inconsistent outputs of base models. The models, evaluation framework, and dataset are released to enable replication for other languages. This work demonstrates a practical approach for extending tool-augmented capabilities beyond English-centric systems.",
    "authors": "Simeon Emanuilov",
    "x": 1.5551934242248535,
    "y": 1.1542798280715942,
    "z": -1.0645861625671387,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 1.5551934242248535,
        "y": 1.1542798280715942,
        "z": -1.0645861625671387
      },
      "starmap": {
        "x": -0.01903616636991501,
        "y": 0.00448632100597024,
        "z": 0.0006667081033810973
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-29",
    "abstract": "Generating Wikipedia articles autonomously is a challenging task requiring the integration of accurate, comprehensive, and well-structured information from diverse sources. This paper introduces the Memory Organization-based Generation (MOG) framework, a novel approach to address these challenges by leveraging a hierarchical memory architecture. MOG extracts fine-grained memory units from web documents, recursively organizes them into a Wikipedia-style hierarchical structure, and uses this structure to guide the generation process. This ensures alignment between memory and the article outline, improving both informativeness and verifiability while minimizing hallucinations. Additionally, a citation module is implemented to enhance traceability by linking every generated sentence to specific memory units. Evaluations on our newly created WikiStart dataset demonstrate that MOG outperforms baseline methods in producing informative and reliable articles, making it particularly robust in real-world scenarios.",
    "title": "Hierarchical Memory Organization for Wikipedia Generation",
    "authors": "Eugene J. Yu, Dawei Zhu, Yifan Song, Xiangyu Wong, Jiebin Zhang, Wenxuan Shi, Xiaoguang Li, Qun Liu, Sujian Li",
    "id": "2506.23393v1",
    "x": -0.2997952699661255,
    "y": 1.1189618110656738,
    "z": 1.506373405456543,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": -0.2997952699661255,
        "y": 1.1189618110656738,
        "z": 1.506373405456543
      },
      "starmap": {
        "x": -0.01738790236413479,
        "y": 0.005466022528707981,
        "z": 0.0012742410181090236
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23382v1",
    "abstract": "Implicit Neural Representations (INRs) offer exceptional fidelity for video compression by learning per-video optimized functions, but their adoption is crippled by impractically slow encoding times. Existing attempts to accelerate INR encoding often sacrifice reconstruction quality or crucial coordinate-level control essential for adaptive streaming and transcoding. We introduce SIEDD (Shared-Implicit Encoder with Discrete Decoders), a novel architecture that fundamentally accelerates INR encoding without these compromises. SIEDD first rapidly trains a shared, coordinate-based encoder on sparse anchor frames to efficiently capture global, low-frequency video features. This encoder is then frozen, enabling massively parallel training of lightweight, discrete decoders for individual frame groups, further expedited by aggressive coordinate-space sampling. This synergistic design delivers a remarkable 20-30X encoding speed-up over state-of-the-art INR codecs on HD and 4K benchmarks, while maintaining competitive reconstruction quality and compression ratios. Critically, SIEDD retains full coordinate-based control, enabling continuous resolution decoding and eliminating costly transcoding. Our approach significantly advances the practicality of high-fidelity neural video compression, demonstrating a scalable and efficient path towards real-world deployment. Our codebase is available at https://github.com/VikramRangarajan/SIEDD .",
    "authors": "Vikram Rangarajan, Shishira Maiya, Max Ehrlich, Abhinav Shrivastava",
    "date": "2025-06-29",
    "title": "SIEDD: Shared-Implicit Encoder with Discrete Decoders",
    "x": -2.5440573692321777,
    "y": 2.517272472381592,
    "z": 1.3623523712158203,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -2.5440573692321777,
        "y": 2.517272472381592,
        "z": 1.3623523712158203
      },
      "starmap": {
        "x": 0.03027629666030407,
        "y": -0.002790117170661688,
        "z": 0.026999039575457573
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-29",
    "abstract": "Large language models (LLMs) are used in a variety of mission-critical roles. Due to the rapidly developing nature of LLMs, there is a lack of quantifiable understanding of the bias and perspective associated with LLM output. Inspired by this need, this paper considers the broader issue of perspective or viewpoint of general text and perspective control of large-language model (LLM) output. Perspective-Dial consists of two main components: a (1) metric space, dubbed Perspective Space, that enables quantitative measurements of different perspectives regarding a topic, and the use of (2) Systematic Prompt Engineering that utilizes greedy-coordinate descent to control LLM output perspective based on measurement feedback from the Perspective Space. The empirical nature of the approach allows progress to side step a principled understanding of perspective or bias -- effectively quantifying and adjusting outputs for a variety of topics. Potential applications include detection, tracking and mitigation of LLM bias, narrative detection, sense making and tracking in public discourse, and debate bot advocating given perspective.",
    "id": "2506.23377v1",
    "authors": "Taejin Kim, Siun-Chuon Mau, Konrad Vesey",
    "title": "Perspective Dial: Measuring Perspective of Text and Guiding LLM Outputs",
    "x": 3.4630286693573,
    "y": 0.8242442607879639,
    "z": 0.8823686242103577,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 3.4630286693573,
        "y": 0.8242442607879639,
        "z": 0.8823686242103577
      },
      "starmap": {
        "x": -0.02000737003982067,
        "y": 0.00682807108387351,
        "z": 0.004313611891120672
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23374v1",
    "title": "When Additive Noise Meets Unobserved Mediators: Bivariate Denoising Diffusion for Causal Discovery",
    "date": "2025-06-29",
    "abstract": "Distinguishing cause and effect from bivariate observational data is a foundational problem in many disciplines, but challenging without additional assumptions. Additive noise models (ANMs) are widely used to enable sample-efficient bivariate causal discovery. However, conventional ANM-based methods fail when unobserved mediators corrupt the causal relationship between variables. This paper makes three key contributions: first, we rigorously characterize why standard ANM approaches break down in the presence of unmeasured mediators. Second, we demonstrate that prior solutions for hidden mediation are brittle in finite sample settings, limiting their practical utility. To address these gaps, we propose Bivariate Denoising Diffusion (BiDD) for causal discovery, a method designed to handle latent noise introduced by unmeasured mediators. Unlike prior methods that infer directionality through mean squared error loss comparisons, our approach introduces a novel independence test statistic: during the noising and denoising processes for each variable, we condition on the other variable as input and evaluate the independence of the predicted noise relative to this input. We prove asymptotic consistency of BiDD under the ANM, and conjecture that it performs well under hidden mediation. Experiments on synthetic and real-world data demonstrate consistent performance, outperforming existing methods in mediator-corrupted settings while maintaining strong performance in mediator-free settings.",
    "authors": "Dominik Meier, Sujai Hiremath, Promit Ghosal, Kyra Gan",
    "x": -2.694074869155884,
    "y": -0.7278222441673279,
    "z": 0.7981051206588745,
    "cluster_id": 8,
    "layouts": {
      "pacmap": {
        "x": -2.694074869155884,
        "y": -0.7278222441673279,
        "z": 0.7981051206588745
      },
      "starmap": {
        "x": 0.016231438145041466,
        "y": -0.011105870828032494,
        "z": 0.004455828107893467
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Multi-Pitch Estimation (MPE) continues to be a sought after capability of Music Information Retrieval (MIR) systems, and is critical for many applications and downstream tasks involving pitch, including music transcription. However, existing methods are largely based on supervised learning, and there are significant challenges in collecting annotated data for the task. Recently, self-supervised techniques exploiting intrinsic properties of pitch and harmonic signals have shown promise for both monophonic and polyphonic pitch estimation, but these still remain inferior to supervised methods. In this work, we extend the classic supervised MPE paradigm by incorporating several self-supervised objectives based on pitch-invariant and pitch-equivariant properties. This joint training results in a substantial improvement under closed training conditions, which naturally suggests that applying the same objectives to a broader collection of data will yield further improvements. However, in doing so we uncover a phenomenon whereby our model simultaneously overfits to the supervised data while degenerating on data used for self-supervision only. We demonstrate and investigate this and offer our insights on the underlying problem.",
    "date": "2025-06-29",
    "authors": "Frank Cwitkowitz, Zhiyao Duan",
    "title": "Investigating an Overfitting and Degeneration Phenomenon in Self-Supervised Multi-Pitch Estimation",
    "id": "2506.23371v1",
    "x": -1.446999192237854,
    "y": -3.1108152866363525,
    "z": -0.8753987550735474,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -1.446999192237854,
        "y": -3.1108152866363525,
        "z": -0.8753987550735474
      },
      "starmap": {
        "x": 0.004982535727322102,
        "y": -0.02061844989657402,
        "z": -0.01872733235359192
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23367v1",
    "abstract": "We present the first text-to-speech (TTS) system tailored to second language (L2) speakers. We use duration differences between American English tense (longer) and lax (shorter) vowels to create a \"clarity mode\" for Matcha-TTS. Our perception studies showed that French-L1, English-L2 listeners had fewer (at least 9.15%) transcription errors when using our clarity mode, and found it more encouraging and respectful than overall slowed down speech. Remarkably, listeners were not aware of these effects: despite the decreased word error rate in clarity mode, listeners still believed that slowing all target words was the most intelligible, suggesting that actual intelligibility does not correlate with perceived intelligibility. Additionally, we found that Whisper-ASR did not use the same cues as L2 speakers to differentiate difficult vowels and is not sufficient to assess the intelligibility of TTS systems for these individuals.",
    "authors": "Paige Tutt\u00f6s\u00ed, H. Henny Yeung, Yue Wang, Jean-Julien Aucouturier, Angelica Lim",
    "title": "You Sound a Little Tense: L2 Tailored Clear TTS Using Durational Vowel Properties",
    "date": "2025-06-29",
    "x": 1.0942555665969849,
    "y": 2.682554244995117,
    "z": -0.5436350107192993,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 1.0942555665969849,
        "y": 2.682554244995117,
        "z": -0.5436350107192993
      },
      "starmap": {
        "x": -0.020088577643036842,
        "y": 0.006315118167549372,
        "z": 0.006355386693030596
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-29",
    "abstract": "Scientific behavior is often characterized by a tension between building upon established knowledge and introducing novel ideas. Here, we investigate whether this tension is reflected in the relationship between the similarity of a scientific paper to previous research and its eventual citation rate. To operationalize similarity to previous research, we introduce two complementary metrics to characterize the local geometry of a publication's semantic neighborhood: (1) \\emph{density} ($\\rho$), defined as the ratio between a fixed number of previously-published papers and the minimum distance enclosing those papers in a semantic embedding space, and (2) asymmetry ($\\alpha$), defined as the average directional difference between a paper and its nearest neighbors. We tested the predictive relationship between these two metrics and its subsequent citation rate using a Bayesian hierarchical regression approach, surveying $\\sim 53,000$ publications across nine academic disciplines and five different document embeddings. While the individual effects of $\\rho$ on citation count are small and variable, incorporating density-based predictors consistently improves out-of-sample prediction when added to baseline models. These results suggest that the density of a paper's surrounding scientific literature may carry modest but informative signals about its eventual impact. Meanwhile, we find no evidence that publication asymmetry improves model predictions of citation rates. Our work provides a scalable framework for linking document embeddings to scientometric outcomes and highlights new questions regarding the role that semantic similarity plays in shaping the dynamics of scientific reward.",
    "id": "2506.23366v1",
    "authors": "Nathaniel Imel, Zachary Hafen",
    "title": "Density, asymmetry and citation dynamics in scientific literature",
    "x": 1.6937271356582642,
    "y": -1.6868188381195068,
    "z": -0.3875270485877991,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 1.6937271356582642,
        "y": -1.6868188381195068,
        "z": -0.3875270485877991
      },
      "starmap": {
        "x": -0.016742412000894547,
        "y": 0.00255106040276587,
        "z": -0.0029504033736884594
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Yuanhao Cai, He Zhang, Xi Chen, Jinbo Xing, Yiwei Hu, Yuqian Zhou, Kai Zhang, Zhifei Zhang, Soo Ye Kim, Tianyu Wang, Yulun Zhang, Xiaokang Yang, Zhe Lin, Alan Yuille",
    "abstract": "Existing feedforward subject-driven video customization methods mainly study single-subject scenarios due to the difficulty of constructing multi-subject training data pairs. Another challenging problem that how to use the signals such as depth, mask, camera, and text prompts to control and edit the subject in the customized video is still less explored. In this paper, we first propose a data construction pipeline, VideoCus-Factory, to produce training data pairs for multi-subject customization from raw videos without labels and control signals such as depth-to-video and mask-to-video pairs. Based on our constructed data, we develop an Image-Video Transfer Mixed (IVTM) training with image editing data to enable instructive editing for the subject in the customized video. Then we propose a diffusion Transformer framework, OmniVCus, with two embedding mechanisms, Lottery Embedding (LE) and Temporally Aligned Embedding (TAE). LE enables inference with more subjects by using the training subjects to activate more frame embeddings. TAE encourages the generation process to extract guidance from temporally aligned control signals by assigning the same frame embeddings to the control and noise tokens. Experiments demonstrate that our method significantly surpasses state-of-the-art methods in both quantitative and qualitative evaluations. Video demos are at our project page: https://caiyuanhao1998.github.io/project/OmniVCus/. Our code will be released at https://github.com/caiyuanhao1998/Open-OmniVCus",
    "title": "OmniVCus: Feedforward Subject-driven Video Customization with Multimodal Control Conditions",
    "date": "2025-06-29",
    "id": "2506.23361v1",
    "x": -2.4984993934631348,
    "y": 2.025285482406616,
    "z": 1.3046696186065674,
    "cluster_id": 2,
    "layouts": {
      "pacmap": {
        "x": -2.4984993934631348,
        "y": 2.025285482406616,
        "z": 1.3046696186065674
      },
      "starmap": {
        "x": 0.028333405032753944,
        "y": -0.0020181755535304546,
        "z": 0.03592304512858391
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23358v1",
    "authors": "Pawel Renc, Michal K. Grzeszczyk, Linglong Qian, Nassim Oufattole, Jeff Rasley, Arkadiusz Sitek",
    "title": "Federated Timeline Synthesis: Scalable and Private Methodology For Model Training and Deployment",
    "date": "2025-06-29",
    "abstract": "We present Federated Timeline Synthesis (FTS), a novel framework for training generative foundation models across distributed timeseries data applied to electronic health records (EHR). At its core, FTS represents patient history as tokenized Patient Health Timelines (PHTs), language-agnostic sequences encoding temporal, categorical, and continuous clinical information. Each institution trains an autoregressive transformer on its local PHTs and transmits only model weights to a central server. The server uses the generators to synthesize a large corpus of trajectories and train a Global Generator (GG), enabling zero-shot inference via Monte Carlo simulation of future PHTs. We evaluate FTS on five clinically meaningful prediction tasks using MIMIC-IV data, showing that models trained on synthetic data generated by GG perform comparably to those trained on real data. FTS offers strong privacy guarantees, scalability across institutions, and extensibility to diverse prediction and simulation tasks especially in healthcare, including counterfactual inference, early warning detection, and synthetic trial design.",
    "x": 0.2738448679447174,
    "y": -0.6521247625350952,
    "z": 0.054460979998111725,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": 0.2738448679447174,
        "y": -0.6521247625350952,
        "z": 0.054460979998111725
      },
      "starmap": {
        "x": 0.009984961710870266,
        "y": -0.02094387821853161,
        "z": -0.012168290093541145
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-29",
    "abstract": "Infrared image helps improve the perception capabilities of autonomous driving in complex weather conditions such as fog, rain, and low light. However, infrared image often suffers from low contrast, especially in non-heat-emitting targets like bicycles, which significantly affects the performance of downstream high-level vision tasks. Furthermore, achieving contrast enhancement without amplifying noise and losing important information remains a challenge. To address these challenges, we propose a task-oriented infrared image enhancement method. Our approach consists of two key components: layer decomposition and saliency information extraction. First, we design an layer decomposition method for infrared images, which enhances scene details while preserving dark region features, providing more features for subsequent saliency information extraction. Then, we propose a morphological reconstruction-based saliency extraction method that effectively extracts and enhances target information without amplifying noise. Our method improves the image quality for object detection and semantic segmentation tasks. Extensive experiments demonstrate that our approach outperforms state-of-the-art methods.",
    "authors": "Siyuan Chai, Xiaodong Guo, Tong Liu",
    "id": "2506.23353v1",
    "title": "Layer Decomposition and Morphological Reconstruction for Task-Oriented Infrared Image Enhancement",
    "x": -3.180788278579712,
    "y": 0.7620671987533569,
    "z": 0.14035765826702118,
    "cluster_id": 1,
    "layouts": {
      "pacmap": {
        "x": -3.180788278579712,
        "y": 0.7620671987533569,
        "z": 0.14035765826702118
      },
      "starmap": {
        "x": 0.03819695860147476,
        "y": 0.03737663850188255,
        "z": 4.3281612306600437e-05
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-29",
    "abstract": "The advancement of 3D language fields has enabled intuitive interactions with 3D scenes via natural language. However, existing approaches are typically limited to small-scale environments, lacking the scalability and compositional reasoning capabilities necessary for large, complex urban settings. To overcome these limitations, we propose GeoProg3D, a visual programming framework that enables natural language-driven interactions with city-scale high-fidelity 3D scenes. GeoProg3D consists of two key components: (i) a Geography-aware City-scale 3D Language Field (GCLF) that leverages a memory-efficient hierarchical 3D model to handle large-scale data, integrated with geographic information for efficiently filtering vast urban spaces using directional cues, distance measurements, elevation data, and landmark references; and (ii) Geographical Vision APIs (GV-APIs), specialized geographic vision tools such as area segmentation and object detection. Our framework employs large language models (LLMs) as reasoning engines to dynamically combine GV-APIs and operate GCLF, effectively supporting diverse geographic vision tasks. To assess performance in city-scale reasoning, we introduce GeoEval3D, a comprehensive benchmark dataset containing 952 query-answer pairs across five challenging tasks: grounding, spatial reasoning, comparison, counting, and measurement. Experiments demonstrate that GeoProg3D significantly outperforms existing 3D language fields and vision-language models across multiple tasks. To our knowledge, GeoProg3D is the first framework enabling compositional geographic reasoning in high-fidelity city-scale 3D environments via natural language. The code is available at https://snskysk.github.io/GeoProg3D/.",
    "authors": "Shunsuke Yasuki, Taiki Miyanishi, Nakamasa Inoue, Shuhei Kurita, Koya Sakamoto, Daichi Azuma, Masato Taki, Yutaka Matsuo",
    "id": "2506.23352v1",
    "title": "GeoProg3D: Compositional Visual Reasoning for City-Scale 3D Language Fields",
    "x": 0.9732207655906677,
    "y": 1.7984322309494019,
    "z": 0.26702624559402466,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": 0.9732207655906677,
        "y": 1.7984322309494019,
        "z": 0.26702624559402466
      },
      "starmap": {
        "x": 0.015319067053496838,
        "y": 0.021625293418765068,
        "z": -0.01840106211602688
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-29",
    "id": "2506.23351v1",
    "authors": "Tianxing Chen, Kaixuan Wang, Zhaohui Yang, Yuhao Zhang, Zanxin Chen, Baijun Chen, Wanxi Dong, Ziyuan Liu, Dong Chen, Tianshuo Yang, Haibao Yu, Xiaokang Yang, Yusen Qin, Zhiqiang Xie, Yao Mu, Ping Luo, Tian Nian, Weiliang Deng, Yiheng Ge, Yibin Liu, Zixuan Li, Dehui Wang, Zhixuan Liang, Haohui Xie, Rijie Zeng, Yunfei Ge, Peiqing Cong, Guannan He, Zhaoming Han, Ruocheng Yin, Jingxiang Guo, Lunkai Lin, Tianling Xu, Hongzhe Bi, Xuewu Lin, Tianwei Lin, Shujie Luo, Keyu Li, Ziyan Zhao, Ke Fan, Heyang Xu, Bo Peng, Wenlong Gao, Dongjiang Li, Feng Jin, Hui Shen, Jinming Li, Chaowei Cui, Yuchen, Yaxin Peng, Lingdong Zeng, Wenlong Dong, Tengfei Li, Weijie Ke, Jun Chen, Erdemt Bao, Tian Lan, Tenglong Liu, Jin Yang, Huiping Zhuang, Baozhi Jia, Shuai Zhang, Zhengfeng Zou, Fangheng Guan, Tianyi Jia, Ke Zhou, Hongjiu Zhang, Yating Han, Cheng Fang, Yixian Zou, Chongyang Xu, Qinglun Zhang, Shen Cheng, Xiaohe Wang, Ping Tan, Haoqiang Fan, Shuaicheng Liu, Jiaheng Chen, Chuxuan Huang, Chengliang Lin, Kaijun Luo, Boyu Yue, Yi Liu, Jinyu Chen, Zichang Tan, Liming Deng, Shuo Xu, Zijian Cai, Shilong Yin, Hao Wang, Hongshan Liu, Tianyang Li, Long Shi, Ran Xu, Huilin Xu, Zhengquan Zhang, Congsheng Xu, Jinchang Yang, Feng Xu",
    "title": "Benchmarking Generalizable Bimanual Manipulation: RoboTwin Dual-Arm Collaboration Challenge at CVPR 2025 MEIS Workshop",
    "abstract": "Embodied Artificial Intelligence (Embodied AI) is an emerging frontier in robotics, driven by the need for autonomous systems that can perceive, reason, and act in complex physical environments. While single-arm systems have shown strong task performance, collaborative dual-arm systems are essential for handling more intricate tasks involving rigid, deformable, and tactile-sensitive objects. To advance this goal, we launched the RoboTwin Dual-Arm Collaboration Challenge at the 2nd MEIS Workshop, CVPR 2025. Built on the RoboTwin Simulation platform (1.0 and 2.0) and the AgileX COBOT-Magic Robot platform, the competition consisted of three stages: Simulation Round 1, Simulation Round 2, and a final Real-World Round. Participants totally tackled 17 dual-arm manipulation tasks, covering rigid, deformable, and tactile-based scenarios. The challenge attracted 64 global teams and over 400 participants, producing top-performing solutions like SEM and AnchorDP3 and generating valuable insights into generalizable bimanual policy learning. This report outlines the competition setup, task design, evaluation methodology, key findings and future direction, aiming to support future research on robust and generalizable bimanual manipulation policies. The Challenge Webpage is available at https://robotwin-benchmark.github.io/cvpr-2025-challenge/.",
    "x": 3.312187671661377,
    "y": 0.6949499845504761,
    "z": 1.4254080057144165,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 3.312187671661377,
        "y": 0.6949499845504761,
        "z": 1.4254080057144165
      },
      "starmap": {
        "x": -0.02450213022530079,
        "y": 0.00900363177061081,
        "z": -0.0013364102924242616
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-29",
    "id": "2506.23349v1",
    "authors": "Keziah Naggita, Julienne LaChance",
    "abstract": "Following the rise in popularity of data-centric machine learning (ML), various data valuation methods have been proposed to quantify the contribution of each datapoint to desired ML model performance metrics (e.g., accuracy). Beyond the technical applications of data valuation methods (e.g., data cleaning, data acquisition, etc.), it has been suggested that within the context of data markets, data buyers might utilize such methods to fairly compensate data owners. Here we demonstrate that data valuation metrics are inherently biased and unstable under simple algorithmic design choices, resulting in both technical and ethical implications. By analyzing 9 tabular classification datasets and 6 data valuation methods, we illustrate how (1) common and inexpensive data pre-processing techniques can drastically alter estimated data values; (2) subsampling via data valuation metrics may increase class imbalance; and (3) data valuation metrics may undervalue underrepresented group data. Consequently, we argue in favor of increased transparency associated with data valuation in-the-wild and introduce the novel Data Valuation Cards (DValCards) framework towards this aim. The proliferation of DValCards will reduce misuse of data valuation metrics, including in data pricing, and build trust in responsible ML systems.",
    "title": "A case for data valuation transparency via DValCards",
    "x": 0.118523508310318,
    "y": -1.3585385084152222,
    "z": -0.21510913968086243,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": 0.118523508310318,
        "y": -1.3585385084152222,
        "z": -0.21510913968086243
      },
      "starmap": {
        "x": 0.0060493843629956245,
        "y": -0.013678434304893017,
        "z": -0.016067590564489365
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Yi Liu, Shengqian Li, Zuzeng Lin, Feng Wang, Si Liu",
    "abstract": "The current conditional autoregressive image generation methods have shown promising results, yet their potential remains largely unexplored in the practical unsupervised image translation domain, which operates without explicit cross-domain correspondences. A critical limitation stems from the discrete quantization inherent in traditional Vector Quantization-based frameworks, which disrupts gradient flow between the Variational Autoencoder decoder and causal Transformer, impeding end-to-end optimization during adversarial training in image space. To tackle this issue, we propose using Softmax Relaxed Quantization, a novel approach that reformulates codebook selection as a continuous probability mixing process via Softmax, thereby preserving gradient propagation. Building upon this differentiable foundation, we introduce CycleVAR, which reformulates image-to-image translation as image-conditional visual autoregressive generation by injecting multi-scale source image tokens as contextual prompts, analogous to prefix-based conditioning in language models. CycleVAR exploits two modes to generate the target image tokens, including (1) serial multi-step generation, enabling iterative refinement across scales, and (2) parallel one-step generation synthesizing all resolution outputs in a single forward pass. Experimental findings indicate that the parallel one-step generation mode attains superior translation quality with quicker inference speed than the serial multi-step mode in unsupervised scenarios. Furthermore, both quantitative and qualitative results indicate that CycleVAR surpasses previous state-of-the-art unsupervised image translation models, \\textit{e}.\\textit{g}., CycleGAN-Turbo.",
    "date": "2025-06-29",
    "id": "2506.23347v1",
    "title": "CycleVAR: Repurposing Autoregressive Model for Unsupervised One-Step Image Translation",
    "x": -3.2578532695770264,
    "y": 0.33595946431159973,
    "z": -0.1729225367307663,
    "cluster_id": 2,
    "layouts": {
      "pacmap": {
        "x": -3.2578532695770264,
        "y": 0.33595946431159973,
        "z": -0.1729225367307663
      },
      "starmap": {
        "x": 0.019952278584241867,
        "y": -0.00022811828239355236,
        "z": 0.07093442976474762
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "Data-Driven Self-Supervised Learning for the Discovery of Solution Singularity for Partial Differential Equations",
    "abstract": "The appearance of singularities in the function of interest constitutes a fundamental challenge in scientific computing. It can significantly undermine the effectiveness of numerical schemes for function approximation, numerical integration, and the solution of partial differential equations (PDEs), etc. The problem becomes more sophisticated if the location of the singularity is unknown, which is often encountered in solving PDEs. Detecting the singularity is therefore critical for developing efficient adaptive methods to reduce computational costs in various applications. In this paper, we consider singularity detection in a purely data-driven setting. Namely, the input only contains given data, such as the vertex set from a mesh. To overcome the limitation of the raw unlabeled data, we propose a self-supervised learning (SSL) framework for estimating the location of the singularity. A key component is a filtering procedure as the pretext task in SSL, where two filtering methods are presented, based on $k$ nearest neighbors and kernel density estimation, respectively. We provide numerical examples to illustrate the potential pathological or inaccurate results due to the use of raw data without filtering. Various experiments are presented to demonstrate the ability of the proposed approach to deal with input perturbation, label corruption, and different kinds of singularities such interior circle, boundary layer, concentric semicircles, etc.",
    "id": "2506.23344v1",
    "authors": "Difeng Cai, Paulina Sep\u00falveda",
    "date": "2025-06-29",
    "x": -0.5226454138755798,
    "y": -2.970643997192383,
    "z": -0.875295102596283,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": -0.5226454138755798,
        "y": -2.970643997192383,
        "z": -0.875295102596283
      },
      "starmap": {
        "x": 0.004920805338770151,
        "y": -0.01797282136976719,
        "z": -0.020965291187167168
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-29",
    "id": "2506.23342v1",
    "title": "ATGen: A Framework for Active Text Generation",
    "abstract": "Active learning (AL) has demonstrated remarkable potential in reducing the annotation effort required for training machine learning models. However, despite the surging popularity of natural language generation (NLG) tasks in recent years, the application of AL to NLG has been limited. In this paper, we introduce Active Text Generation (ATGen) - a comprehensive framework that bridges AL with text generation tasks, enabling the application of state-of-the-art AL strategies to NLG. Our framework simplifies AL-empowered annotation in NLG tasks using both human annotators and automatic annotation agents based on large language models (LLMs). The framework supports LLMs deployed as services, such as ChatGPT and Claude, or operated on-premises. Furthermore, ATGen provides a unified platform for smooth implementation and benchmarking of novel AL strategies tailored to NLG tasks. Finally, we present evaluation results for state-of-the-art AL strategies across diverse settings and multiple text generation tasks. We show that ATGen reduces both the effort of human annotators and costs associated with API calls to LLM-based annotation agents. The code of the framework is available on GitHub under the MIT license. The video presentation is available at http://atgen-video.nlpresearch.group",
    "authors": "Akim Tsvigun, Daniil Vasilev, Ivan Tsvigun, Ivan Lysenko, Talgat Bektleuov, Aleksandr Medvedev, Uliana Vinogradova, Nikita Severin, Mikhail Mozikov, Andrey Savchenko, Rostislav Grigorev, Ramil Kuleev, Fedor Zhdanov, Artem Shelmanov, Ilya Makarov",
    "x": 3.039177179336548,
    "y": 0.992362916469574,
    "z": 2.2763898372650146,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 3.039177179336548,
        "y": 0.992362916469574,
        "z": 2.2763898372650146
      },
      "starmap": {
        "x": -0.01953315921127796,
        "y": 0.006807755213230848,
        "z": 0.005232899449765682
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-29",
    "authors": "Yumeng Lin, Xufeng Duan, David Haslett, Yige Chen, Zhenguang G. Cai",
    "abstract": "Large language models have achieved impressive progress in multilingual translation, yet they continue to face challenges with certain language pairs-particularly those with limited training data or significant linguistic divergence from English. This study systematically investigates how training data, language proximity, and language family affect information loss in multilingual translation. We evaluate two large language models, GPT-4 and Llama 2, by performing round-trip translations. Translation quality was assessed using BLEU scores and BERT similarity metrics. Our results reveal a robust interaction between training data size and language distance: while abundant training data can mitigate the effects of linguistic divergence, languages structurally closer to English consistently yield higher translation quality in low-resource conditions. Among various distance metrics, orthographic, phylogenetic, syntactic, and geographical distances emerge as strong predictors of translation performance. Language family also exerts an independent influence. These findings contribute to a deeper understanding of the linguistic constraints shaping multilingual translation in large language models, emphasizing that translation quality is shaped not only by data volume but also by structural and typological relationships between languages.",
    "id": "2506.23340v1",
    "title": "Information Loss in LLMs' Multilingual Translation: The Role of Training Data, Language Proximity, and Language Family",
    "x": 1.4387332201004028,
    "y": 1.0448051691055298,
    "z": -1.181524395942688,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 1.4387332201004028,
        "y": 1.0448051691055298,
        "z": -1.181524395942688
      },
      "starmap": {
        "x": -0.018894115462899208,
        "y": 0.004698616452515125,
        "z": 0.003914742264896631
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Malikussaid, Hilal Hudan Nuha",
    "id": "2506.23339v1",
    "title": "VALID-Mol: a Systematic Framework for Validated LLM-Assisted Molecular Design",
    "date": "2025-06-29",
    "abstract": "Large Language Models (LLMs) demonstrate remarkable potential for scientific discovery, but their application in domains requiring factual accuracy and domain-specific constraints remains challenging. In molecular design for drug discovery, LLMs can suggest creative molecular modifications but often produce chemically invalid or impractical structures. We present VALID-Mol, a systematic framework for integrating chemical validation with LLM-driven molecular design that increases the rate of generating valid chemical structures from 3% to 83%. Our approach combines methodical prompt engineering, automated chemical validation, and a fine-tuned domain-adapted LLM to ensure reliable generation of synthesizable molecules with improved properties. Beyond the specific implementation, we contribute a generalizable methodology for scientifically-constrained LLM applications, with quantifiable reliability improvements. Computational predictions suggest our framework can generate promising candidates for synthesis with up to 17-fold computationally predicted improvements in target affinity while maintaining synthetic accessibility. We provide a detailed analysis of our prompt engineering process, validation architecture, and fine-tuning approach, offering a reproducible blueprint for applying LLMs to other scientific domains where domain-specific validation is essential.",
    "x": 1.875203013420105,
    "y": 0.027814792469143867,
    "z": 0.5773130655288696,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 1.875203013420105,
        "y": 0.027814792469143867,
        "z": 0.5773130655288696
      },
      "starmap": {
        "x": -0.01912577450275421,
        "y": 0.0034535338636487722,
        "z": -0.002615632489323616
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-29",
    "title": "Federated Breast Cancer Detection Enhanced by Synthetic Ultrasound Image Augmentation",
    "authors": "Hongyi Pan, Ziliang Hong, Gorkem Durak, Ziyue Xu, Ulas Bagci",
    "abstract": "Federated learning (FL) has emerged as a promising paradigm for collaboratively training deep learning models across institutions without exchanging sensitive medical data. However, its effectiveness is often hindered by limited data availability and non-independent, identically distributed data across participating clients, which can degrade model performance and generalization. To address these challenges, we propose a generative AI based data augmentation framework that integrates synthetic image sharing into the federated training process for breast cancer diagnosis via ultrasound images. Specifically, we train two simple class-specific Deep Convolutional Generative Adversarial Networks: one for benign and one for malignant lesions. We then simulate a realistic FL setting using three publicly available breast ultrasound image datasets: BUSI, BUS-BRA, and UDIAT. FedAvg and FedProx are adopted as baseline FL algorithms. Experimental results show that incorporating a suitable number of synthetic images improved the average AUC from 0.9206 to 0.9237 for FedAvg and from 0.9429 to 0.9538 for FedProx. We also note that excessive use of synthetic data reduced performance, underscoring the importance of maintaining a balanced ratio of real and synthetic samples. Our findings highlight the potential of generative AI based data augmentation to enhance FL results in the breast ultrasound image classification task.",
    "id": "2506.23334v1",
    "x": -0.81315016746521,
    "y": -1.1673978567123413,
    "z": -0.02791915275156498,
    "cluster_id": 4,
    "layouts": {
      "pacmap": {
        "x": -0.81315016746521,
        "y": -1.1673978567123413,
        "z": -0.02791915275156498
      },
      "starmap": {
        "x": 0.01932978630065918,
        "y": -0.026278143748641014,
        "z": -0.012217141687870026
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Parker Liu, Chenxin Li, Zhengxin Li, Yipeng Wu, Wuyang Li, Zhiqin Yang, Zhenyuan Zhang, Yunlong Lin, Sirui Han, Brandon Y. Feng",
    "date": "2025-06-29",
    "title": "IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering",
    "id": "2506.23329v1",
    "abstract": "Vision-language models (VLMs) excel at descriptive tasks, but whether they truly understand scenes from visual observations remains uncertain. We introduce IR3D-Bench, a benchmark challenging VLMs to demonstrate understanding through active creation rather than passive recognition. Grounded in the analysis-by-synthesis paradigm, IR3D-Bench tasks Vision-Language Agents (VLAs) with actively using programming and rendering tools to recreate the underlying 3D structure of an input image, achieving agentic inverse rendering through tool use. This \"understanding-by-creating\" approach probes the tool-using generative capacity of VLAs, moving beyond the descriptive or conversational capacity measured by traditional scene understanding benchmarks. We provide a comprehensive suite of metrics to evaluate geometric accuracy, spatial relations, appearance attributes, and overall plausibility. Initial experiments on agentic inverse rendering powered by various state-of-the-art VLMs highlight current limitations, particularly in visual precision rather than basic tool usage. IR3D-Bench, including data and evaluation protocols, is released to facilitate systematic study and development of tool-using VLAs towards genuine scene understanding by creating.",
    "x": 0.1553834229707718,
    "y": 2.1965370178222656,
    "z": -0.7004392147064209,
    "cluster_id": 3,
    "layouts": {
      "pacmap": {
        "x": 0.1553834229707718,
        "y": 2.1965370178222656,
        "z": -0.7004392147064209
      },
      "starmap": {
        "x": 0.012632051482796669,
        "y": -0.030050719156861305,
        "z": -0.024072909727692604
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "XY-Tokenizer: Mitigating the Semantic-Acoustic Conflict in Low-Bitrate Speech Codecs",
    "abstract": "Speech codecs serve as bridges between speech signals and large language models. An ideal codec for speech language models should not only preserve acoustic information but also capture rich semantic information. However, existing speech codecs struggle to balance high-quality audio reconstruction with ease of modeling by language models. In this study, we analyze the limitations of previous codecs in balancing semantic richness and acoustic fidelity. We propose XY-Tokenizer, a novel codec that mitigates the conflict between semantic and acoustic capabilities through multi-stage, multi-task learning. Experimental results demonstrate that XY-Tokenizer achieves performance in both semantic and acoustic tasks comparable to that of state-of-the-art codecs operating at similar bitrates, even though those existing codecs typically excel in only one aspect. Specifically, XY-Tokenizer achieves strong text alignment, surpassing distillation-based semantic modeling methods such as SpeechTokenizer and Mimi, while maintaining a speaker similarity score of 0.83 between reconstructed and original audio. The reconstruction performance of XY-Tokenizer is comparable to that of BigCodec, the current state-of-the-art among acoustic-only codecs, which achieves a speaker similarity score of 0.84 at a similar bitrate. Code and models are available at https://github.com/gyt1145028706/XY-Tokenizer.",
    "date": "2025-06-29",
    "id": "2506.23325v1",
    "authors": "Yitian Gong, Luozhijie Jin, Ruifan Deng, Dong Zhang, Xin Zhang, Qinyuan Cheng, Zhaoye Fei, Shimin Li, Xipeng Qiu",
    "x": -0.5396167635917664,
    "y": 1.4140735864639282,
    "z": 0.21740619838237762,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": -0.5396167635917664,
        "y": 1.4140735864639282,
        "z": 0.21740619838237762
      },
      "starmap": {
        "x": -0.012023837305605412,
        "y": 0.001975679537281394,
        "z": 0.0073873563669621944
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Quang-Huy Che, Vinh-Tiep Nguyen",
    "date": "2025-06-29",
    "id": "2506.23323v1",
    "title": "FastSeg: Efficient Training-Free Open-Vocabulary Segmentation via Hierarchical Attention Refinement Method",
    "abstract": "Open-vocabulary semantic segmentation (OVSS) aims to segment objects from arbitrary text categories without requiring densely annotated datasets. Although contrastive learning based models enable zero-shot segmentation, they often lose fine spatial precision at pixel level, due to global representation bias. In contrast, diffusion-based models naturally encode fine-grained spatial features via attention mechanisms that capture both global context and local details. However, they often face challenges in balancing the number of iterations with the quality of the segmentation. In this work, we propose FastSeg, a novel and efficient training-free framework with only (1+1)-step of reverse process of a pretrained diffusion model (e.g., Stable Diffusion). Moreover, instead of running multiple times for different classes, FastSeg performs segmentation for all classes at once. To further enhance the segmentation quality, FastSeg introduces three key components: (i) a dual-prompt mechanism for discriminative, class-aware attention extraction, (ii) a Hierarchical Attention Refinement Method (HARD) that enhances fused cross-attention using scale-aligned selfattention maps, and (iii) a Test-Time Flipping (TTF) scheme designed to improve spatial consistency. Extensive experiments show that FastSeg achieves state-of-the-art training-free performance, obtaining 43.8% average mIoU across PASCAL VOC, PASCAL Context, and COCO Object benchmarks while maintaining superior inference efficiency. Our results demonstrate that FastSeg provides a strong foundation for extendability, bridging the gap between segmentation quality and inference efficiency.",
    "x": -1.96475088596344,
    "y": 0.625580370426178,
    "z": -1.113569974899292,
    "cluster_id": 4,
    "layouts": {
      "pacmap": {
        "x": -1.96475088596344,
        "y": 0.625580370426178,
        "z": -1.113569974899292
      },
      "starmap": {
        "x": 0.0246124267578125,
        "y": -0.024877768009901047,
        "z": 0.00031532172579318285
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23322v1",
    "abstract": "In the financial industry, data is the lifeblood of operations, and DBAs shoulder significant responsibilities for SQL tuning, database deployment, diagnosis, and service repair. In recent years, both database vendors and customers have increasingly turned to autonomous database platforms in an effort to alleviate the heavy workload of DBAs. However, existing autonomous database platforms are limited in their capabilities, primarily addressing single-point issues such as NL2SQL, anomaly detection, and SQL tuning. Manual intervention remains a necessity for comprehensive database maintenance. GaussMaster aims to revolutionize this landscape by introducing an LLM-based database copilot system. This innovative solution is designed not only to assist developers in writing efficient SQL queries but also to provide comprehensive care for database services. When database instances exhibit abnormal behavior, GaussMaster is capable of orchestrating the entire maintenance process automatically. It achieves this by analyzing hundreds of metrics and logs, employing a Tree-of-thought approach to identify root causes, and invoking appropriate tools to resolve issues. We have successfully implemented GaussMaster in real-world scenarios, such as the banking industry, where it has achieved zero human intervention for over 34 database maintenance scenarios. In this paper, we present significant improvements in these tasks with code at https://gitcode.com/opengauss/openGauss-GaussMaster.",
    "date": "2025-06-29",
    "title": "GaussMaster: An LLM-based Database Copilot System",
    "authors": "Wei Zhou, Ji Sun, Xuanhe Zhou, Guoliang Li, Luyang Liu, Hao Wu, Tianyuan Wang",
    "x": 0.31722292304039,
    "y": 2.3288204669952393,
    "z": 1.4380979537963867,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 0.31722292304039,
        "y": 2.3288204669952393,
        "z": 1.4380979537963867
      },
      "starmap": {
        "x": -0.020277267321944237,
        "y": 0.0067687430419027805,
        "z": 0.002098041819408536
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Learning to Rank (LTR) methods generally assume that each document in a top-K ranking is presented in an equal format. However, previous work has shown that users' perceptions of relevance can be changed by varying presentations, i.e., allocating more vertical space to some documents to provide additional textual or image information. Furthermore, presentation length can also redirect attention, as users are more likely to notice longer presentations when scrolling through results. Deciding on the document presentation lengths in a fixed vertical space ranking is an important problem that has not been addressed by existing LTR methods.   We address this gap by introducing the variable presentation length ranking task, where simultaneously the ordering of documents and their presentation length is decided. Despite being a generalization of standard ranking, we show that this setting brings significant new challenges: Firstly, the probability ranking principle no longer applies to this setting, and secondly, the problem cannot be divided into separate ordering and length selection tasks.   We therefore propose VLPL - a new family of Plackett-Luce list-wise gradient estimation methods for the joint optimization of document ordering and lengths. Our semi-synthetic experiments show that VLPL can effectively balance the expected exposure and attractiveness of all documents, achieving the best performance across different ranking settings. Furthermore, we observe that even simple length-aware methods can achieve significant performance improvements over fixed-length models. Altogether, our theoretical and empirical results highlight the importance and difficulties of combining document presentation with LTR.",
    "authors": "Norman Knyazev, Harrie Oosterhuis",
    "date": "2025-06-29",
    "title": "Learning to Rank with Variable Result Presentation Lengths",
    "id": "2506.23319v1",
    "x": 0.6789728999137878,
    "y": -3.0745973587036133,
    "z": -0.19522839784622192,
    "cluster_id": -1,
    "layouts": {
      "pacmap": {
        "x": 0.6789728999137878,
        "y": -3.0745973587036133,
        "z": -0.19522839784622192
      },
      "starmap": {
        "x": -0.0018697992200031877,
        "y": -0.004637428559362888,
        "z": -0.005798898637294769
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-29",
    "title": "InfGen: Scenario Generation as Next Token Group Prediction",
    "abstract": "Realistic and interactive traffic simulation is essential for training and evaluating autonomous driving systems. However, most existing data-driven simulation methods rely on static initialization or log-replay data, limiting their ability to model dynamic, long-horizon scenarios with evolving agent populations. We propose InfGen, a scenario generation framework that outputs agent states and trajectories in an autoregressive manner. InfGen represents the entire scene as a sequence of tokens, including traffic light signals, agent states, and motion vectors, and uses a transformer model to simulate traffic over time. This design enables InfGen to continuously insert new agents into traffic, supporting infinite scene generation. Experiments demonstrate that InfGen produces realistic, diverse, and adaptive traffic behaviors. Furthermore, reinforcement learning policies trained in InfGen-generated scenarios achieve superior robustness and generalization, validating its utility as a high-fidelity simulation environment for autonomous driving. More information is available at https://metadriverse.github.io/infgen/.",
    "id": "2506.23316v1",
    "authors": "Zhenghao Peng, Yuxin Liu, Bolei Zhou",
    "x": 1.560620903968811,
    "y": 0.7655525803565979,
    "z": 2.0999574661254883,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 1.560620903968811,
        "y": 0.7655525803565979,
        "z": 2.0999574661254883
      },
      "starmap": {
        "x": -0.022910455241799355,
        "y": 0.009071086533367634,
        "z": 0.0008224062621593475
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "Ensemble BERT for Medication Event Classification on Electronic Health Records (EHRs)",
    "authors": "Shouvon Sarker, Xishuang Dong, Lijun Qian",
    "date": "2025-06-29",
    "abstract": "Identification of key variables such as medications, diseases, relations from health records and clinical notes has a wide range of applications in the clinical domain. n2c2 2022 provided shared tasks on challenges in natural language processing for clinical data analytics on electronic health records (EHR), where it built a comprehensive annotated clinical data Contextualized Medication Event Dataset (CMED). This study focuses on subtask 2 in Track 1 of this challenge that is to detect and classify medication events from clinical notes through building a novel BERT-based ensemble model. It started with pretraining BERT models on different types of big data such as Wikipedia and MIMIC. Afterwards, these pretrained BERT models were fine-tuned on CMED training data. These fine-tuned BERT models were employed to accomplish medication event classification on CMED testing data with multiple predictions. These multiple predictions generated by these fine-tuned BERT models were integrated to build final prediction with voting strategies. Experimental results demonstrated that BERT-based ensemble models can effectively improve strict Micro-F score by about 5% and strict Macro-F score by about 6%, respectively.",
    "id": "2506.23315v1",
    "x": 1.2010632753372192,
    "y": -0.6655445098876953,
    "z": 0.41775572299957275,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 1.2010632753372192,
        "y": -0.6655445098876953,
        "z": 0.41775572299957275
      },
      "starmap": {
        "x": -0.015130977146327496,
        "y": -2.261357440147549e-05,
        "z": -0.0019480828195810318
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "abstract": "Malware detection in Android systems requires both cybersecurity expertise and machine learning (ML) techniques. Automated Machine Learning (AutoML) has emerged as an approach to simplify ML development by reducing the need for specialized knowledge. However, current AutoML solutions typically operate as black-box systems with limited transparency, interpretability, and experiment traceability. To address these limitations, we present MH-AutoML, a domain-specific framework for Android malware detection. MH-AutoML automates the entire ML pipeline, including data preprocessing, feature engineering, algorithm selection, and hyperparameter tuning. The framework incorporates capabilities for interpretability, debugging, and experiment tracking that are often missing in general-purpose solutions. In this study, we compare MH-AutoML against seven established AutoML frameworks: Auto-Sklearn, AutoGluon, TPOT, HyperGBM, Auto-PyTorch, LightAutoML, and MLJAR. Results show that MH-AutoML achieves better recall rates while providing more transparency and control. The framework maintains computational efficiency comparable to other solutions, making it suitable for cybersecurity applications where both performance and explainability matter.",
    "id": "2506.23314v1",
    "date": "2025-06-29",
    "title": "Interpretable by Design: MH-AutoML for Transparent and Efficient Android Malware Detection without Compromising Performance",
    "authors": "Joner Assolin, Gabriel Canto, Diego Kreutz, Eduardo Feitosa, Hendrio Bragan\u00e7a, Angelo Nogueira, Vanderson Rocha",
    "x": 1.6143354177474976,
    "y": -0.845235288143158,
    "z": -0.1325540691614151,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 1.6143354177474976,
        "y": -0.845235288143158,
        "z": -0.1325540691614151
      },
      "starmap": {
        "x": -0.018836278468370438,
        "y": 0.004704498220235109,
        "z": -0.0024557425640523434
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-29",
    "title": "Physics informed guided diffusion for accelerated multi-parametric MRI reconstruction",
    "id": "2506.23311v1",
    "abstract": "We introduce MRF-DiPh, a novel physics informed denoising diffusion approach for multiparametric tissue mapping from highly accelerated, transient-state quantitative MRI acquisitions like Magnetic Resonance Fingerprinting (MRF). Our method is derived from a proximal splitting formulation, incorporating a pretrained denoising diffusion model as an effective image prior to regularize the MRF inverse problem. Further, during reconstruction it simultaneously enforces two key physical constraints: (1) k-space measurement consistency and (2) adherence to the Bloch response model. Numerical experiments on in-vivo brain scans data show that MRF-DiPh outperforms deep learning and compressed sensing MRF baselines, providing more accurate parameter maps while better preserving measurement fidelity and physical model consistency-critical for solving reliably inverse problems in medical imaging.",
    "authors": "Perla Mayo, Carolin M. Pirkl, Alin Achim, Bjoern Menze, Mohammad Golbabaee",
    "x": -2.8761918544769287,
    "y": -0.9886912107467651,
    "z": -1.3473550081253052,
    "cluster_id": 8,
    "layouts": {
      "pacmap": {
        "x": -2.8761918544769287,
        "y": -0.9886912107467651,
        "z": -1.3473550081253052
      },
      "starmap": {
        "x": 0.014632525853812695,
        "y": -0.020929038524627686,
        "z": -0.0034726341255009174
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "id": "2506.23309v2",
    "authors": "Yiming Huang, Long Bai, Beilei Cui, Kun Yuan, Guankun Wang, Mobarak I. Hoque, Nicolas Padoy, Nassir Navab, Hongliang Ren",
    "date": "2025-06-29",
    "abstract": "In contemporary surgical research and practice, accurately comprehending 3D surgical scenes with text-promptable capabilities is particularly crucial for surgical planning and real-time intra-operative guidance, where precisely identifying and interacting with surgical tools and anatomical structures is paramount. However, existing works focus on surgical vision-language model (VLM), 3D reconstruction, and segmentation separately, lacking support for real-time text-promptable 3D queries. In this paper, we present SurgTPGS, a novel text-promptable Gaussian Splatting method to fill this gap. We introduce a 3D semantics feature learning strategy incorporating the Segment Anything model and state-of-the-art vision-language models. We extract the segmented language features for 3D surgical scene reconstruction, enabling a more in-depth understanding of the complex surgical environment. We also propose semantic-aware deformation tracking to capture the seamless deformation of semantic features, providing a more precise reconstruction for both texture and semantic features. Furthermore, we present semantic region-aware optimization, which utilizes regional-based semantic information to supervise the training, particularly promoting the reconstruction quality and semantic smoothness. We conduct comprehensive experiments on two real-world surgical datasets to demonstrate the superiority of SurgTPGS over state-of-the-art methods, highlighting its potential to revolutionize surgical practices. SurgTPGS paves the way for developing next-generation intelligent surgical systems by enhancing surgical precision and safety. Our code is available at: https://github.com/lastbasket/SurgTPGS.",
    "title": "SurgTPGS: Semantic 3D Surgical Scene Understanding with Text Promptable Gaussian Splatting",
    "x": -2.081815719604492,
    "y": 2.271590232849121,
    "z": -0.14313039183616638,
    "cluster_id": 1,
    "layouts": {
      "pacmap": {
        "x": -2.081815719604492,
        "y": 2.271590232849121,
        "z": -0.14313039183616638
      },
      "starmap": {
        "x": 0.04722254350781441,
        "y": 0.06119583174586296,
        "z": -0.027624983340501785
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-29",
    "id": "2506.23308v1",
    "authors": "Yiming Huang, Long Bai, Beilei Cui, Yanheng Li, Tong Chen, Jie Wang, Jinlin Wu, Zhen Lei, Hongbin Liu, Hongliang Ren",
    "abstract": "Accurate reconstruction of soft tissue is crucial for advancing automation in image-guided robotic surgery. The recent 3D Gaussian Splatting (3DGS) techniques and their variants, 4DGS, achieve high-quality renderings of dynamic surgical scenes in real-time. However, 3D-GS-based methods still struggle in scenarios with varying illumination, such as low light and over-exposure. Training 3D-GS in such extreme light conditions leads to severe optimization problems and devastating rendering quality. To address these challenges, we present Endo-4DGX, a novel reconstruction method with illumination-adaptive Gaussian Splatting designed specifically for endoscopic scenes with uneven lighting. By incorporating illumination embeddings, our method effectively models view-dependent brightness variations. We introduce a region-aware enhancement module to model the sub-area lightness at the Gaussian level and a spatial-aware adjustment module to learn the view-consistent brightness adjustment. With the illumination adaptive design, Endo-4DGX achieves superior rendering performance under both low-light and over-exposure conditions while maintaining geometric accuracy. Additionally, we employ an exposure control loss to restore the appearance from adverse exposure to the normal level for illumination-adaptive optimization. Experimental results demonstrate that Endo-4DGX significantly outperforms combinations of state-of-the-art reconstruction and restoration methods in challenging lighting environments, underscoring its potential to advance robot-assisted surgical applications. Our code is available at https://github.com/lastbasket/Endo-4DGX.",
    "title": "Endo-4DGX: Robust Endoscopic Scene Reconstruction and Illumination Correction with Gaussian Splatting",
    "x": -2.466400146484375,
    "y": 2.5913333892822266,
    "z": -0.16214269399642944,
    "cluster_id": 1,
    "layouts": {
      "pacmap": {
        "x": -2.466400146484375,
        "y": 2.5913333892822266,
        "z": -0.16214269399642944
      },
      "starmap": {
        "x": 0.04969508573412895,
        "y": 0.06789986789226532,
        "z": -0.024087319150567055
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Qi Liu, Can Li, Wanjing Ma",
    "date": "2025-06-29",
    "id": "2506.23306v1",
    "title": "GATSim: Urban Mobility Simulation with Generative Agents",
    "abstract": "Traditional agent-based urban mobility simulations rely on rigid rule-based systems that fail to capture the complexity, adaptability, and behavioral diversity characteristic of human travel decision-making. Recent advances in large language models and AI agent technology offer opportunities to create agents with reasoning capabilities, persistent memory, and adaptive learning mechanisms. We propose GATSim (Generative-Agent Transport Simulation), a novel framework that leverages these advances to create generative agents with rich behavioral characteristics for urban mobility simulation. Unlike conventional approaches, GATSim agents possess diverse socioeconomic attributes, individual lifestyles, and evolving preferences that shape their mobility decisions through psychologically-informed memory systems, tool usage capabilities, and lifelong learning mechanisms. The main contributions of this study include: (1) a comprehensive architecture combining an urban mobility foundation model with agent cognitive systems and transport simulation environment, (2) a fully functional prototype implementation, and (3) systematic validation demonstrating that generative agents produce believable travel behaviors. Through designed reflection processes, generative agents in this study can transform specific travel experiences into generalized insights, enabling realistic behavioral adaptation over time with specialized mechanisms for activity planning and real-time reactive behaviors tailored to urban mobility contexts. Experiments show that generative agents perform competitively with human annotators in mobility scenarios while naturally producing macroscopic traffic evolution patterns. The code for the prototype system is shared at https://github.com/qiliuchn/gatsim.",
    "x": 2.4865353107452393,
    "y": 0.8899843692779541,
    "z": 2.0216798782348633,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 2.4865353107452393,
        "y": 0.8899843692779541,
        "z": 2.0216798782348633
      },
      "starmap": {
        "x": -0.025012589991092682,
        "y": 0.010111609473824501,
        "z": 0.0014965174486860633
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "authors": "Rachit Saluja, Arzu Kovanlikaya, Candace Chien, Lauren Kathryn Blatt, Jeffrey M. Perlman, Stefan Worgall, Mert R. Sabuncu, Jonathan P. Dyke",
    "date": "2025-06-29",
    "title": "BPD-Neo: An MRI Dataset for Lung-Trachea Segmentation with Clinical Data for Neonatal Bronchopulmonary Dysplasia",
    "abstract": "Bronchopulmonary dysplasia (BPD) is a common complication among preterm neonates, with portable X-ray imaging serving as the standard diagnostic modality in neonatal intensive care units (NICUs). However, lung magnetic resonance imaging (MRI) offers a non-invasive alternative that avoids sedation and radiation while providing detailed insights into the underlying mechanisms of BPD. Leveraging high-resolution 3D MRI data, advanced image processing and semantic segmentation algorithms can be developed to assist clinicians in identifying the etiology of BPD. In this dataset, we present MRI scans paired with corresponding semantic segmentations of the lungs and trachea for 40 neonates, the majority of whom are diagnosed with BPD. The imaging data consist of free-breathing 3D stack-of-stars radial gradient echo acquisitions, known as the StarVIBE series. Additionally, we provide comprehensive clinical data and baseline segmentation models, validated against clinical assessments, to support further research and development in neonatal lung imaging.",
    "id": "2506.23305v1",
    "x": -2.6542561054229736,
    "y": -0.22627755999565125,
    "z": -1.7916793823242188,
    "cluster_id": 4,
    "layouts": {
      "pacmap": {
        "x": -2.6542561054229736,
        "y": -0.22627755999565125,
        "z": -1.7916793823242188
      },
      "starmap": {
        "x": 0.02593151293694973,
        "y": -0.041777241975069046,
        "z": -0.008449315093457699
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "title": "Securing AI Systems: A Guide to Known Attacks and Impacts",
    "abstract": "Embedded into information systems, artificial intelligence (AI) faces security threats that exploit AI-specific vulnerabilities. This paper provides an accessible overview of adversarial attacks unique to predictive and generative AI systems. We identify eleven major attack types and explicitly link attack techniques to their impacts -- including information leakage, system compromise, and resource exhaustion -- mapped to the confidentiality, integrity, and availability (CIA) security triad. We aim to equip researchers, developers, security practitioners, and policymakers, even those without specialized AI security expertise, with foundational knowledge to recognize AI-specific risks and implement effective defenses, thereby enhancing the overall security posture of AI systems.",
    "id": "2506.23296v1",
    "date": "2025-06-29",
    "authors": "Naoto Kiribuchi, Kengo Zenitani, Takayuki Semitsu",
    "x": 2.8752448558807373,
    "y": -0.40067020058631897,
    "z": 1.3089276552200317,
    "cluster_id": 0,
    "layouts": {
      "pacmap": {
        "x": 2.8752448558807373,
        "y": -0.40067020058631897,
        "z": 1.3089276552200317
      },
      "starmap": {
        "x": -0.02497074566781521,
        "y": 0.009176583029329777,
        "z": 0.00060742610367015
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  },
  {
    "date": "2025-06-29",
    "title": "DiffFit: Disentangled Garment Warping and Texture Refinement for Virtual Try-On",
    "abstract": "Virtual try-on (VTON) aims to synthesize realistic images of a person wearing a target garment, with broad applications in e-commerce and digital fashion. While recent advances in latent diffusion models have substantially improved visual quality, existing approaches still struggle with preserving fine-grained garment details, achieving precise garment-body alignment, maintaining inference efficiency, and generalizing to diverse poses and clothing styles. To address these challenges, we propose DiffFit, a novel two-stage latent diffusion framework for high-fidelity virtual try-on. DiffFit adopts a progressive generation strategy: the first stage performs geometry-aware garment warping, aligning the garment with the target body through fine-grained deformation and pose adaptation. The second stage refines texture fidelity via a cross-modal conditional diffusion model that integrates the warped garment, the original garment appearance, and the target person image for high-quality rendering. By decoupling geometric alignment and appearance refinement, DiffFit effectively reduces task complexity and enhances both generation stability and visual realism. It excels in preserving garment-specific attributes such as textures, wrinkles, and lighting, while ensuring accurate alignment with the human body. Extensive experiments on large-scale VTON benchmarks demonstrate that DiffFit achieves superior performance over existing state-of-the-art methods in both quantitative metrics and perceptual evaluations.",
    "id": "2506.23295v1",
    "authors": "Xiang Xu",
    "x": -2.856416702270508,
    "y": 2.088860273361206,
    "z": -1.0082815885543823,
    "cluster_id": 2,
    "layouts": {
      "pacmap": {
        "x": -2.856416702270508,
        "y": 2.088860273361206,
        "z": -1.0082815885543823
      },
      "starmap": {
        "x": 0.02201865240931511,
        "y": 0.004208255559206009,
        "z": 0.07333787530660629
      }
    },
    "layout_algorithm": "pacmap",
    "layout_version": "hybrid-graph-v1"
  }
]